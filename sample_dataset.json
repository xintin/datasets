[{"id":"acronym_identification","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:structure-prediction-other-acronym-identification"],"citation":"@inproceedings{veyseh-et-al-2020-what,\n   title={{What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation}},\n   author={Amir Pouran Ben Veyseh and Franck Dernoncourt and Quan Hung Tran and Thien Huu Nguyen},\n   year={2020},\n   booktitle={Proceedings of COLING},\n   link={https://arxiv.org/pdf/2010.14678v1.pdf}\n}","description":"Acronym identification training and development sets for the acronym identification task at SDU@AAAI-21.","paperswithcode_id":"acronym-identification","key":""},{"id":"ade_corpus_v2","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:1K<n<10K","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_categories:structure-prediction","task_ids:fact-checking","task_ids:coreference-resolution"],"citation":"@article{GURULINGAPPA2012885,\ntitle = \"Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports\",\njournal = \"Journal of Biomedical Informatics\",\nvolume = \"45\",\nnumber = \"5\",\npages = \"885 - 892\",\nyear = \"2012\",\nnote = \"Text Mining and Natural Language Processing in Pharmacogenomics\",\nissn = \"1532-0464\",\ndoi = \"https://doi.org/10.1016/j.jbi.2012.04.008\",\nurl = \"http://www.sciencedirect.com/science/article/pii/S1532046412000615\",\nauthor = \"Harsha Gurulingappa and Abdul Mateen Rajput and Angus Roberts and Juliane Fluck and Martin Hofmann-Apitius and Luca Toldo\",\nkeywords = \"Adverse drug effect, Benchmark corpus, Annotation, Harmonization, Sentence classification\",\nabstract = \"A significant amount of information about drug-related safety issues such as adverse effects are published in medical case reports that can only be explored by human readers due to their unstructured nature. The work presented here aims at generating a systematically annotated corpus that can support the development and validation of methods for the automatic extraction of drug-related adverse effects from medical case reports. The documents are systematically double annotated in various rounds to ensure consistent annotations. The annotated documents are finally harmonized to generate representative consensus annotations. In order to demonstrate an example use case scenario, the corpus was employed to train and validate models for the classification of informative against the non-informative sentences. A Maximum Entropy classifier trained with simple features and evaluated by 10-fold cross-validation resulted in the F1 score of 0.70 indicating a potential useful application of the corpus.\"\n}","description":" ADE-Corpus-V2  Dataset: Adverse Drug Reaction Data.\n This is a dataset for Classification if a sentence is ADE-related (True) or not (False) and Relation Extraction between Adverse Drug Event and Drug.\n DRUG-AE.rel provides relations between drugs and adverse effects.\n DRUG-DOSE.rel provides relations between drugs and dosages.\n ADE-NEG.txt provides all sentences in the ADE corpus that DO NOT contain any drug-related adverse effects.","key":""},{"id":"adversarial_qa","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa","task_ids:open-domain-qa"],"citation":"@article{bartolo2020beat,\n    author = {Bartolo, Max and Roberts, Alastair and Welbl, Johannes and Riedel, Sebastian and Stenetorp, Pontus},\n    title = {Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension},\n    journal = {Transactions of the Association for Computational Linguistics},\n    volume = {8},\n    number = {},\n    pages = {662-678},\n    year = {2020},\n    doi = {10.1162/tacl_a_00338},\n    URL = { https://doi.org/10.1162/tacl_a_00338 },\n    eprint = { https://doi.org/10.1162/tacl_a_00338 },\n    abstract = { Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the annotation process: Humans create questions adversarially, such that the model fails to answer them correctly. In this work we investigate this annotation methodology and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop. This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalization to data collected without a model. We find that training on adversarially collected samples leads to strong generalization to non-adversarially collected datasets, yet with progressive performance deterioration with increasingly stronger models-in-the-loop. Furthermore, we find that stronger models can still learn from datasets collected with substantially weaker models-in-the-loop. When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 39.9F1 on questions that it cannot answer when trained on SQuAD—only marginally lower than when trained on data collected using RoBERTa itself (41.0F1). }\n}","description":"AdversarialQA is a Reading Comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles using an adversarial model-in-the-loop.\nWe use three different models; BiDAF (Seo et al., 2016), BERT-Large (Devlin et al., 2018), and RoBERTa-Large (Liu et al., 2019) in the annotation loop and construct three datasets; D(BiDAF), D(BERT), and D(RoBERTa), each with 10,000 training examples, 1,000 validation, and 1,000 test examples.\nThe adversarial human annotation paradigm ensures that these datasets consist of questions that current state-of-the-art models (at least the ones used as adversaries in the annotation loop) find challenging.","paperswithcode_id":"adversarialqa","key":""},{"id":"aeslc","tags":["languages:en"],"citation":"@misc{zhang2019email,\n    title={This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation},\n    author={Rui Zhang and Joel Tetreault},\n    year={2019},\n    eprint={1906.03497},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"A collection of email messages of employees in the Enron Corporation.\n\nThere are two features:\n  - email_body: email body text.\n  - subject_line: email subject text.","paperswithcode_id":"aeslc","key":""},{"id":"afrikaans_ner_corpus","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:af","licenses:other-Creative Commons Attribution 2.5 South Africa License","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{afrikaans_ner_corpus,\n  author    = {\tGerhard van Huyssteen and\n                Martin Puttkammer and\n                E.B. Trollip and\n                J.C. Liversage and\n              Roald Eiselen},\n  title     = {NCHLT Afrikaans Named Entity Annotated Corpus},\n  booktitle = {Eiselen, R. 2016. Government domain named entity recognition for South African languages. Proceedings of the 10th      Language Resource and Evaluation Conference, Portorož, Slovenia.},\n  year      = {2016},\n  url       = {https://repo.sadilar.org/handle/20.500.12185/299},\n}","description":"Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.","key":""},{"id":"ag_news","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:topic-classification"],"citation":"@inproceedings{Zhang2015CharacterlevelCN,\n  title={Character-level Convolutional Networks for Text Classification},\n  author={Xiang Zhang and Junbo Jake Zhao and Yann LeCun},\n  booktitle={NIPS},\n  year={2015}\n}","description":"AG is a collection of more than 1 million news articles. News articles have been\ngathered from more than 2000 news sources by ComeToMyHead in more than 1 year of\nactivity. ComeToMyHead is an academic news search engine which has been running\nsince July, 2004. The dataset is provided by the academic comunity for research\npurposes in data mining (clustering, classification, etc), information retrieval\n(ranking, search, etc), xml, data compression, data streaming, and any other\nnon-commercial activity. For more information, please refer to the link\nhttp://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n\nThe AG's news topic classification dataset is constructed by Xiang Zhang\n(xiang.zhang@nyu.edu) from the dataset above. It is used as a text\nclassification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann\nLeCun. Character-level Convolutional Networks for Text Classification. Advances\nin Neural Information Processing Systems 28 (NIPS 2015).","paperswithcode_id":"ag-news","key":""},{"id":"ai2_arc","tags":["annotations_creators:found","language_creators:found","languages:en-US","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa","task_ids:multiple-choice-qa"],"citation":"@article{allenai:arc,\n      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and\n                    Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},\n      title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},\n      journal   = {arXiv:1803.05457v1},\n      year      = {2018},\n}","description":"A new dataset of 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in\n advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, where the former contains\n only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. We are also\n including a corpus of over 14 million science sentences relevant to the task, and an implementation of three neural baseline models for this dataset. We pose ARC as a challenge to the community.","key":""},{"id":"air_dialogue","tags":["annotations_creators:human-annotated","language_creators:machine-generated","languages:en","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_ids:conditional-text-generation-other-dialogue-generation","task_ids:dialogue-modeling","task_ids:language-modeling"],"citation":"@inproceedings{wei-etal-2018-airdialogue,\n    title = \"{A}ir{D}ialogue: An Environment for Goal-Oriented Dialogue Research\",\n    author = \"Wei, Wei  and\n      Le, Quoc  and\n      Dai, Andrew  and\n      Li, Jia\",\n    booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\",\n    month = oct # \"-\" # nov,\n    year = \"2018\",\n    address = \"Brussels, Belgium\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D18-1419\",\n    doi = \"10.18653/v1/D18-1419\",\n    pages = \"3844--3854\",\n    abstract = \"Recent progress in dialogue generation has inspired a number of studies on dialogue systems that are capable of accomplishing tasks through natural language interactions. A promising direction among these studies is the use of reinforcement learning techniques, such as self-play, for training dialogue agents. However, current datasets are limited in size, and the environment for training agents and evaluating progress is relatively unsophisticated. We present AirDialogue, a large dataset that contains 301,427 goal-oriented conversations. To collect this dataset, we create a context-generator which provides travel and flight restrictions. We then ask human annotators to play the role of a customer or an agent and interact with the goal of successfully booking a trip given the restrictions. Key to our environment is the ease of evaluating the success of the dialogue, which is achieved by using ground-truth states (e.g., the flight being booked) generated by the restrictions. Any dialogue agent that does not generate the correct states is considered to fail. Our experimental results indicate that state-of-the-art dialogue models can only achieve a score of 0.17 while humans can reach a score of 0.91, which suggests significant opportunities for future improvement.\",\n}","description":"AirDialogue, is a large dataset that contains 402,038 goal-oriented conversations. To collect this dataset, we create a contextgenerator which provides travel and flight restrictions. Then the human annotators are asked to play the role of a customer or an agent and interact with the goal of successfully booking a trip given the restrictions.","key":""},{"id":"ajgt_twitter_ar","tags":["annotations_creators:found","language_creators:found","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{alomari2017arabic,\n  title={Arabic tweets sentimental analysis using machine learning},\n  author={Alomari, Khaled Mohammad and ElSherif, Hatem M and Shaalan, Khaled},\n  booktitle={International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems},\n  pages={602--610},\n  year={2017},\n  organization={Springer}\n}","description":"Arabic Jordanian General Tweets (AJGT) Corpus consisted of 1,800 tweets annotated as positive and negative. Modern Standard Arabic (MSA) or Jordanian dialect.","key":""},{"id":"allegro_reviews","tags":["annotations_creators:found","language_creators:found","languages:pl","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-scoring","task_ids:sentiment-scoring"],"citation":"@inproceedings{rybak-etal-2020-klej,\n    title = \"{KLEJ}: Comprehensive Benchmark for Polish Language Understanding\",\n    author = \"Rybak, Piotr and Mroczkowski, Robert and Tracz, Janusz and Gawlik, Ireneusz\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.111\",\n    pages = \"1191--1201\",\n}","description":"Allegro Reviews is a sentiment analysis dataset, consisting of 11,588 product reviews written in Polish and extracted\nfrom Allegro.pl - a popular e-commerce marketplace. Each review contains at least 50 words and has a rating on a scale\nfrom one (negative review) to five (positive review).\n\nWe recommend using the provided train/dev/test split. The ratings for the test set reviews are kept hidden.\nYou can evaluate your model using the online evaluation tool available on klejbenchmark.com.","paperswithcode_id":"allegro-reviews","key":""},{"id":"allocine","tags":["annotations_creators:no-annotation","language_creators:found","languages:fr","licenses:mit","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@misc{blard2019allocine,\n  author = {Blard, Theophile},\n  title = {french-sentiment-analysis-with-bert},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished={\\\\url{https://github.com/TheophileBlard/french-sentiment-analysis-with-bert}},\n}","description":" Allocine Dataset: A Large-Scale French Movie Reviews Dataset.\n This is a dataset for binary sentiment classification, made of user reviews scraped from Allocine.fr.\n It contains 100k positive and 100k negative reviews divided into 3 balanced splits: train (160k reviews), val (20k) and test (20k).","paperswithcode_id":"allocine","key":""},{"id":"alt","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:bn","languages:en","languages:fil","languages:hi","languages:id","languages:ja","languages:km","languages:lo","languages:ms","languages:my","languages:th","languages:vi","languages:zh","licenses:cc-by-4.0","multilinguality:multilingual","multilinguality:translation","size_categories:10K<n<100K","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_categories:structure-prediction","task_ids:machine-translation","task_ids:parsing"],"citation":"@inproceedings{riza2016introduction,\n  title={Introduction of the asian language treebank},\n  author={Riza, Hammam and Purwoadi, Michael and Uliniansyah, Teduh and Ti, Aw Ai and Aljunied, Sharifah Mahani and Mai, Luong Chi and Thang, Vu Tat and Thai, Nguyen Phuong and Chea, Vichet and Sam, Sethserey and others},\n  booktitle={2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA)},\n  pages={1--6},\n  year={2016},\n  organization={IEEE}\n}","description":"The ALT project aims to advance the state-of-the-art Asian natural language processing (NLP) techniques through the open collaboration for developing and using ALT. It was first conducted by NICT and UCSY as described in Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch and Eiichiro Sumita (2016). Then, it was developed under ASEAN IVO as described in this Web page. The process of building ALT began with sampling about 20,000 sentences from English Wikinews, and then these sentences were translated into the other languages. ALT now has 13 languages: Bengali, English, Filipino, Hindi, Bahasa Indonesia, Japanese, Khmer, Lao, Malay, Myanmar (Burmese), Thai, Vietnamese, Chinese (Simplified Chinese).","paperswithcode_id":"alt","key":""},{"id":"amazon_polarity","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{mcauley2013hidden,\n  title={Hidden factors and hidden topics: understanding rating dimensions with review text},\n  author={McAuley, Julian and Leskovec, Jure},\n  booktitle={Proceedings of the 7th ACM conference on Recommender systems},\n  pages={165--172},\n  year={2013}\n}","description":"The Amazon reviews dataset consists of reviews from amazon.\nThe data span a period of 18 years, including ~35 million reviews up to March 2013.\nReviews include product and user information, ratings, and a plaintext review.","key":""},{"id":"amazon_reviews_multi","tags":["annotations_creators:found","language_creators:found","languages:de","languages:en","languages:es","languages:fr","languages:ja","languages:zh","licenses:other-amazon-license","multilinguality:multilingual","multilinguality:monolingual","size_categories:1M<n<10M","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_categories:text-classification","task_categories:text-scoring","task_ids:language-modeling","task_ids:sentiment-classification","task_ids:sentiment-scoring","task_ids:summarization","task_ids:topic-classification"],"citation":"@inproceedings{marc_reviews,\n    title={The Multilingual Amazon Reviews Corpus},\n    author={Keung, Phillip and Lu, Yichao and Szarvas, György and Smith, Noah A.},\n    booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},\n    year={2020}\n}","description":"We provide an Amazon product reviews dataset for multilingual text classification. The dataset contains reviews in English, Japanese, German, French, Chinese and Spanish, collected between November 1, 2015 and November 1, 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID and the coarse-grained product category (e.g. ‘books’, ‘appliances’, etc.) The corpus is balanced across stars, so each star rating constitutes 20% of the reviews in each language.\n\nFor each language, there are 200,000, 5,000 and 5,000 reviews in the training, development and test sets respectively. The maximum number of reviews per reviewer is 20 and the maximum number of reviews per product is 20. All reviews are truncated after 2,000 characters, and all reviews are at least 20 characters long.\n\nNote that the language of a review does not necessarily match the language of its marketplace (e.g. reviews from amazon.de are primarily written in German, but could also be written in English, etc.). For this reason, we applied a language detection algorithm based on the work in Bojanowski et al. (2017) to determine the language of the review text and we removed reviews that were not written in the expected language.","key":""},{"id":"amazon_us_reviews","tags":["languages:en"],"citation":"\\","description":"Amazon Customer Reviews (a.k.a. Product Reviews) is one of Amazons iconic products. In a period of over two decades since the first review in 1995, millions of Amazon customers have contributed over a hundred million reviews to express opinions and describe their experiences regarding products on the Amazon.com website. This makes Amazon Customer Reviews a rich source of information for academic researchers in the fields of Natural Language Processing (NLP), Information Retrieval (IR), and Machine Learning (ML), amongst others. Accordingly, we are releasing this data to further research in multiple disciplines related to understanding customer product experiences. Specifically, this dataset was constructed to represent a sample of customer evaluations and opinions, variation in the perception of a product across geographical regions, and promotional intent or bias in reviews.\n\nOver 130+ million customer reviews are available to researchers as part of this release. The data is available in TSV files in the amazon-reviews-pds S3 bucket in AWS US East Region. Each line in the data files corresponds to an individual review (tab delimited, with no quote and escape characters).\n\nEach Dataset contains the following columns:\n\n- marketplace: 2 letter country code of the marketplace where the review was written.\n- customer_id: Random identifier that can be used to aggregate reviews written by a single author.\n- review_id: The unique ID of the review.\n- product_id: The unique Product ID the review pertains to. In the multilingual dataset the reviews for the same product in different countries can be grouped by the same product_id.\n- product_parent: Random identifier that can be used to aggregate reviews for the same product.\n- product_title: Title of the product.\n- product_category: Broad product category that can be used to group reviews (also used to group the dataset into coherent parts).\n- star_rating: The 1-5 star rating of the review.\n- helpful_votes: Number of helpful votes.\n- total_votes: Number of total votes the review received.\n- vine: Review was written as part of the Vine program.\n- verified_purchase: The review is on a verified purchase.\n- review_headline: The title of the review.\n- review_body: The review text.\n- review_date: The date the review was written.","key":""},{"id":"ambig_qa","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|natural_questions","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@inproceedings{ min2020ambigqa,\n    title={ {A}mbig{QA}: Answering Ambiguous Open-domain Questions },\n    author={ Min, Sewon and Michael, Julian and Hajishirzi, Hannaneh and Zettlemoyer, Luke },\n    booktitle={ EMNLP },\n    year={2020}\n}","description":"AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous. The types of ambiguity are diverse and sometimes subtle, many of which are only apparent after examining evidence provided by a very large text corpus.  AMBIGNQ, a dataset with\n14,042 annotations on NQ-OPEN questions containing diverse types of ambiguity.\nWe provide two distributions of our new dataset AmbigNQ: a full version with all annotation metadata and a light version with only inputs and outputs.","paperswithcode_id":"ambigqa","key":""},{"id":"amttl","tags":["annotations_creators:crowdsourced","language_creators:found","languages:zh","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:parsing"],"citation":"@inproceedings{xing2018adaptive,\n  title={Adaptive multi-task transfer learning for Chinese word segmentation in medical text},\n  author={Xing, Junjie and Zhu, Kenny and Zhang, Shaodian},\n  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},\n  pages={3619--3630},\n  year={2018}\n}","description":"Chinese word segmentation (CWS) trained from open source corpus faces dramatic performance drop\nwhen dealing with domain text, especially for a domain with lots of special terms and diverse\nwriting styles, such as the biomedical domain. However, building domain-specific CWS requires\nextremely high annotation cost. In this paper, we propose an approach by exploiting domain-invariant\nknowledge from high resource to low resource domains. Extensive experiments show that our mode\nachieves consistently higher accuracy than the single-task CWS and other transfer learning\nbaselines, especially when there is a large disparity between source and target domains.\n\nThis dataset is the accompanied medical Chinese word segmentation (CWS) dataset.\nThe tags are in BIES scheme.\n\nFor more details see https://www.aclweb.org/anthology/C18-1307/","key":""},{"id":"anli","tags":["languages:en"],"citation":"@InProceedings{nie2019adversarial,\n    title={Adversarial NLI: A New Benchmark for Natural Language Understanding},\n    author={Nie, Yixin\n                and Williams, Adina\n                and Dinan, Emily\n                and Bansal, Mohit\n                and Weston, Jason\n                and Kiela, Douwe},\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n}","description":"The Adversarial Natural Language Inference (ANLI) is a new large-scale NLI benchmark dataset,\nThe dataset is collected via an iterative, adversarial human-and-model-in-the-loop procedure.\nANLI is much more difficult than its predecessors including SNLI and MNLI.\nIt contains three rounds. Each round has train/dev/test splits.","paperswithcode_id":"anli","key":""},{"id":"app_reviews","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-scoring","task_ids:sentiment-scoring"],"citation":"@InProceedings{Zurich Open Repository and\nArchive:dataset,\ntitle = {Software Applications User Reviews},\nauthors={Grano, Giovanni; Di Sorbo, Andrea; Mercaldo, Francesco; Visaggio, Corrado A; Canfora, Gerardo;\nPanichella, Sebastiano},\nyear={2017}\n}","description":"It is a large dataset of Android applications belonging to 23 differentapps categories, which provides an overview of the types of feedback users report on the apps and documents the evolution of the related code metrics. The dataset contains about 395 applications of the F-Droid repository, including around 600 versions, 280,000 user reviews (extracted with specific text mining approaches)","key":""},{"id":"aqua_rat","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:expert-generated","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@InProceedings{ACL,\ntitle = {Program induction by rationale generation: Learning to solve and explain algebraic word problems},\nauthors={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},\nyear={2017}\n}","description":"A large-scale dataset consisting of approximately 100,000 algebraic word problems.\nThe solution to each question is explained step-by-step using natural language.\nThis data is used to train a program generation model that learns to generate the explanation,\nwhile generating the program that solves the question.","paperswithcode_id":"aqua-rat","key":""},{"id":"aquamuse","tags":["annotations_creators:crowdsourced","annotations_creators:expert-generated","language_creators:crowdsourced","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|natural_questions","source_datasets:extended|other-Common-Crawl","source_datasets:original","task_categories:other","task_categories:question-answering","task_ids:abstractive-qa","task_ids:extractive-qa","task_ids:other-other-query-based-multi-document-summarization"],"citation":"@misc{kulkarni2020aquamuse,\n      title={AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization},\n      author={Sayali Kulkarni and Sheide Chammas and Wan Zhu and Fei Sha and Eugene Ie},\n      year={2020},\n      eprint={2010.12694},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"AQuaMuSe is a novel scalable approach to automatically mine dual query based multi-document summarization datasets for extractive and abstractive summaries using question answering dataset (Google Natural Questions) and large document corpora (Common Crawl)","paperswithcode_id":"aquamuse","key":""},{"id":"ar_cov19","tags":["annotations_creators:no-annotation","language_creators:found","languages:ar","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:other","task_ids:other-other-data-mining"],"citation":"@article{haouari2020arcov19,\n  title={ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation Networks},\n  author={Fatima Haouari and Maram Hasanain and Reem Suwaileh and Tamer Elsayed},\n  journal={arXiv preprint arXiv:2004.05861},\n  year={2020}","description":"ArCOV-19 is an Arabic COVID-19 Twitter dataset that covers the period from 27th of January till 30th of April 2020. ArCOV-19 is designed to enable research under several domains including natural language processing, information retrieval, and social computing, among others","paperswithcode_id":"arcov-19","key":""},{"id":"ar_res_reviews","tags":["annotations_creators:found","language_creators:found","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@InProceedings{10.1007/978-3-319-18117-2_2,\nauthor=\"ElSahar, Hady\nand El-Beltagy, Samhaa R.\",\neditor=\"Gelbukh, Alexander\",\ntitle=\"Building Large Arabic Multi-domain Resources for Sentiment Analysis\",\nbooktitle=\"Computational Linguistics and Intelligent Text Processing\",\nyear=\"2015\",\npublisher=\"Springer International Publishing\",\naddress=\"Cham\",\npages=\"23--34\",\nisbn=\"978-3-319-18117-2\"\n}","description":"Dataset of 8364 restaurant reviews scrapped from qaym.com in Arabic for sentiment analysis","key":""},{"id":"ar_sarcasm","tags":["annotations_creators:no-annotation","language_creators:found","languages:ar","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other-semeval_2017","source_datasets:extended|other-astd","task_categories:text-classification","task_ids:sentiment-classification","task_ids:text-classification-other-sarcasm-detection"],"citation":"@inproceedings{abu-farha-magdy-2020-arabic,\n    title = \"From {A}rabic Sentiment Analysis to Sarcasm Detection: The {A}r{S}arcasm Dataset\",\n    author = \"Abu Farha, Ibrahim  and Magdy, Walid\",\n    booktitle = \"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resource Association\",\n    url = \"https://www.aclweb.org/anthology/2020.osact-1.5\",\n    pages = \"32--39\",\n    language = \"English\",\n    ISBN = \"979-10-95546-51-1\",\n}","description":"ArSarcasm is a new Arabic sarcasm detection dataset.\nThe dataset was created using previously available Arabic sentiment analysis datasets (SemEval 2017 and ASTD)\n and adds sarcasm and dialect labels to them. The dataset contains 10,547 tweets, 1,682 (16%) of which are sarcastic.","key":""},{"id":"arabic_billion_words","tags":["annotations_creators:found","language_creators:found","languages:ar","licenses:unkown","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@article{el20161,\n  title={1.5 billion words arabic corpus},\n  author={El-Khair, Ibrahim Abu},\n  journal={arXiv preprint arXiv:1611.04033},\n  year={2016}\n}","description":"Abu El-Khair Corpus is an Arabic text corpus, that includes more than five million newspaper articles.\nIt contains over a billion and a half words in total, out of which, there are about three million unique words.\nThe corpus is encoded with two types of encoding, namely: UTF-8, and Windows CP-1256.\nAlso it was marked with two mark-up languages, namely: SGML, and XML.","key":""},{"id":"arabic_pos_dialect","tags":["annotations_creators:expert-generated","language_creators:found","languages:ar","licenses:apache-2.0","multilinguality:multilingual","size_categories:n<1K","source_datasets:extended","task_categories:structure-prediction","task_ids:part-of-speech-tagging"],"citation":"@InProceedings{DARWISH18.562,  author = {Kareem Darwish ,Hamdy Mubarak ,Ahmed Abdelali ,Mohamed Eldesouki ,Younes Samih ,Randah Alharbi ,Mohammed Attia ,Walid Magdy and Laura Kallmeyer},\ntitle = {Multi-Dialect Arabic POS Tagging: A CRF Approach},\nbooktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},\nyear = {2018},\nmonth = {may},\ndate = {7-12},\nlocation = {Miyazaki, Japan},\neditor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},\npublisher = {European Language Resources Association (ELRA)},\naddress = {Paris, France},\nisbn = {979-10-95546-00-9},\nlanguage = {english}\n}","description":"The Dialectal Arabic Datasets contain four dialects of Arabic, Etyptian (EGY), Levantine (LEV), Gulf (GLF), and Maghrebi (MGR). Each dataset consists of a set of 350 manually segmented and POS tagged tweets.","key":""},{"id":"arabic_speech_corpus","tags":["pretty_name:Arabic Speech Corpus","annotations_creators:expert-generated","language_creators:crowdsourced","languages:ar","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:speech-processing","task_ids:automatic-speech-recognition"],"citation":"@phdthesis{halabi2016modern,\n  title={Modern standard Arabic phonetics for speech synthesis},\n  author={Halabi, Nawar},\n  year={2016},\n  school={University of Southampton}\n}","description":"This Speech corpus has been developed as part of PhD work carried out by Nawar Halabi at the University of Southampton.\nThe corpus was recorded in south Levantine Arabic\n(Damascian accent) using a professional studio. Synthesized speech as an output using this corpus has produced a high quality, natural voice.\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .flac format and is not converted to a float32 array. To convert, the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```","paperswithcode_id":"arabic-speech-corpus","key":""},{"id":"arcd","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:ar-SA","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@inproceedings{mozannar-etal-2019-neural,\n    title = {Neural {A}rabic Question Answering},\n    author = {Mozannar, Hussein  and Maamary, Elie  and El Hajal, Karl  and Hajj, Hazem},\n    booktitle = {Proceedings of the Fourth Arabic Natural Language Processing Workshop},\n    month = {aug},\n    year = {2019},\n    address = {Florence, Italy},\n    publisher = {Association for Computational Linguistics},\n    url = {https://www.aclweb.org/anthology/W19-4612},\n    doi = {10.18653/v1/W19-4612},\n    pages = {108--118},\n    abstract = {This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.}\n}","description":" Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions      posed by crowdworkers on Wikipedia articles.","paperswithcode_id":"arcd","key":""},{"id":"arsentd_lev","tags":["annotations_creators:crowdsourced","language_creators:found","languages:apc","languages:apj","licenses:other-Copyright-2018-by-[American-University-of-Beirut]","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification","task_ids:topic-classification"],"citation":"@article{ArSenTDLev2018,\ntitle={ArSentD-LEV: A Multi-Topic Corpus for Target-based Sentiment Analysis in Arabic Levantine Tweets},\nauthor={Baly, Ramy, and Khaddaj, Alaa and Hajj, Hazem and El-Hajj, Wassim and Bashir Shaban, Khaled},\njournal={OSACT3},\npages={},\nyear={2018}}","description":"The Arabic Sentiment Twitter Dataset for Levantine dialect (ArSenTD-LEV) contains 4,000 tweets written in Arabic and equally retrieved from Jordan, Lebanon, Palestine and Syria.","paperswithcode_id":"arsentd-lev","key":""},{"id":"art","tags":["languages:en"],"citation":"@InProceedings{anli,\n  author = {Chandra, Bhagavatula and Ronan, Le Bras and Chaitanya, Malaviya and Keisuke, Sakaguchi and Ari, Holtzman\n    and Hannah, Rashkin and Doug, Downey and Scott, Wen-tau Yih and Yejin, Choi},\n  title = {Abductive Commonsense Reasoning},\n  year = {2020}\n}","description":"the Abductive Natural Language Inference Dataset from AI2","paperswithcode_id":"art-dataset","key":""},{"id":"arxiv_dataset","tags":["annotations_creators:no-annotation","language_creators:expert-generated","languages:en","licenses:cc0-1.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-retrieval","task_ids:document-retrieval","task_ids:entity-linking-retrieval","task_ids:explanation-generation","task_ids:fact-checking-retrieval","task_ids:machine-translation","task_ids:summarization","task_ids:text-simplification"],"citation":"@misc{clement2019arxiv,\n    title={On the Use of ArXiv as a Dataset},\n    author={Colin B. Clement and Matthew Bierbaum and Kevin P. O'Keeffe and Alexander A. Alemi},\n    year={2019},\n    eprint={1905.00075},\n    archivePrefix={arXiv},\n    primaryClass={cs.IR}\n}","description":"A dataset of 1.7 million arXiv articles for applications like trend analysis, paper recommender engines, category prediction, co-citation networks, knowledge graph construction and semantic search interfaces.","key":""},{"id":"ascent_kb","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:other","task_ids:other-other-knowledge-base"],"citation":"@InProceedings{nguyen2021www,\n  title={Advanced Semantics for Commonsense Knowledge Extraction},\n  author={Nguyen, Tuan-Phong and Razniewski, Simon and Weikum, Gerhard},\n  year={2021},\n  booktitle={The Web Conference 2021},\n}","description":"This dataset contains 8.9M commonsense assertions extracted by the Ascent pipeline (https://ascent.mpi-inf.mpg.de/).","paperswithcode_id":"ascentkb","key":""},{"id":"aslg_pc12","tags":["languages:en"],"citation":"@inproceedings{othman2012english,\n  title={English-asl gloss parallel corpus 2012: Aslg-pc12},\n  author={Othman, Achraf and Jemni, Mohamed},\n  booktitle={5th Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon LREC},\n  year={2012}\n}","description":"A large synthetic collection of parallel English and ASL-Gloss texts.\nThere are two string features: text, and gloss.","paperswithcode_id":"aslg-pc12","key":""},{"id":"asnq","tags":["languages:en"],"citation":"@article{garg2019tanda,\n    title={TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection},\n    author={Siddhant Garg and Thuy Vu and Alessandro Moschitti},\n    year={2019},\n    eprint={1911.04118},\n}","description":"ASNQ is a dataset for answer sentence selection derived from\nGoogle's Natural Questions (NQ) dataset (Kwiatkowski et al. 2019).\n\nEach example contains a question, candidate sentence, label indicating whether or not\nthe sentence answers the question, and two additional features --\nsentence_in_long_answer and short_answer_in_sentence indicating whether ot not the\ncandidate sentence is contained in the long_answer and if the short_answer is in the candidate sentence.\n\nFor more details please see\nhttps://arxiv.org/pdf/1911.04118.pdf\n\nand\n\nhttps://research.google/pubs/pub47761/","paperswithcode_id":"asnq","key":""},{"id":"asset","tags":["annotations_creators:machine-generated","language_creators:found","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","source_datasets:extented|other-turkcorpus","task_categories:text-scoring","task_categories:conditional-text-generation","task_ids:text-scoring-other-simplification-evaluation","task_ids:text-simplification"],"citation":"@inproceedings{alva-manchego-etal-2020-asset,\n    title = \"{ASSET}: {A} Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations\",\n    author = \"Alva-Manchego, Fernando  and\n      Martin, Louis  and\n      Bordes, Antoine  and\n      Scarton, Carolina  and\n      Sagot, Benoit  and\n      Specia, Lucia\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.424\",\n    pages = \"4668--4679\",\n}","description":"ASSET is a dataset for evaluating Sentence Simplification systems with multiple rewriting transformations,\nas described in \"ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations\".\nThe corpus is composed of 2000 validation and 359 test original sentences that were each simplified 10 times by different annotators.\nThe corpus also contains human judgments of meaning preservation, fluency and simplicity for the outputs of several automatic text simplification systems.","paperswithcode_id":"asset","key":""},{"id":"assin","tags":["annotations_creators:expert-generated","language_creators:found","languages:pt","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_categories:text-scoring","task_ids:natural-language-inference","task_ids:semantic-similarity-scoring"],"citation":"@inproceedings{fonseca2016assin,\n  title={ASSIN: Avaliacao de similaridade semantica e inferencia textual},\n  author={Fonseca, E and Santos, L and Criscuolo, Marcelo and Aluisio, S},\n  booktitle={Computational Processing of the Portuguese Language-12th International Conference, Tomar, Portugal},\n  pages={13--15},\n  year={2016}\n}","description":"The ASSIN (Avaliação de Similaridade Semântica e INferência textual) corpus is a corpus annotated with pairs of sentences written in\nPortuguese that is suitable for the  exploration of textual entailment and paraphrasing classifiers. The corpus contains pairs of sentences\nextracted from news articles written in European Portuguese (EP) and Brazilian Portuguese (BP), obtained from Google News Portugal\nand Brazil, respectively. To create the corpus, the authors started by collecting a set of news articles describing the\nsame event (one news article from Google News Portugal and another from Google News Brazil) from Google News.\nThen, they employed Latent Dirichlet Allocation (LDA) models to retrieve pairs of similar sentences between sets of news\narticles that were grouped together around the same topic. For that, two LDA models were trained (for EP and for BP)\non external and large-scale collections of unannotated news articles from Portuguese and Brazilian news providers, respectively.\nThen, the authors defined a lower and upper threshold for the sentence similarity score of the retrieved pairs of sentences,\ntaking into account that high similarity scores correspond to sentences that contain almost the same content (paraphrase candidates),\nand low similarity scores correspond to sentences that are very different in content from each other (no-relation candidates).\nFrom the collection of pairs of sentences obtained at this stage, the authors performed some manual grammatical corrections\nand discarded some of the pairs wrongly retrieved. Furthermore, from a preliminary analysis made to the retrieved sentence pairs\nthe authors noticed that the number of contradictions retrieved during the previous stage was very low. Additionally, they also\nnoticed that event though paraphrases are not very frequent, they occur with some frequency in news articles. Consequently,\nin contrast with the majority of the currently available corpora for other languages, which consider as labels “neutral”, “entailment”\nand “contradiction” for the task of RTE, the authors of the ASSIN corpus decided to use as labels “none”, “entailment” and “paraphrase”.\nFinally, the manual annotation of pairs of sentences was performed by human annotators. At least four annotators were randomly\nselected to annotate each pair of sentences, which is done in two steps: (i) assigning a semantic similarity label (a score between 1 and 5,\nfrom unrelated to very similar); and (ii) providing an entailment label (one sentence entails the other, sentences are paraphrases,\nor no relation). Sentence pairs where at least three annotators do not agree on the entailment label were considered controversial\nand thus discarded from the gold standard annotations. The full dataset has 10,000 sentence pairs, half of which in Brazilian Portuguese\nand half in European Portuguese. Either language variant has 2,500 pairs for training, 500 for validation and 2,000 for testing.","paperswithcode_id":"assin","key":""},{"id":"assin2","tags":["annotations_creators:expert-generated","language_creators:found","languages:pt","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_categories:text-scoring","task_ids:natural-language-inference","task_ids:semantic-similarity-scoring"],"citation":"@inproceedings{real2020assin,\n  title={The assin 2 shared task: a quick overview},\n  author={Real, Livy and Fonseca, Erick and Oliveira, Hugo Goncalo},\n  booktitle={International Conference on Computational Processing of the Portuguese Language},\n  pages={406--412},\n  year={2020},\n  organization={Springer}\n}","description":"The ASSIN 2 corpus is composed of rather simple sentences. Following the procedures of SemEval 2014 Task 1.\nThe training and validation data are composed, respectively, of 6,500 and 500 sentence pairs in Brazilian Portuguese,\nannotated for entailment and semantic similarity. Semantic similarity values range from 1 to 5, and text entailment\nclasses are either entailment or none. The test data are composed of approximately 3,000 sentence pairs with the same\nannotation. All data were manually annotated.","paperswithcode_id":"assin2","key":""},{"id":"atomic","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:other-stuctured-to-text"],"citation":"@article{Sap2019ATOMICAA,\n  title={ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning},\n  author={Maarten Sap and Ronan Le Bras and Emily Allaway and Chandra Bhagavatula and Nicholas Lourie and Hannah Rashkin and Brendan Roof and Noah A. Smith and Yejin Choi},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1811.00146}\n}","description":"This dataset provides the template sentences and\nrelationships defined in the ATOMIC common sense dataset. There are\nthree splits - train, test, and dev.\n\nFrom the authors.\n\nDisclaimer/Content warning: the events in atomic have been\nautomatically extracted from blogs, stories and books written at\nvarious times. The events might depict violent or problematic actions,\nwhich we left in the corpus for the sake of learning the (probably\nnegative but still important) commonsense implications associated with\nthe events. We removed a small set of truly out-dated events, but\nmight have missed some so please email us (msap@cs.washington.edu) if\nyou have any concerns.","paperswithcode_id":"atomic","key":""},{"id":"autshumato","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","languages:ts","languages:tn","languages:zu","licenses:cc-by-2.5","multilinguality:multilingual","size_categories:100K<n<1M","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@article{groenewald2010processing,\n  title={Processing parallel text corpora for three South African language pairs in the Autshumato project},\n  author={Groenewald, Hendrik J and du Plooy, Liza},\n  journal={AfLaT 2010},\n  pages={27},\n  year={2010}\n}","description":"Multilingual information access is stipulated in the South African constitution. In practise, this\nis hampered by a lack of resources and capacity to perform the large volumes of translation\nwork required to realise multilingual information access. One of the aims of the Autshumato\nproject is to develop machine translation systems for three South African languages pairs.","key":""},{"id":"babi_qa","tags":["annotations_creators:machine-generated","language_creators:machine-generated","languages:en","licenses:cc-by-3.0","multilinguality:monolingual","size_categories:n<1K","size_categories:1K<n<10K","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:question-answering-other-chained-qa"],"citation":"@misc{weston2015aicomplete,\n      title={Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks},\n      author={Jason Weston and Antoine Bordes and Sumit Chopra and Alexander M. Rush and Bart van Merriënboer and Armand Joulin and Tomas Mikolov},\n      year={2015},\n      eprint={1502.05698},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}","description":"The (20) QA bAbI tasks are a set of proxy tasks that evaluate reading\ncomprehension via question answering. Our tasks measure understanding\nin several ways: whether a system is able to answer questions via chaining facts,\nsimple induction, deduction and many more. The tasks are designed to be prerequisites\nfor any system that aims to be capable of conversing with a human.\nThe aim is to classify these tasks into skill sets,so that researchers\ncan identify (and then rectify)the failings of their systems.","paperswithcode_id":"babi-1","key":""},{"id":"banking77","tags":["annotations_creators:expert-generated","extended:original","language_creators:expert-generated","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification","task_ids:multi-class-classification"],"citation":"@inproceedings{Casanueva2020,\n    author      = {I{\\~{n}}igo Casanueva and Tadas Temcinas and Daniela Gerz and Matthew Henderson and Ivan Vulic},\n    title       = {Efficient Intent Detection with Dual Sentence Encoders},\n    year        = {2020},\n    month       = {mar},\n    note        = {Data available at https://github.com/PolyAI-LDN/task-specific-datasets},\n    url         = {https://arxiv.org/abs/2003.04807},\n    booktitle   = {Proceedings of the 2nd Workshop on NLP for ConvAI - ACL 2020}\n}","description":"BANKING77 dataset provides a very fine-grained set of intents in a banking domain.\nIt comprises 13,083 customer service queries labeled with 77 intents.\nIt focuses on fine-grained single-domain intent detection.","key":""},{"id":"bbaw_egyptian","tags":["annotations_creators:expert-generated","language_creators:found","languages:de","languages:en","languages:egy","licenses:cc-by-4.0","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:extended|wikipedia","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@misc{OPUS4-2919,\ntitle  = {Teilauszug der Datenbank des Vorhabens \"Strukturen und Transformationen des Wortschatzes der {\\\"a}gyptischen Sprache\" vom Januar 2018},\ninstitution = {Akademienvorhaben Strukturen und Transformationen des Wortschatzes der {\\\"a}gyptischen Sprache. Text- und Wissenskultur im alten {\\\"A}gypten},\ntype = {other},\nyear = {2018},\n}","description":"This dataset comprises parallel sentences of hieroglyphic encodings, transcription and translation\nas used in the paper Multi-Task Modeling of Phonographic Languages: Translating Middle Egyptian\nHieroglyph. The data triples are extracted from the digital corpus of Egyptian texts compiled by\nthe project \"Strukturen und Transformationen des Wortschatzes der ägyptischen Sprache\".","key":""},{"id":"bbc_hindi_nli","tags":["annotations_creators:machine-generated","language_creators:found","languages:hi","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|bbc__hindi_news_classification","task_categories:text-classification","task_ids:natural-language-inference"],"citation":"    @inproceedings{uppal-etal-2020-two,\n    title = \"Two-Step Classification using Recasted Data for Low Resource Settings\",\n    author = \"Uppal, Shagun  and\n      Gupta, Vivek  and\n      Swaminathan, Avinash  and\n      Zhang, Haimin  and\n      Mahata, Debanjan  and\n      Gosangi, Rakesh  and\n      Shah, Rajiv Ratn  and\n      Stent, Amanda\",\n    booktitle = \"Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing\",\n    month = dec,\n    year = \"2020\",\n    address = \"Suzhou, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.aacl-main.71\",\n    pages = \"706--719\",\n    abstract = \"An NLP model{'}s ability to reason should be independent of language. Previous works utilize Natural Language Inference (NLI) to understand the reasoning ability of models, mostly focusing on high resource languages like English. To address scarcity of data in low-resource languages such as Hindi, we use data recasting to create NLI datasets for four existing text classification datasets. Through experiments, we show that our recasted dataset is devoid of statistical irregularities and spurious patterns. We further study the consistency in predictions of the textual entailment models and propose a consistency regulariser to remove pairwise-inconsistencies in predictions. We propose a novel two-step classification method which uses textual-entailment predictions for classification task. We further improve the performance by using a joint-objective for classification and textual entailment. We therefore highlight the benefits of data recasting and improvements on classification performance using our approach with supporting experimental results.\",\n}","description":"This dataset is used to train models for Natural Language Inference Tasks in Low-Resource Languages like Hindi.","key":""},{"id":"bc2gm_corpus","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@article{smith2008overview,\n        title={Overview of BioCreative II gene mention recognition},\n        author={Smith, Larry and Tanabe, Lorraine K and nee Ando, Rie Johnson and Kuo, Cheng-Ju and Chung, I-Fang and Hsu, Chun-Nan and Lin, Yu-Shi and Klinger, Roman and Friedrich, Christoph M and Ganchev, Kuzman and others},\n        journal={Genome biology},\n        volume={9},\n        number={S2},\n        pages={S2},\n        year={2008},\n        publisher={Springer}\n}","description":"Nineteen teams presented results for the Gene Mention Task at the BioCreative II Workshop.\nIn this task participants designed systems to identify substrings in sentences corresponding to gene name mentions.\nA variety of different methods were used and the results varied with a highest achieved F1 score of 0.8721.\nHere we present brief descriptions of all the methods used and a statistical analysis of the results.\nWe also demonstrate that, by combining the results from all submissions, an F score of 0.9066 is feasible,\nand furthermore that the best result makes use of the lowest scoring submissions.\n\nFor more details, see: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2559986/\n\nThe original dataset can be downloaded from: https://biocreative.bioinformatics.udel.edu/resources/corpora/biocreative-ii-corpus/\nThis dataset has been converted to CoNLL format for NER using the following tool: https://github.com/spyysalo/standoff2conll","key":""},{"id":"best2009","tags":["annotations_creators:expert-generated","language_creators:found","languages:th","licenses:cc-by-nc-sa-3.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:structure-prediction","task_ids:structure-prediction-other-word-tokenization"],"citation":"@inproceedings{kosawat2009best,\n  title={BEST 2009: Thai word segmentation software contest},\n  author={Kosawat, Krit and Boriboon, Monthika and Chootrakool, Patcharika and Chotimongkol, Ananlada and Klaithin, Supon and Kongyoung, Sarawoot and Kriengket, Kanyanut and Phaholphinyo, Sitthaa and Purodakananda, Sumonmas and Thanakulwarapas, Tipraporn and others},\n  booktitle={2009 Eighth International Symposium on Natural Language Processing},\n  pages={83--88},\n  year={2009},\n  organization={IEEE}\n}\n@inproceedings{boriboon2009best,\n  title={Best corpus development and analysis},\n  author={Boriboon, Monthika and Kriengket, Kanyanut and Chootrakool, Patcharika and Phaholphinyo, Sitthaa and Purodakananda, Sumonmas and Thanakulwarapas, Tipraporn and Kosawat, Krit},\n  booktitle={2009 International Conference on Asian Language Processing},\n  pages={322--327},\n  year={2009},\n  organization={IEEE}\n}","description":"`best2009` is a Thai word-tokenization dataset from encyclopedia, novels, news and articles by\n[NECTEC](https://www.nectec.or.th/) (148,995/2,252 lines of train/test). It was created for\n[BEST 2010: Word Tokenization Competition](https://thailang.nectec.or.th/archive/indexa290.html?q=node/10).\nThe test set answers are not provided publicly.","key":""},{"id":"bianet","tags":["annotations_creators:found","language_creators:found","languages:en","languages:ku","languages:tr","licenses:unknown","multilinguality:translation","size_categories:1K<n<10K","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{ATAMAN18.6,\n  author = {Duygu Ataman},\n  title = {Bianet: A Parallel News Corpus in Turkish, Kurdish and English},\n  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},\n  year = {2018},\n  month = {may},\n  date = {7-12},\n  location = {Miyazaki, Japan},\n  editor = {Jinhua Du and Mihael Arcan and Qun Liu and Hitoshi Isahara},\n  publisher = {European Language Resources Association (ELRA)},\n  address = {Paris, France},\n  isbn = {979-10-95546-15-3},\n  language = {english}\n  }","description":"A parallel news corpus in Turkish, Kurdish and English.\nBianet collects 3,214 Turkish articles with their sentence-aligned Kurdish or English translations from the Bianet online newspaper.\n3 languages, 3 bitexts\ntotal number of files: 6\ntotal number of tokens: 2.25M\ntotal number of sentence fragments: 0.14M","paperswithcode_id":"bianet","key":""},{"id":"bible_para","tags":["annotations_creators:found","language_creators:found","languages:acu","languages:af","languages:agr","languages:ake","languages:am","languages:amu","languages:ar","languages:bg","languages:bsn","languages:cak","languages:ceb","languages:ch","languages:chq","languages:chr","languages:cjp","languages:cni","languages:cop","languages:crp","languages:cs","languages:da","languages:de","languages:dik","languages:dje","languages:djk","languages:dop","languages:ee","languages:el","languages:en","languages:eo","languages:es","languages:et","languages:eu","languages:fi","languages:fr","languages:gbi","languages:gd","languages:gu","languages:gv","languages:he","languages:hi","languages:hr","languages:hu","languages:hy","languages:id","languages:is","languages:it","languages:jak","languages:jap","languages:jiv","languages:kab","languages:kbh","languages:kek","languages:kn","languages:ko","languages:la","languages:lt","languages:lv","languages:mam","languages:mi","languages:ml","languages:mr","languages:my","languages:ne","languages:nhg","languages:nl","languages:no","languages:ojb","languages:pck","languages:pes","languages:pl","languages:plt","languages:pot","languages:ppk","languages:pt","languages:quc","languages:quw","languages:ro","languages:rom","languages:ru","languages:shi","languages:sk","languages:sl","languages:sn","languages:so","languages:sq","languages:sr","languages:ss","languages:sv","languages:syr","languages:te","languages:th","languages:tl","languages:tmh","languages:tr","languages:uk","languages:usp","languages:vi","languages:wal","languages:wo","languages:xh","languages:zh","languages:zu","licenses:cc0-1.0","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"OPUS and A massively parallel corpus: the Bible in 100 languages, Christos Christodoulopoulos and Mark Steedman, *Language Resources and Evaluation*, 49 (2)","description":"This is a multilingual parallel corpus created from translations of the Bible compiled by Christos Christodoulopoulos and Mark Steedman.\n\n102 languages, 5,148 bitexts\ntotal number of files: 107\ntotal number of tokens: 56.43M\ntotal number of sentence fragments: 2.84M","key":""},{"id":"big_patent","tags":["annotations_creators:no-annotation","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:1M<n<10M","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@misc{sharma2019bigpatent,\n    title={BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization},\n    author={Eva Sharma and Chen Li and Lu Wang},\n    year={2019},\n    eprint={1906.03741},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"BIGPATENT, consisting of 1.3 million records of U.S. patent documents\nalong with human written abstractive summaries.\nEach US patent application is filed under a Cooperative Patent Classification\n(CPC) code. There are nine such classification categories:\nA (Human Necessities), B (Performing Operations; Transporting),\nC (Chemistry; Metallurgy), D (Textiles; Paper), E (Fixed Constructions),\nF (Mechanical Engineering; Lightning; Heating; Weapons; Blasting),\nG (Physics), H (Electricity), and\nY (General tagging of new or cross-sectional technology)\nThere are two features:\n  - description: detailed description of patent.\n  - abstract: Patent abastract.","paperswithcode_id":"bigpatent","key":""},{"id":"billsum","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@misc{kornilova2019billsum,\n    title={BillSum: A Corpus for Automatic Summarization of US Legislation},\n    author={Anastassia Kornilova and Vlad Eidelman},\n    year={2019},\n    eprint={1910.00523},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"BillSum, summarization of US Congressional and California state bills.\n\nThere are several features:\n  - text: bill text.\n  - summary: summary of the bills.\n  - title: title of the bills.\nfeatures for us bills. ca bills does not have.\n  - text_len: number of chars in text.\n  - sum_len: number of chars in summary.","paperswithcode_id":"billsum","key":""},{"id":"bing_coronavirus_query_set","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:o-uda-1.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:intent-classification"],"description":"This dataset was curated from the Bing search logs (desktop users only) over the period of Jan 1st, 2020 – (Current Month - 1). Only searches that were issued many times by multiple users were included. The dataset includes queries from all over the world that had an intent related to the Coronavirus or Covid-19. In some cases this intent is explicit in the query itself (e.g., “Coronavirus updates Seattle”), in other cases it is implicit , e.g. “Shelter in place”. The implicit intent of search queries (e.g., “Toilet paper”) was extracted using random walks on the click graph as outlined in this paper by Microsoft Research. All personal data were removed.","key":""},{"id":"biomrc","tags":["languages:en"],"citation":"@inproceedings{pappas-etal-2020-biomrc,\n    title = \"{B}io{MRC}: A Dataset for Biomedical Machine Reading Comprehension\",\n    author = \"Pappas, Dimitris  and\n      Stavropoulos, Petros  and\n      Androutsopoulos, Ion  and\n      McDonald, Ryan\",\n    booktitle = \"Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.bionlp-1.15\",\n    pages = \"140--149\",\n    abstract = \"We introduce BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.\",\n}","description":"We introduce BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.","paperswithcode_id":"biomrc","key":""},{"id":"blended_skill_talk","tags":["languages:en"],"citation":"@misc{smith2020evaluating,\n    title={Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills},\n    author={Eric Michael Smith and Mary Williamson and Kurt Shuster and Jason Weston and Y-Lan Boureau},\n    year={2020},\n    eprint={2004.08449},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"A dataset of 7k conversations explicitly designed to exhibit multiple conversation modes: displaying personality, having empathy, and demonstrating knowledge.","paperswithcode_id":"blended-skill-talk","key":""},{"id":"blimp","tags":["languages:en"],"citation":"@article{warstadt2019blimp,\n  title={BLiMP: A Benchmark of Linguistic Minimal Pairs for English},\n  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei, and Wang, Sheng-Fu and Bowman, Samuel R},\n  journal={arXiv preprint arXiv:1912.00582},\n  year={2019}\n}","description":"BLiMP is a challenge set for evaluating what language models (LMs) know about\nmajor grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each\ncontaining 1000 minimal pairs isolating specific contrasts in syntax,\nmorphology, or semantics. The data is automatically generated according to\nexpert-crafted grammars.","paperswithcode_id":"blimp","key":""},{"id":"blog_authorship_corpus","tags":["languages:en"],"citation":"@inproceedings{schler2006effects,\n    title={Effects of age and gender on blogging.},\n    author={Schler, Jonathan and Koppel, Moshe and Argamon, Shlomo and Pennebaker, James W},\n    booktitle={AAAI spring symposium: Computational approaches to analyzing weblogs},\n    volume={6},\n    pages={199--205},\n    year={2006}\n}","description":"The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.\n\nEach blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n\nAll bloggers included in the corpus fall into one of three age groups:\n\n·          8240 \"10s\" blogs (ages 13-17),\n\n·          8086 \"20s\" blogs(ages 23-27)\n\n·          2994 \"30s\" blogs (ages 33-47).\n\nFor each age group there are an equal number of male and female bloggers.\n\nEach blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.\n\nThe corpus may be freely used for non-commercial research purposes","paperswithcode_id":"blog-authorship-corpus","key":""},{"id":"bn_hate_speech","tags":["annotations_creators:crowdsourced","annotations_creators:expert-generated","language_creators:found","languages:bn","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-hate-speech-topic-classification"],"citation":"@misc{karim2020classification,\n      title={Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network},\n      author={Md. Rezaul Karim and Bharathi Raja Chakravarthi and John P. McCrae and Michael Cochez},\n      year={2020},\n      eprint={2004.07807},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"The Bengali Hate Speech Dataset is a collection of Bengali articles collected from Bengali news articles,\nnews dump of Bengali TV channels, books, blogs, and social media. Emphasis was placed on Facebook pages and\nnewspaper sources because they attract close to 50 million followers and is a common source of opinions\nand hate speech. The raw text corpus contains 250 million articles and the full dataset is being prepared\nfor release. This is a subset of the full dataset.\n\nThis dataset was prepared for hate-speech text classification benchmark on Bengali, an under-resourced language.","paperswithcode_id":"bengali-hate-speech","key":""},{"id":"bookcorpus","tags":["languages:en"],"citation":"@InProceedings{Zhu_2015_ICCV,\n    title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},\n    author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},\n    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},\n    month = {December},\n    year = {2015}\n}","description":"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story.This work aims to align books to their movie releases in order to providerich descriptive explanations for visual content that go semantically farbeyond the captions available in current datasets. \\","paperswithcode_id":"bookcorpus","key":""},{"id":"bookcorpusopen","tags":["languages:en"],"citation":"@InProceedings{Zhu_2015_ICCV,\n    title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},\n    author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},\n    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},\n    month = {December},\n    year = {2015}\n}","description":"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story.\nThis version of bookcorpus has 17868 dataset items (books). Each item contains two fields: title and text. The title is the name of the book (just the file name) while text contains unprocessed book text. The bookcorpus has been prepared by Shawn Presser and is generously hosted by The-Eye. The-Eye is a non-profit, community driven platform dedicated to the archiving and long-term preservation of any and all data including but by no means limited to... websites, books, games, software, video, audio, other digital-obscura and ideas.","key":""},{"id":"boolq","tags":["languages:en"],"citation":"@inproceedings{clark2019boolq,\n  title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},\n  author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei, and Kwiatkowski, Tom and Collins, Michael, and Toutanova, Kristina},\n  booktitle = {NAACL},\n  year =      {2019},\n}","description":"BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally\noccurring ---they are generated in unprompted and unconstrained settings.\nEach example is a triplet of (question, passage, answer), with the title of the page as optional additional context.\nThe text-pair classification setup is similar to existing natural language inference tasks.","paperswithcode_id":"boolq","key":""},{"id":"bprec","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:pl","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-retrieval","task_ids:entity-linking-retrieval"],"citation":"@inproceedings{inproceedings,\nauthor = {Janz, Arkadiusz and Kopociński, Łukasz and Piasecki, Maciej and Pluwak, Agnieszka},\nyear = {2020},\nmonth = {05},\npages = {},\ntitle = {Brand-Product Relation Extraction Using Heterogeneous Vector Space Representations}\n}","description":"Dataset consisting of Polish language texts annotated to recognize brand-product relations.","key":""},{"id":"break_data","tags":["languages:en"],"citation":"@article{Wolfson2020Break,\n  title={Break It Down: A Question Understanding Benchmark},\n  author={Wolfson, Tomer and Geva, Mor and Gupta, Ankit and Gardner, Matt and Goldberg, Yoav and Deutch, Daniel and Berant, Jonathan},\n  journal={Transactions of the Association for Computational Linguistics},\n  year={2020},\n}","description":"Break is a human annotated dataset of natural language questions and their Question Decomposition Meaning Representations\n(QDMRs). Break consists of 83,978 examples sampled from 10 question answering datasets over text, images and databases.\nThis repository contains the Break dataset along with information on the exact data format.","paperswithcode_id":"break","key":""},{"id":"brwac","tags":["annotations_creators:no-annotation","language_creators:found","languages:pt","licenses:unknown","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@inproceedings{wagner2018brwac,\n  title={The brwac corpus: A new open resource for brazilian portuguese},\n  author={Wagner Filho, Jorge A and Wilkens, Rodrigo and Idiart, Marco and Villavicencio, Aline},\n  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},\n  year={2018}\n}","description":"The BrWaC (Brazilian Portuguese Web as Corpus) is a large corpus constructed following the Wacky framework,\nwhich was made public for research purposes. The current corpus version, released in January 2017, is composed by\n3.53 million documents, 2.68 billion tokens and 5.79 million types. Please note that this resource is available\nsolely for academic research purposes, and you agreed not to use it for any commercial applications.\nManually download at https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC","paperswithcode_id":"brwac","key":""},{"id":"bsd_ja_en","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","languages:ja","licenses:cc-by-nc-sa-4.0","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{rikters-etal-2019-designing,\n    title = \"Designing the Business Conversation Corpus\",\n    author = \"Rikters, Matīss  and\n      Ri, Ryokan  and\n      Li, Tong  and\n      Nakazawa, Toshiaki\",\n    booktitle = \"Proceedings of the 6th Workshop on Asian Translation\",\n    month = nov,\n    year = \"2019\",\n    address = \"Hong Kong, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D19-5204\",\n    doi = \"10.18653/v1/D19-5204\",\n    pages = \"54--61\"\n}","description":"This is the Business Scene Dialogue (BSD) dataset,\na Japanese-English parallel corpus containing written conversations\nin various business scenarios.\n\nThe dataset was constructed in 3 steps:\n  1) selecting business scenes,\n  2) writing monolingual conversation scenarios according to the selected scenes, and\n  3) translating the scenarios into the other language.\n\nHalf of the monolingual scenarios were written in Japanese\nand the other half were written in English.\n\nFields:\n- id: dialogue identifier\n- no: sentence pair number within a dialogue\n- en_speaker: speaker name in English\n- ja_speaker: speaker name in Japanese\n- en_sentence: sentence in English\n- ja_sentence: sentence in Japanese\n- original_language: language in which monolingual scenario was written\n- tag: scenario\n- title: scenario title","paperswithcode_id":"business-scene-dialogue","key":""},{"id":"bswac","tags":["annotations_creators:no-annotation","language_creators:found","languages:bs","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:100M<n<1B","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@misc{11356/1062,\n title = {Bosnian web corpus {bsWaC} 1.1},\n author = {Ljube{\\v s}i{\\'c}, Nikola and Klubi{\\v c}ka, Filip},\n url = {http://hdl.handle.net/11356/1062},\n note = {Slovenian language resource repository {CLARIN}.{SI}},\n copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},\n year = {2016} }","description":"The Bosnian web corpus bsWaC was built by crawling the .ba top-level domain in 2014. The corpus was near-deduplicated on paragraph level, normalised via diacritic restoration, morphosyntactically annotated and lemmatised. The corpus is shuffled by paragraphs. Each paragraph contains metadata on the URL, domain and language identification (Bosnian vs. Croatian vs. Serbian).\n\nVersion 1.0 of this corpus is described in http://www.aclweb.org/anthology/W14-0405. Version 1.1 contains newer and better linguistic annotations.","key":""},{"id":"c3","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:zh","licenses:other-non-commercial-research","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@article{sun2019investigating,\n  title={Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension},\n  author={Sun, Kai and Yu, Dian and Yu, Dong and Cardie, Claire},\n  journal={Transactions of the Association for Computational Linguistics},\n  year={2020},\n  url={https://arxiv.org/abs/1904.09679v3}\n}","description":"Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C^3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations.\nWe present a comprehensive analysis of the prior knowledge (i.e., linguistic, domain-specific, and general world knowledge) needed for these real-world problems. We implement rule-based and popular neural methods and find that there is still a significant performance gap between the best performing model (68.5%) and human readers (96.0%), especially on problems that require prior knowledge. We further study the effects of distractor plausibility and data augmentation based on translated relevant datasets for English on model performance. We expect C^3 to present great challenges to existing systems as answering 86.8% of questions requires both knowledge within and beyond the accompanying document, and we hope that C^3 can serve as a platform to study how to leverage various kinds of prior knowledge to better understand a given written or orally oriented text.","paperswithcode_id":"c3","key":""},{"id":"c4","tags":[],"citation":"@article{2019t5,\n    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n    journal = {arXiv e-prints},\n    year = {2019},\n    archivePrefix = {arXiv},\n    eprint = {1910.10683},\n}","description":"A colossal, cleaned version of Common Crawl's web crawl corpus.\n\nBased on Common Crawl dataset: \"https://commoncrawl.org\".\n\nThis is the processed version of Google's C4 dataset by AllenAI.","key":""},{"id":"cail2018","tags":["annotations_creators:found","language_creators:found","languages:zh","licenses:unknown","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:other","task_ids:other-other-judgement-prediction"],"citation":"@misc{xiao2018cail2018,\n      title={CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction},\n      author={Chaojun Xiao and Haoxi Zhong and Zhipeng Guo and Cunchao Tu and Zhiyuan Liu and Maosong Sun and Yansong Feng and Xianpei Han and Zhen Hu and Heng Wang and Jianfeng Xu},\n      year={2018},\n      eprint={1807.02478},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"In this paper, we introduce Chinese AI and Law challenge dataset (CAIL2018),\nthe first large-scale Chinese legal dataset for judgment prediction. CAIL contains more than 2.6 million\ncriminal cases published by the Supreme People's Court of China, which are several times larger than other\ndatasets in existing works on judgment prediction. Moreover, the annotations of judgment results are more\ndetailed and rich. It consists of applicable law articles, charges, and prison terms, which are expected\nto be inferred according to the fact descriptions of cases. For comparison, we implement several conventional\ntext classification baselines for judgment prediction and experimental results show that it is still a\nchallenge for current models to predict the judgment results of legal cases, especially on prison terms.\nTo help the researchers make improvements on legal judgment prediction.","paperswithcode_id":"chinese-ai-and-law-cail-2018","key":""},{"id":"caner","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:ar","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@article{article,\nauthor = {Salah, Ramzi and Zakaria, Lailatul},\nyear = {2018},\nmonth = {12},\npages = {},\ntitle = {BUILDING THE CLASSICAL ARABIC NAMED ENTITY RECOGNITION CORPUS (CANERCORPUS)},\nvolume = {96},\njournal = {Journal of Theoretical and Applied Information Technology}\n}","description":"Classical Arabic Named Entity Recognition corpus as a new corpus of tagged data that can be useful for handling the issues in recognition of Arabic named entities.","key":""},{"id":"capes","tags":["annotations_creators:found","language_creators:found","languages:en","languages:pt","licenses:unknown","multilinguality:multilingual","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{soares2018parallel,\n  title={A Parallel Corpus of Theses and Dissertations Abstracts},\n  author={Soares, Felipe and Yamashita, Gabrielli Harumi and Anzanello, Michel Jose},\n  booktitle={International Conference on Computational Processing of the Portuguese Language},\n  pages={345--352},\n  year={2018},\n  organization={Springer}\n}","description":"A parallel corpus of theses and dissertations abstracts in English and Portuguese were collected from the CAPES website (Coordenação de Aperfeiçoamento de Pessoal de Nível Superior) - Brazil. The corpus is sentence aligned for all language pairs. Approximately 240,000 documents were collected and aligned using the Hunalign algorithm.","paperswithcode_id":"capes","key":""},{"id":"catalonia_independence","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:ca","languages:es","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-stance-detection"],"citation":"@inproceedings{zotova-etal-2020-multilingual,\n    title = \"Multilingual Stance Detection in Tweets: The {C}atalonia Independence Corpus\",\n    author = \"Zotova, Elena  and\n      Agerri, Rodrigo  and\n      Nunez, Manuel  and\n      Rigau, German\",\n    booktitle = \"Proceedings of the 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.171\",\n    pages = \"1368--1375\",\n    abstract = \"Stance detection aims to determine the attitude of a given text with respect to a specific topic or claim. While stance detection has been fairly well researched in the last years, most the work has been focused on English. This is mainly due to the relative lack of annotated data in other languages. The TW-10 referendum Dataset released at IberEval 2018 is a previous effort to provide multilingual stance-annotated data in Catalan and Spanish. Unfortunately, the TW-10 Catalan subset is extremely imbalanced. This paper addresses these issues by presenting a new multilingual dataset for stance detection in Twitter for the Catalan and Spanish languages, with the aim of facilitating research on stance detection in multilingual and cross-lingual settings. The dataset is annotated with stance towards one topic, namely, the ndependence of Catalonia. We also provide a semi-automatic method to annotate the dataset based on a categorization of Twitter users. We experiment on the new corpus with a number of supervised approaches, including linear classifiers and deep learning methods. Comparison of our new corpus with the with the TW-1O dataset shows both the benefits and potential of a well balanced corpus for multilingual and cross-lingual research on stance detection. Finally, we establish new state-of-the-art results on the TW-10 dataset, both for Catalan and Spanish.\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}","description":"This dataset contains two corpora in Spanish and Catalan that consist of annotated Twitter messages for automatic stance detection. The data was collected over 12 days during February and March of 2019 from tweets posted in Barcelona, and during September of 2018 from tweets posted in the town of Terrassa, Catalonia.\n\nEach corpus is annotated with three classes: AGAINST, FAVOR and NEUTRAL, which express the stance towards the target - independence of Catalonia.","paperswithcode_id":"cic","key":""},{"id":"cawac","tags":["annotations_creators:no-annotation","language_creators:found","languages:ca","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:10M<n<100M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@inproceedings{DBLP:conf/lrec/LjubesicT14,\n  author    = {Nikola Ljubesic and\n               Antonio Toral},\n  editor    = {Nicoletta Calzolari and\n               Khalid Choukri and\n               Thierry Declerck and\n               Hrafn Loftsson and\n               Bente Maegaard and\n               Joseph Mariani and\n               Asunci{\\'{o}}n Moreno and\n               Jan Odijk and\n               Stelios Piperidis},\n  title     = {caWaC - {A} web corpus of Catalan and its application to language\n               modeling and machine translation},\n  booktitle = {Proceedings of the Ninth International Conference on Language Resources\n               and Evaluation, {LREC} 2014, Reykjavik, Iceland, May 26-31, 2014},\n  pages     = {1728--1732},\n  publisher = {European Language Resources Association {(ELRA)}},\n  year      = {2014},\n  url       = {http://www.lrec-conf.org/proceedings/lrec2014/summaries/841.html},\n  timestamp = {Mon, 19 Aug 2019 15:23:35 +0200},\n  biburl    = {https://dblp.org/rec/conf/lrec/LjubesicT14.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"caWaC is a 780-million-token web corpus of Catalan built from the .cat top-level-domain in late 2013.","paperswithcode_id":"cawac","key":""},{"id":"cbt","tags":["annotations_creators:machine-generated","language_creators:found","languages:en","licenses:gfdl-1.3","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:n<1K","source_datasets:original","task_categories:question-answering","task_categories:other","task_ids:multiple-choice-qa","task_ids:other-other-raw-dataset"],"citation":"@misc{hill2016goldilocks,\n      title={The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations},\n      author={Felix Hill and Antoine Bordes and Sumit Chopra and Jason Weston},\n      year={2016},\n      eprint={1511.02301},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"The Children’s Book Test (CBT) is designed to measure directly\nhow well language models can exploit wider linguistic context.\nThe CBT is built from books that are freely available.","paperswithcode_id":"cbt","key":""},{"id":"cc100","tags":["annotations_creators:found","language_creators:found","languages:af","languages:am","languages:ar","languages:as","languages:az","languages:be","languages:bg","languages:bn","languages:bn-Latn","languages:br","languages:bs","languages:ca","languages:cs","languages:cy","languages:da","languages:de","languages:el","languages:en","languages:eo","languages:es","languages:et","languages:eu","languages:fa","languages:ff","languages:fi","languages:fr","languages:fy","languages:ga","languages:gd","languages:gl","languages:gn","languages:gu","languages:ha","languages:he","languages:hi","languages:hi-Latn","languages:hr","languages:ht","languages:hu","languages:hy","languages:id","languages:ig","languages:is","languages:it","languages:ja","languages:jv","languages:ka","languages:kk","languages:km","languages:kn","languages:ko","languages:ku","languages:ky","languages:la","languages:lg","languages:li","languages:ln","languages:lo","languages:lt","languages:lv","languages:mg","languages:mk","languages:ml","languages:mn","languages:mr","languages:ms","languages:my","languages:my-x-zawgyi","languages:ne","languages:nl","languages:no","languages:ns","languages:om","languages:or","languages:pa","languages:pl","languages:ps","languages:pt","languages:qu","languages:rm","languages:ro","languages:ru","languages:sa","languages:si","languages:sc","languages:sd","languages:sk","languages:sl","languages:so","languages:sq","languages:sr","languages:ss","languages:su","languages:sv","languages:sw","languages:ta","languages:ta-Latn","languages:te","languages:te-Latn","languages:th","languages:tl","languages:tn","languages:tr","languages:ug","languages:uk","languages:ur","languages:ur-Latn","languages:uz","languages:vi","languages:wo","languages:xh","languages:yi","languages:yo","languages:zh-Hans","languages:zh-Hant","languages:zu","licenses:unknown","multilinguality:multilingual","size_categories:1M<n<10M","size_categories:10M<n<100M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling","pretty_name:CC100"],"citation":"@inproceedings{conneau-etal-2020-unsupervised,\n    title = \"Unsupervised Cross-lingual Representation Learning at Scale\",\n    author = \"Conneau, Alexis  and\n      Khandelwal, Kartikay  and\n      Goyal, Naman  and\n      Chaudhary, Vishrav  and\n      Wenzek, Guillaume  and\n      Guzm{'a}n, Francisco  and\n      Grave, Edouard  and\n      Ott, Myle  and\n      Zettlemoyer, Luke  and\n      Stoyanov, Veselin\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.747\",\n    doi = \"10.18653/v1/2020.acl-main.747\",\n    pages = \"8440--8451\",\n    abstract = \"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{%} average accuracy on XNLI, +13{%} average F1 score on MLQA, and +2.4{%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{%} in XNLI accuracy for Swahili and 11.4{%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.\",\n}\n@inproceedings{wenzek-etal-2020-ccnet,\n    title = \"{CCN}et: Extracting High Quality Monolingual Datasets from Web Crawl Data\",\n    author = \"Wenzek, Guillaume  and\n      Lachaux, Marie-Anne  and\n      Conneau, Alexis  and\n      Chaudhary, Vishrav  and\n      Guzm{'a}n, Francisco  and\n      Joulin, Armand  and\n      Grave, Edouard\",\n    booktitle = \"Proceedings of the 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.494\",\n    pages = \"4003--4012\",\n    abstract = \"Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}","description":"This corpus is an attempt to recreate the dataset used for training XLM-R. This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages (indicated by *_rom). This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Each file comprises of documents separated by double-newlines and paragraphs within the same document separated by a newline. The data is generated using the open source CC-Net repository. No claims of intellectual property are made on the work of preparation of the corpus.","paperswithcode_id":"cc100","key":""},{"id":"cc_news","tags":["annotations_creators:no-annotation","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@InProceedings{Hamborg2017,\n  author     = {Hamborg, Felix and Meuschke, Norman and Breitinger, Corinna and Gipp, Bela},\n  title      = {news-please: A Generic News Crawler and Extractor},\n  year       = {2017},\n  booktitle  = {Proceedings of the 15th International Symposium of Information Science},\n  location   = {Berlin},\n  doi        = {10.5281/zenodo.4120316},\n  pages      = {218--223},\n  month      = {March}\n}","description":"CC-News containing news articles from news sites all over the world The data is available on AWS S3 in the Common Crawl bucket at /crawl-data/CC-NEWS/. This version of the dataset has 708241 articles. It represents a small portion of English  language subset of the CC-News dataset created using news-please(Hamborg et al.,2017) to collect and extract English language portion of CC-News.","paperswithcode_id":"cc-news","key":""},{"id":"ccaligned_multilingual","tags":["annotations_creators:no-annotation","language_creators:found","languages:af","languages:ak","languages:am","languages:ar","languages:as","languages:ay","languages:az","languages:be","languages:bg","languages:bm","languages:bn","languages:br","languages:bs","languages:ca","languages:ckb","languages:cs","languages:ceb","languages:cy","languages:de","languages:dv","languages:el","languages:eo","languages:es","languages:fa","languages:ff","languages:fi","languages:fo","languages:fr","languages:fy","languages:ga","languages:gl","languages:gn","languages:gu","languages:he","languages:hi","languages:hr","languages:hu","languages:id","languages:ig","languages:is","languages:it","languages:iu","languages:ja","languages:ka","languages:kg","languages:kk","languages:km","languages:kn","languages:ko","languages:ku","languages:ky","languages:la","languages:lg","languages:li","languages:ln","languages:lo","languages:lt","languages:lv","languages:mg","languages:mi","languages:mk","languages:ml","languages:mn","languages:mr","languages:ms","languages:mt","languages:my","languages:ne","languages:nl","languages:no","languages:nso","languages:ny","languages:om","languages:or","languages:pa","languages:pl","languages:ps","languages:pt","languages:shn","languages:kac","languages:rm","languages:ro","languages:ru","languages:rw","languages:sc","languages:sd","languages:se","languages:si","languages:sk","languages:sl","languages:sn","languages:so","languages:sq","languages:sr","languages:ss","languages:st","languages:su","languages:sv","languages:sw","languages:syc","languages:szl","languages:ta","languages:te","languages:tg","languages:th","languages:ti","languages:tl","languages:tn","languages:tr","languages:ts","languages:tt","languages:zgh","languages:ug","languages:uk","languages:ur","languages:uz","languages:ve","languages:vi","languages:wo","languages:war","languages:xh","languages:yi","languages:yo","languages:zh","languages:zu","languages:zza","licenses:unknown","multilinguality:translation","size_categories:n<1K","size_categories:1K<n<10K","size_categories:10K<n<100K","size_categories:100K<n<1M","size_categories:1M<n<10M","size_categories:10M<n<100M","source_datasets:original","task_categories:other","task_ids:other-other-translation","pretty_name:CCAligned"],"citation":"@inproceedings{elkishky_ccaligned_2020,\n author = {El-Kishky, Ahmed and Chaudhary, Vishrav and Guzm{\\'a}n, Francisco and Koehn, Philipp},\n booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)},\n month = {November},\n title = {{CCAligned}: A Massive Collection of Cross-lingual Web-Document Pairs},\n year = {2020}\n address = \"Online\",\n publisher = \"Association for Computational Linguistics\",\n url = \"https://www.aclweb.org/anthology/2020.emnlp-main.480\",\n doi = \"10.18653/v1/2020.emnlp-main.480\",\n pages = \"5960--5969\"\n}","description":"CCAligned consists of parallel or comparable web-document pairs in 137 languages aligned with English. These web-document pairs were constructed by performing language identification on raw web-documents, and ensuring corresponding language codes were corresponding in the URLs of web documents. This pattern matching approach yielded more than 100 million aligned documents paired with English. Recognizing that each English document was often aligned to mulitple documents in different target language, we can join on English documents to obtain aligned documents that directly pair two non-English documents (e.g., Arabic-French).","paperswithcode_id":"ccaligned","key":""},{"id":"cdsc","tags":["annotations_creators:expert-generated","language_creators:other","languages:pl","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:other","task_ids:other-other-sentences entailment and relatedness"],"citation":"@inproceedings{wroblewska2017polish,\ntitle={Polish evaluation dataset for compositional distributional semantics models},\nauthor={Wr{\\'o}blewska, Alina and Krasnowska-Kiera{\\'s}, Katarzyna},\nbooktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\npages={784--792},\nyear={2017}\n}","description":"Polish CDSCorpus consists of 10K Polish sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish. The dataset was presented at ACL 2017. Please refer to the Wróblewska and Krasnowska-Kieraś (2017) for a detailed description of the resource.","paperswithcode_id":"polish-cdscorpus","key":""},{"id":"cdt","tags":["annotations_creators:expert-generated","language_creators:other","languages:pl","licenses:bsd-3-clause","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@article{ptaszynski2019results,\ntitle={Results of the PolEval 2019 Shared Task 6: First Dataset and Open Shared Task for Automatic Cyberbullying Detection in Polish Twitter},\nauthor={Ptaszynski, Michal and Pieciukiewicz, Agata and Dybala, Pawel},\njournal={Proceedings of the PolEval 2019 Workshop},\npublisher={Institute of Computer Science, Polish Academy of Sciences},\npages={89},\nyear={2019}\n}","description":"The Cyberbullying Detection task was part of 2019 edition of PolEval competition. The goal is to predict if a given Twitter message contains a cyberbullying (harmful) content.","key":""},{"id":"cfq","tags":["languages:en"],"citation":"@inproceedings{Keysers2020,\n  title={Measuring Compositional Generalization: A Comprehensive Method on\n         Realistic Data},\n  author={Daniel Keysers and Nathanael Sch\\\"{a}rli and Nathan Scales and\n          Hylke Buisman and Daniel Furrer and Sergii Kashubin and\n          Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and\n          Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and\n          Olivier Bousquet},\n  booktitle={ICLR},\n  year={2020},\n  url={https://arxiv.org/abs/1912.09713.pdf},\n}","description":"The CFQ dataset (and it's splits) for measuring compositional generalization.\n\nSee https://arxiv.org/abs/1912.09713.pdf for background.\n\nExample usage:\ndata = datasets.load_dataset('cfq/mcd1')","paperswithcode_id":"cfq","key":""},{"id":"chr_en","tags":["annotations_creators:no-annotation","annotations_creators:found","annotations_creators:expert-generated","language_creators:found","languages:chr","languages:en","licenses:other-different-license-per-source","multilinguality:multilingual","multilinguality:monolingual","multilinguality:translation","size_categories:100K<n<1M","size_categories:1K<n<10K","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_ids:machine-translation","task_ids:language-modeling"],"citation":"@inproceedings{zhang2020chren,\n  title={ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization},\n  author={Zhang, Shiyue and Frey, Benjamin and Bansal, Mohit},\n  booktitle={EMNLP2020},\n  year={2020}\n}","description":"ChrEn is a Cherokee-English parallel dataset to facilitate machine translation research between Cherokee and English.\nChrEn is extremely low-resource contains 14k sentence pairs in total, split in ways that facilitate both in-domain and out-of-domain evaluation.\nChrEn also contains 5k Cherokee monolingual data to enable semi-supervised learning.","paperswithcode_id":"chren","key":""},{"id":"cifar10","tags":["annotations_creators:crowdsourced","language_creators:found","licenses:unknown","size_categories:10K<n<100K","source_datasets:extended|other-80-Million-Tiny-Images","task_categories:other","task_ids:other-other-image-classification"],"citation":"@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}","description":"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images\nper class. There are 50000 training images and 10000 test images.","paperswithcode_id":"cifar-10","key":""},{"id":"cifar100","tags":["annotations_creators:crowdsourced","language_creators:found","licenses:unknown","size_categories:10K<n<100K","source_datasets:extended|other-80-Million-Tiny-Images","task_categories:other","task_ids:other-other-image-classification"],"citation":"@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}","description":"The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images\nper class. There are 500 training images and 100 testing images per class. There are 50000 training images and 10000 test images. The 100 classes are grouped into 20 superclasses.\nThere are two labels per image - fine label (actual class) and coarse label (superclass).","paperswithcode_id":"cifar-100","key":""},{"id":"circa","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification","task_ids:text-classification-other-question-answer-pair-classification"],"citation":"@InProceedings{louis_emnlp2020,\n  author =      \"Annie Louis and Dan Roth and Filip Radlinski\",\n  title =       \"\"{I}'d rather just go to bed\": {U}nderstanding {I}ndirect {A}nswers\",\n  booktitle =   \"Proceedings of the 2020 Conference on Empirical Methods\n  in Natural Language Processing\",\n  year =        \"2020\",\n}","description":"The Circa (meaning ‘approximately’) dataset aims to help machine learning systems\nto solve the problem of interpreting indirect answers to polar questions.\n\nThe dataset contains pairs of yes/no questions and indirect answers, together with\nannotations for the interpretation of the answer. The data is collected in 10\ndifferent social conversational situations (eg. food preferences of a friend).\n\nNOTE: There might be missing labels in the dataset and we have replaced them with -1.\nThe original dataset contains no train/dev/test splits.","paperswithcode_id":"circa","key":""},{"id":"civil_comments","tags":["languages:en"],"citation":"@article{DBLP:journals/corr/abs-1903-04561,\n  author    = {Daniel Borkan and\n               Lucas Dixon and\n               Jeffrey Sorensen and\n               Nithum Thain and\n               Lucy Vasserman},\n  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text\n               Classification},\n  journal   = {CoRR},\n  volume    = {abs/1903.04561},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1903.04561},\n  archivePrefix = {arXiv},\n  eprint    = {1903.04561},\n  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"The comments in this dataset come from an archive of the Civil Comments\nplatform, a commenting plugin for independent news sites. These public comments\nwere created from 2015 - 2017 and appeared on approximately 50 English-language\nnews sites across the world. When Civil Comments shut down in 2017, they chose\nto make the public comments available in a lasting open archive to enable future\nresearch. The original data, published on figshare, includes the public comment\ntext, some associated metadata such as article IDs, timestamps and\ncommenter-generated \"civility\" labels, but does not include user ids. Jigsaw\nextended this dataset by adding additional labels for toxicity and identity\nmentions. This data set is an exact replica of the data released for the\nJigsaw Unintended Bias in Toxicity Classification Kaggle challenge.  This\ndataset is released under CC0, as is the underlying comment text.","key":""},{"id":"clickbait_news_bg","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:bg","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:fact-checking"],"citation":"@InProceedings{clickbait_news_bg,\ntitle = {Dataset with clickbait and fake news in Bulgarian. Introduced for the Hack the Fake News 2017.},\nauthors={Data Science Society},\nyear={2017},\nurl={https://gitlab.com/datasciencesociety/case_fake_news/}\n}","description":"Dataset with clickbait and fake news in Bulgarian. Introduced for the Hack the Fake News 2017.","key":""},{"id":"climate_fever","tags":["annotations_creators:crowdsourced","annotations_creators:expert-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|wikipedia","source_datasets:original","task_categories:text-classification","task_categories:text-retrieval","task_categories:text-scoring","task_ids:fact-checking","task_ids:fact-checking-retrieval","task_ids:semantic-similarity-scoring"],"citation":"@misc{diggelmann2020climatefever,\n      title={CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n      author={Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n      year={2020},\n      eprint={2012.00614},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"A dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change collected on the internet. Each claim is accompanied by five manually annotated evidence sentences retrieved from the English Wikipedia that support, refute or do not give enough information to validate the claim totalling in 7,675 claim-evidence pairs. The dataset features challenging claims that relate multiple facets and disputed cases of claims where both supporting and refuting evidence are present.","paperswithcode_id":"climate-fever","key":""},{"id":"clinc_oos","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","licenses:cc-by-3.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification"],"citation":"    @inproceedings{larson-etal-2019-evaluation,\n    title = \"An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction\",\n    author = \"Larson, Stefan  and\n      Mahendran, Anish  and\n      Peper, Joseph J.  and\n      Clarke, Christopher  and\n      Lee, Andrew  and\n      Hill, Parker  and\n      Kummerfeld, Jonathan K.  and\n      Leach, Kevin  and\n      Laurenzano, Michael A.  and\n      Tang, Lingjia  and\n      Mars, Jason\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\",\n    year = \"2019\",\n    url = \"https://www.aclweb.org/anthology/D19-1131\"\n}","description":"    This dataset is for evaluating the performance of intent classification systems in the\n    presence of \"out-of-scope\" queries. By \"out-of-scope\", we mean queries that do not fall\n    into any of the system-supported intent classes. Most datasets include only data that is\n    \"in-scope\". Our dataset includes both in-scope and out-of-scope data. You might also know\n    the term \"out-of-scope\" by other terms, including \"out-of-domain\" or \"out-of-distribution\".","paperswithcode_id":"clinc150","key":""},{"id":"clue","tags":[],"citation":"@misc{xu2020clue,\n    title={CLUE: A Chinese Language Understanding Evaluation Benchmark},\n    author={Liang Xu and Xuanwei Zhang and Lu Li and Hai Hu and Chenjie Cao and Weitang Liu and Junyi Li and Yudong Li and Kai Sun and Yechen Xu and Yiming Cui and Cong Yu and Qianqian Dong and Yin Tian and Dian Yu and Bo Shi and Jun Zeng and Rongzhao Wang and Weijian Xie and Yanting Li and Yina Patterson and Zuoyu Tian and Yiwen Zhang and He Zhou and Shaoweihua Liu and Qipeng Zhao and Cong Yue and Xinrui Zhang and Zhengliang Yang and Zhenzhong Lan},\n    year={2020},\n    eprint={2004.05986},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"CLUE, A Chinese Language Understanding Evaluation Benchmark\n(https://www.cluebenchmarks.com/) is a collection of resources for training,\nevaluating, and analyzing Chinese language understanding systems.","paperswithcode_id":"clue","key":""},{"id":"cmrc2018","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:zh","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@inproceedings{cui-emnlp2019-cmrc2018,\n    title = {A Span-Extraction Dataset for {C}hinese Machine Reading Comprehension},\n    author = {Cui, Yiming  and\n      Liu, Ting  and\n      Che, Wanxiang  and\n      Xiao, Li  and\n      Chen, Zhipeng  and\n      Ma, Wentao  and\n      Wang, Shijin  and\n      Hu, Guoping},\n    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},\n    month = {nov},\n    year = {2019},\n    address = {Hong Kong, China},\n    publisher = {Association for Computational Linguistics},\n    url = {https://www.aclweb.org/anthology/D19-1600},\n    doi = {10.18653/v1/D19-1600},\n    pages = {5886--5891}}","description":"A Span-Extraction dataset for Chinese machine reading comprehension to add language\ndiversities in this area. The dataset is composed by near 20,000 real questions annotated\non Wikipedia paragraphs by human experts. We also annotated a challenge set which\ncontains the questions that need comprehensive understanding and multi-sentence\ninference throughout the context.","paperswithcode_id":"cmrc-2018","key":""},{"id":"cnn_dailymail","tags":["annotations_creators:no-annotation","language_creators:found","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@article{DBLP:journals/corr/SeeLM17,\n  author    = {Abigail See and\n               Peter J. Liu and\n               Christopher D. Manning},\n  title     = {Get To The Point: Summarization with Pointer-Generator Networks},\n  journal   = {CoRR},\n  volume    = {abs/1704.04368},\n  year      = {2017},\n  url       = {http://arxiv.org/abs/1704.04368},\n  archivePrefix = {arXiv},\n  eprint    = {1704.04368},\n  timestamp = {Mon, 13 Aug 2018 16:46:08 +0200},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/SeeLM17},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@inproceedings{hermann2015teaching,\n  title={Teaching machines to read and comprehend},\n  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},\n  booktitle={Advances in neural information processing systems},\n  pages={1693--1701},\n  year={2015}\n}","description":"CNN/DailyMail non-anonymized summarization dataset.\n\nThere are two features:\n  - article: text of news article, used as the document to be summarized\n  - highlights: joined text of highlights with <s> and </s> around each\n    highlight, which is the target summary","paperswithcode_id":"cnn-daily-mail-1","key":""},{"id":"coached_conv_pref","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:other","task_categories:sequence-modeling","task_categories:structure-prediction","task_ids:other-other-Conversational Recommendation","task_ids:dialogue-modeling","task_ids:parsing"],"citation":"@inproceedings{48414,\ntitle\t= {Coached Conversational Preference Elicitation: A Case Study in Understanding Movie Preferences},\nauthor\t= {Filip Radlinski and Krisztian Balog and Bill Byrne and Karthik Krishnamoorthi},\nyear\t= {2019},\nbooktitle\t= {Proceedings of the Annual SIGdial Meeting on Discourse and Dialogue}\n}","description":"A dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing\nmovie preferences in natural language. It was collected using a Wizard-of-Oz methodology between two paid crowd-workers,\nwhere one worker plays the role of an 'assistant', while the other plays the role of a 'user'. The 'assistant' elicits\nthe 'user’s' preferences about movies following a Coached Conversational Preference Elicitation (CCPE) method. The\nassistant asks questions designed to minimize the bias in the terminology the 'user' employs to convey his or her\npreferences as much as possible, and to obtain these preferences in natural language. Each dialog is annotated with\nentity mentions, preferences expressed about entities, descriptions of entities provided, and other statements of\nentities.","paperswithcode_id":"coached-conversational-preference-elicitation","key":""},{"id":"coarse_discourse","tags":[],"citation":"@inproceedings{coarsediscourse, title={Characterizing Online Discussion Using Coarse Discourse Sequences}, author={Zhang, Amy X. and Culbertson, Bryan and Paritosh, Praveen}, booktitle={Proceedings of the 11th International AAAI Conference on Weblogs and Social Media}, series={ICWSM '17}, year={2017}, location = {Montreal, Canada} }","description":"dataset contains discourse annotation and relation on threads from reddit during 2016","paperswithcode_id":"coarse-discourse","key":""},{"id":"codah","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@inproceedings{chen2019codah,\n  title={CODAH: An Adversarially-Authored Question Answering Dataset for Common Sense},\n  author={Chen, Michael and D'Arcy, Mike and Liu, Alisa and Fernandez, Jared and Downey, Doug},\n  booktitle={Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP},\n  pages={63--69},\n  year={2019}\n}","description":"The COmmonsense Dataset Adversarially-authored by Humans (CODAH) is an evaluation set for commonsense question-answering in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions. Our experimental results show that CODAH questions present a complementary extension to the SWAG dataset, testing additional modes of common sense.","paperswithcode_id":"codah","key":""},{"id":"code_search_net","tags":["annotations_creators:no-annotation","language_creators:machine-generated","languages:code","licenses:other-several-licenses","multilinguality:multilingual","size_categories:1M<n<10M","size_categories:100K<n<1M","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling","pretty_name:CodeSearchNet"],"citation":"@article{husain2019codesearchnet,\n    title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},\n    author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\n    journal={arXiv preprint arXiv:1909.09436},\n    year={2019}\n}","description":"CodeSearchNet corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation.","paperswithcode_id":"codesearchnet","key":""},{"id":"code_x_glue_cc_clone_detection_big_clone_bench","tags":["annotations_creators:found","language_creators:found","languages:code","licenses:other-C-UDA","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:text-classification","task_ids:semantic-similarity-classification"],"citation":"@inproceedings{svajlenko2014towards,\ntitle={Towards a big data curated benchmark of inter-project code clones},\nauthor={Svajlenko, Jeffrey and Islam, Judith F and Keivanloo, Iman and Roy, Chanchal K and Mia, Mohammad Mamun},\nbooktitle={2014 IEEE International Conference on Software Maintenance and Evolution},\npages={476--480},\nyear={2014},\norganization={IEEE}\n}\n\n@inproceedings{wang2020detecting,\ntitle={Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree},\nauthor={Wang, Wenhan and Li, Ge and Ma, Bo and Xia, Xin and Jin, Zhi},\nbooktitle={2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)},\npages={261--271},\nyear={2020},\norganization={IEEE}\n}","description":"Given two codes as the input, the task is to do binary classification (0/1), where 1 stands for semantic equivalence and 0 for others. Models are evaluated by F1 score.\nThe dataset we use is BigCloneBench and filtered following the paper Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree.","key":""},{"id":"code_x_glue_cc_clone_detection_poj104","tags":["annotations_creators:found","language_creators:found","languages:code","licenses:other-C-UDA","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-retrieval","task_ids:document-retrieval"],"citation":"@inproceedings{mou2016convolutional,\ntitle={Convolutional neural networks over tree structures for programming language processing},\nauthor={Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},\nbooktitle={Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},\npages={1287--1293},\nyear={2016}\n}","description":"Given a code and a collection of candidates as the input, the task is to return Top K codes with the same semantic. Models are evaluated by MAP score.\nWe use POJ-104 dataset on this task.","key":""},{"id":"code_x_glue_cc_cloze_testing_all","tags":["annotations_creators:found","language_creators:found","languages:code","licenses:other-C-UDA","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:slot-filling"],"citation":"@article{CodeXGLUE,\ntitle={CodeXGLUE: An Open Challenge for Code Intelligence},\njournal={arXiv},\nyear={2020},\n}\n@article{feng2020codebert,\ntitle={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},\nauthor={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},\njournal={arXiv preprint arXiv:2002.08155},\nyear={2020}\n}\n@article{husain2019codesearchnet,\ntitle={CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},\nauthor={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\njournal={arXiv preprint arXiv:1909.09436},\nyear={2019}\n}","description":"Cloze tests are widely adopted in Natural Languages Processing to evaluate the performance of the trained language models. The task is aimed to predict the answers for the blank with the context of the blank, which can be formulated as a multi-choice classification problem.\nHere we present the two cloze testing datasets in code domain with six different programming languages: ClozeTest-maxmin and ClozeTest-all. Each instance in the dataset contains a masked code function, its docstring and the target word.\nThe only difference between ClozeTest-maxmin and ClozeTest-all is their selected words sets, where ClozeTest-maxmin only contains two words while ClozeTest-all contains 930 words.","key":""},{"id":"code_x_glue_cc_cloze_testing_maxmin","tags":["annotations_creators:found","language_creators:found","languages:code","licenses:other-C-UDA","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:slot-filling"],"citation":"@article{CodeXGLUE,\ntitle={CodeXGLUE: An Open Challenge for Code Intelligence},\njournal={arXiv},\nyear={2020},\n}\n@article{feng2020codebert,\ntitle={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},\nauthor={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},\njournal={arXiv preprint arXiv:2002.08155},\nyear={2020}\n}\n@article{husain2019codesearchnet,\ntitle={CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},\nauthor={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\njournal={arXiv preprint arXiv:1909.09436},\nyear={2019}\n}","description":"Cloze tests are widely adopted in Natural Languages Processing to evaluate the performance of the trained language models. The task is aimed to predict the answers for the blank with the context of the blank, which can be formulated as a multi-choice classification problem.\nHere we present the two cloze testing datasets in code domain with six different programming languages: ClozeTest-maxmin and ClozeTest-all. Each instance in the dataset contains a masked code function, its docstring and the target word.\nThe only difference between ClozeTest-maxmin and ClozeTest-all is their selected words sets, where ClozeTest-maxmin only contains two words while ClozeTest-all contains 930 words.","key":""},{"id":"code_x_glue_cc_code_completion_line","tags":["annotations_creators:found","language_creators:found","languages:code","licenses:other-C-UDA","multilinguality:monolingual","size_categories:n<1K","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:slot-filling"],"citation":"@article{raychev2016probabilistic,\ntitle={Probabilistic Model for Code with Decision Trees},\nauthor={Raychev, Veselin and Bielik, Pavol and Vechev, Martin},\njournal={ACM SIGPLAN Notices},\npages={731--747},\nyear={2016},\npublisher={ACM New York, NY, USA}\n}\n@inproceedings{allamanis2013mining,\ntitle={Mining Source Code Repositories at Massive Scale using Language Modeling},\nauthor={Allamanis, Miltiadis and Sutton, Charles},\nbooktitle={2013 10th Working Conference on Mining Software Repositories (MSR)},\npages={207--216},\nyear={2013},\norganization={IEEE}\n}","description":"Complete the unfinished line given previous context. Models are evaluated by exact match and edit similarity.\nWe propose line completion task to test model's ability to autocomplete a line. Majority code completion systems behave well in token level completion, but fail in completing an unfinished line like a method call with specific parameters, a function signature, a loop condition, a variable definition and so on. When a software develop finish one or more tokens of the current line, the line level completion model is expected to generate the entire line of syntactically correct code.\nLine level code completion task shares the train/dev dataset with token level completion. After training a model on CodeCompletion-token, you could directly use it to test on line-level completion.","key":""},{"id":"code_x_glue_cc_code_completion_token","tags":["annotations_creators:found","language_creators:found","languages:code","licenses:other-C-UDA","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@article{raychev2016probabilistic,\n    title={Probabilistic Model for Code with Decision Trees},\n    author={Raychev, Veselin and Bielik, Pavol and Vechev, Martin},\n    journal={ACM SIGPLAN Notices},\n    pages={731--747},\n    year={2016},\n    publisher={ACM New York, NY, USA}\n}\n@inproceedings{allamanis2013mining,\n    title={Mining Source Code Repositories at Massive Scale using Language Modeling},\n    author={Allamanis, Miltiadis and Sutton, Charles},\n    booktitle={2013 10th Working Conference on Mining Software Repositories (MSR)},\n    pages={207--216},\n    year={2013},\n    organization={IEEE}\n}","description":"Predict next code token given context of previous tokens. Models are evaluated by token level accuracy.\nCode completion is a one of the most widely used features in software development through IDEs. An effective code completion tool could improve software developers' productivity. We provide code completion evaluation tasks in two granularities -- token level and line level. Here we introduce token level code completion. Token level task is analogous to language modeling. Models should have be able to predict the next token in arbitary types.","key":""},{"id":"code_x_glue_cc_code_refinement","tags":["annotations_creators:expert-generated","language_creators:found","languages:code","licenses:other-C-UDA","multilinguality:other-programming-languages","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-debugging"],"citation":"@article{10.1145/3340544,\nauthor = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Penta, Massimiliano Di and White, Martin and Poshyvanyk, Denys},\ntitle = {An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation},\nyear = {2019},\nissue_date = {October 2019},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nvolume = {28},\nnumber = {4},\nissn = {1049-331X},\nurl = {https://doi-org.proxy.wm.edu/10.1145/3340544},\ndoi = {10.1145/3340544},\nabstract = {Millions of open source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation, we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9--50% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.},\njournal = {ACM Trans. Softw. Eng. Methodol.},\nmonth = sep,\narticleno = {19},\nnumpages = {29},\nkeywords = {bug-fixes, Neural machine translation}\n}","description":"We use the dataset released by this paper(https://arxiv.org/pdf/1812.08693.pdf). The source side is a Java function with bugs and the target side is the refined one. All the function and variable names are normalized. Their dataset contains two subsets ( i.e.small and medium) based on the function length.","key":""},{"id":"code_x_glue_cc_code_to_code_trans","tags":["annotations_creators:expert-generated","language_creators:found","languages:code","licenses:other-C-UDA","multilinguality:other-programming-languages","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@article{DBLP:journals/corr/abs-2102-04664,\n  author    = {Shuai Lu and\n               Daya Guo and\n               Shuo Ren and\n               Junjie Huang and\n               Alexey Svyatkovskiy and\n               Ambrosio Blanco and\n               Colin B. Clement and\n               Dawn Drain and\n               Daxin Jiang and\n               Duyu Tang and\n               Ge Li and\n               Lidong Zhou and\n               Linjun Shou and\n               Long Zhou and\n               Michele Tufano and\n               Ming Gong and\n               Ming Zhou and\n               Nan Duan and\n               Neel Sundaresan and\n               Shao Kun Deng and\n               Shengyu Fu and\n               Shujie Liu},\n  title     = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding\n               and Generation},\n  journal   = {CoRR},\n  volume    = {abs/2102.04664},\n  year      = {2021}\n}","description":"The dataset is collected from several public repos, including Lucene(http://lucene.apache.org/), POI(http://poi.apache.org/), JGit(https://github.com/eclipse/jgit/) and Antlr(https://github.com/antlr/).\n        We collect both the Java and C# versions of the codes and find the parallel functions. After removing duplicates and functions with the empty body, we split the whole dataset into training, validation and test sets.","key":""},{"id":"code_x_glue_cc_defect_detection","tags":["annotations_creators:found","language_creators:found","languages:code","licenses:other-C-UDA","multilinguality:other-programming-languages","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"citation":"@inproceedings{zhou2019devign,\ntitle={Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks},\nauthor={Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},\nbooktitle={Advances in Neural Information Processing Systems},\npages={10197--10207}, year={2019}","description":"Given a source code, the task is to identify whether it is an insecure code that may attack software systems, such as resource leaks, use-after-free vulnerabilities and DoS attack. We treat the task as binary classification (0/1), where 1 stands for insecure code and 0 for secure code.\nThe dataset we use comes from the paper Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks. We combine all projects and split 80%/10%/10% for training/dev/test.","key":""},{"id":"code_x_glue_ct_code_to_text","tags":["annotations_creators:found","language_creators:found","languages:code","languages:en","licenses:other-C-UDA","multilinguality:other-programming-languages","size_categories:100K<n<1M","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@article{husain2019codesearchnet,\ntitle={Codesearchnet challenge: Evaluating the state of semantic code search},\nauthor={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\njournal={arXiv preprint arXiv:1909.09436},\nyear={2019}\n}","description":"The dataset we use comes from CodeSearchNet and we filter the dataset as the following:\n- Remove examples that codes cannot be parsed into an abstract syntax tree.\n- Remove examples that #tokens of documents is < 3 or >256\n- Remove examples that documents contain special tokens (e.g. <img ...> or https:...)\n- Remove examples that documents are not English.","key":""},{"id":"code_x_glue_tc_nl_code_search_adv","tags":["annotations_creators:found","language_creators:found","languages:code","languages:en","licenses:other-C-UDA","multilinguality:other-programming-languages","size_categories:100K<n<1M","source_datasets:original","task_categories:text-retrieval","task_ids:document-retrieval"],"citation":"@article{husain2019codesearchnet,\ntitle={Codesearchnet challenge: Evaluating the state of semantic code search},\nauthor={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\njournal={arXiv preprint arXiv:1909.09436},\nyear={2019}\n}","description":"The dataset we use comes from CodeSearchNet and we filter the dataset as the following:\n- Remove examples that codes cannot be parsed into an abstract syntax tree.\n- Remove examples that #tokens of documents is < 3 or >256\n- Remove examples that documents contain special tokens (e.g. <img ...> or https:...)\n- Remove examples that documents are not English.","key":""},{"id":"code_x_glue_tc_text_to_code","tags":["annotations_creators:found","language_creators:found","languages:en","languages:code","licenses:other-C-UDA","multilinguality:other-programming-languages","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@article{iyer2018mapping,\ntitle={Mapping language to code in programmatic context},\nauthor={Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},\njournal={arXiv preprint arXiv:1808.09588},\nyear={2018}\n}","description":"We use concode dataset which is a widely used code generation dataset from Iyer's EMNLP 2018 paper Mapping Language to Code in Programmatic Context. See paper for details.","key":""},{"id":"code_x_glue_tt_text_to_text","tags":["annotations_creators:found","language_creators:found","languages:da","languages:nb","languages:lv","languages:zh","languages:en","licenses:other-C-UDA","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@article{DBLP:journals/corr/abs-2102-04664,\n  author    = {Shuai Lu and\n               Daya Guo and\n               Shuo Ren and\n               Junjie Huang and\n               Alexey Svyatkovskiy and\n               Ambrosio Blanco and\n               Colin B. Clement and\n               Dawn Drain and\n               Daxin Jiang and\n               Duyu Tang and\n               Ge Li and\n               Lidong Zhou and\n               Linjun Shou and\n               Long Zhou and\n               Michele Tufano and\n               Ming Gong and\n               Ming Zhou and\n               Nan Duan and\n               Neel Sundaresan and\n               Shao Kun Deng and\n               Shengyu Fu and\n               Shujie Liu},\n  title     = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding\n               and Generation},\n  journal   = {CoRR},\n  volume    = {abs/2102.04664},\n  year      = {2021}\n}","description":"The dataset we use is crawled and filtered from Microsoft Documentation, whose document located at https://github.com/MicrosoftDocs/.","key":""},{"id":"com_qa","tags":["languages:en"],"citation":"@inproceedings{abujabal-etal-2019-comqa,\n    title = \"{ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters\",\n    author = {Abujabal, Abdalghani  and\n      Saha Roy, Rishiraj  and\n      Yahya, Mohamed  and\n      Weikum, Gerhard},\n    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},\n    month = {jun},\n    year = {2019},\n    address = {Minneapolis, Minnesota},\n    publisher = {Association for Computational Linguistics},\n    url = {https://www.aclweb.org/anthology/N19-1027},\n    doi = {10.18653/v1/N19-1027{,\n    pages = {307--317},\n    }","description":"ComQA is a dataset of 11,214 questions, which were collected from WikiAnswers, a community question answering website.\nBy collecting questions from such a site we ensure that the information needs are ones of interest to actual users.\nMoreover, questions posed there are often cannot be answered by commercial search engines or QA technology, making them\nmore interesting for driving future research compared to those collected from an engine's query log. The dataset contains\nquestions with various challenging phenomena such as the need for temporal reasoning, comparison (e.g., comparatives,\nsuperlatives, ordinals), compositionality (multiple, possibly nested, subquestions with multiple entities), and\nunanswerable questions (e.g., Who was the first human being on Mars?). Through a large crowdsourcing effort, questions\nin ComQA are grouped into 4,834 paraphrase clusters that express the same information need. Each cluster is annotated\nwith its answer(s). ComQA answers come in the form of Wikipedia entities wherever possible. Wherever the answers are\ntemporal or measurable quantities, TIMEX3 and the International System of Units (SI) are used for normalization.","paperswithcode_id":"comqa","key":""},{"id":"common_gen","tags":["languages:en"],"citation":"@inproceedings{lin-etal-2020-commongen,\n    title = \"{C}ommon{G}en: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\",\n    author = \"Lin, Bill Yuchen  and\n      Zhou, Wangchunshu  and\n      Shen, Ming  and\n      Zhou, Pei  and\n      Bhagavatula, Chandra  and\n      Choi, Yejin  and\n      Ren, Xiang\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.findings-emnlp.165\",\n    doi = \"10.18653/v1/2020.findings-emnlp.165\",\n    pages = \"1823--1840\"\n}","description":"CommonGen is a constrained text generation task, associated with a benchmark dataset,\nto explicitly test machines for the ability of generative commonsense reasoning. Given\na set of common concepts; the task is to generate a coherent sentence describing an\neveryday scenario using these concepts.\n\nCommonGen is challenging because it inherently requires 1) relational reasoning using\nbackground commonsense knowledge, and 2) compositional generalization ability to work\non unseen concept combinations. Our dataset, constructed through a combination of\ncrowd-sourcing from AMT and existing caption corpora, consists of 30k concept-sets and\n50k sentences in total.","paperswithcode_id":"commongen","key":""},{"id":"common_voice","tags":["pretty_name:Common Voice","annotations_creators:crowdsourced","language_creators:crowdsourced","languages:ab","languages:ar","languages:as","languages:br","languages:ca","languages:cnh","languages:cs","languages:cv","languages:cy","languages:de","languages:dv","languages:el","languages:en","languages:eo","languages:es","languages:et","languages:eu","languages:fa","languages:fi","languages:fr","languages:fy-NL","languages:ga-IE","languages:hi","languages:hsb","languages:hu","languages:ia","languages:id","languages:it","languages:ja","languages:ka","languages:kab","languages:ky","languages:lg","languages:lt","languages:lv","languages:mn","languages:mt","languages:nl","languages:or","languages:pa-IN","languages:pl","languages:pt","languages:rm-sursilv","languages:rm-vallader","languages:ro","languages:ru","languages:rw","languages:sah","languages:sl","languages:sv-SE","languages:ta","languages:th","languages:tr","languages:tt","languages:uk","languages:vi","languages:vot","languages:zh-CN","languages:zh-HK","languages:zh-TW","licenses:cc-by-nc-4.0","multilinguality:multilingual","size_categories:n<1K","size_categories:10K<n<100K","size_categories:100K<n<1M","size_categories:1K<n<10K","source_datasets:extended|common_voice","task_categories:speech-processing","task_ids:automatic-speech-recognition"],"citation":"@inproceedings{commonvoice:2020,\n  author = {Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F. M. and Weber, G.},\n  title = {Common Voice: A Massively-Multilingual Speech Corpus},\n  booktitle = {Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)},\n  pages = {4211--4215},\n  year = 2020\n}","description":"Common Voice is Mozilla's initiative to help teach machines how real people speak.\nThe dataset currently consists of 7,335 validated hours of speech in 60 languages, but we’re always adding more voices and languages.","paperswithcode_id":"common-voice","key":""},{"id":"commonsense_qa","tags":["languages:en"],"citation":"@InProceedings{commonsense_QA,\ntitle={COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge},\nauthor={Alon, Talmor and Jonathan, Herzig and Nicholas, Lourie and Jonathan ,Berant},\njournal={arXiv preprint arXiv:1811.00937v2},\nyear={2019}","description":"CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge\n to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.\n The dataset is provided in two major training/validation/testing set splits: \"Random split\" which is the main evaluation\n  split, and \"Question token split\", see paper for details.","paperswithcode_id":"commonsenseqa","key":""},{"id":"compguesswhat","tags":[],"citation":"        @inproceedings{suglia2020compguesswhat,\n          title={CompGuessWhat?!: a Multi-task Evaluation Framework for Grounded Language Learning},\n          author={Suglia, Alessandro, Konstas, Ioannis, Vanzo, Andrea, Bastianelli, Emanuele, Desmond Elliott, Stella Frank and Oliver Lemon},\n          booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n          year={2020}\n        }","description":"CompGuessWhat?! is an instance of a multi-task framework for evaluating the quality of learned neural representations,\n        in particular concerning attribute grounding. Use this dataset if you want to use the set of games whose reference\n        scene is an image in VisualGenome. Visit the website for more details: https://compguesswhat.github.io","paperswithcode_id":"compguesswhat","key":""},{"id":"conceptnet5","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:found","languages:de","languages:en","languages:es","languages:fr","languages:it","languages:ja","languages:nl","languages:pt","languages:ru","languages:zh","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10M<n<100M","size_categories:100K<n<1M","size_categories:1M<n<10M","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"citation":"\\\r\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017. \"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge.\" In proceedings of AAAI 31.\r\n}","description":"This dataset is designed to provide training data\r\nfor common sense relationships pulls together from various sources.\r\n\r\nThe dataset is multi-lingual. See langauge codes and language info\r\nhere: https://github.com/commonsense/conceptnet5/wiki/Languages\r\n\r\n\r\nThis dataset provides an interface for the conceptnet5 csv file, and\r\nsome (but not all) of the raw text data used to build conceptnet5:\r\nomcsnet_sentences_free.txt, and omcsnet_sentences_more.txt.\r\n\r\nOne use of this dataset would be to learn to extract the conceptnet\r\nrelationship from the omcsnet sentences.\r\n\r\nConceptnet5 has 34,074,917 relationships. Of those relationships,\r\nthere are 2,176,099 surface text sentences related to those 2M\r\nentries.\r\n\r\nomcsnet_sentences_free has 898,161 lines. omcsnet_sentences_more has\r\n2,001,736 lines.\r\n\r\nOriginal downloads are available here\r\nhttps://github.com/commonsense/conceptnet5/wiki/Downloads. For more\r\ninformation, see: https://github.com/commonsense/conceptnet5/wiki\r\n\r\nThe omcsnet data comes with the following warning from the authors of\r\nthe above site: Remember: this data comes from various forms of\r\ncrowdsourcing. Sentences in these files are not necessarily true,\r\nuseful, or appropriate.","paperswithcode_id":"conceptnet","key":""},{"id":"conll2000","tags":["languages:en"],"citation":"@inproceedings{tksbuchholz2000conll,\n   author     = \"Tjong Kim Sang, Erik F. and Sabine Buchholz\",\n   title      = \"Introduction to the CoNLL-2000 Shared Task: Chunking\",\n   editor     = \"Claire Cardie and Walter Daelemans and Claire\n                 Nedellec and Tjong Kim Sang, Erik\",\n   booktitle  = \"Proceedings of CoNLL-2000 and LLL-2000\",\n   publisher  = \"Lisbon, Portugal\",\n   pages      = \"127--132\",\n   year       = \"2000\"\n}","description":" Text chunking consists of dividing a text in syntactically correlated parts of words. For example, the sentence\n He reckons the current account deficit will narrow to only # 1.8 billion in September . can be divided as follows:\n[NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP to ] [NP only # 1.8 billion ]\n[PP in ] [NP September ] .\n\nText chunking is an intermediate step towards full parsing. It was the shared task for CoNLL-2000. Training and test\ndata for this task is available. This data consists of the same partitions of the Wall Street Journal corpus (WSJ)\nas the widely used data for noun phrase chunking: sections 15-18 as training data (211727 tokens) and section 20 as\ntest data (47377 tokens). The annotation of the data has been derived from the WSJ corpus by a program written by\nSabine Buchholz from Tilburg University, The Netherlands.","paperswithcode_id":"conll-2000-1","key":""},{"id":"conll2002","tags":["annotations_creators:crowdsourced","language_creators:found","languages:es","languages:nl","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition","task_ids:part-of-speech-tagging"],"citation":"@inproceedings{tjong-kim-sang-2002-introduction,\n    title = \"Introduction to the {C}o{NLL}-2002 Shared Task: Language-Independent Named Entity Recognition\",\n    author = \"Tjong Kim Sang, Erik F.\",\n    booktitle = \"{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002)\",\n    year = \"2002\",\n    url = \"https://www.aclweb.org/anthology/W02-2024\",\n}","description":"Named entities are phrases that contain the names of persons, organizations, locations, times and quantities.\n\nExample:\n[PER Wolff] , currently a journalist in [LOC Argentina] , played with [PER Del Bosque] in the final years of the seventies in [ORG Real Madrid] .\n\nThe shared task of CoNLL-2002 concerns language-independent named entity recognition.\nWe will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.\nThe participants of the shared task will be offered training and test data for at least two languages.\nThey will use the data for developing a named-entity recognition system that includes a machine learning component.\nInformation sources other than the training data may be used in this shared task.\nWe are especially interested in methods that can use additional unannotated data for improving their performance (for example co-training).\n\nThe train/validation/test sets are available in Spanish and Dutch.\n\nFor more details see https://www.clips.uantwerpen.be/conll2002/ner/ and https://www.aclweb.org/anthology/W02-2024/","paperswithcode_id":"conll-2002","key":""},{"id":"conll2003","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other-reuters-corpus","task_categories:structure-prediction","task_ids:named-entity-recognition","task_ids:part-of-speech-tagging"],"citation":"@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n    author = \"Tjong Kim Sang, Erik F.  and\n      De Meulder, Fien\",\n    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n    year = \"2003\",\n    url = \"https://www.aclweb.org/anthology/W03-0419\",\n    pages = \"142--147\",\n}","description":"The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\nnot belong to the previous three groups.\n\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\ntagging scheme, whereas the original dataset uses IOB1.\n\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419","paperswithcode_id":"conll-2003","key":""},{"id":"conllpp","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|conll2003","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{wang2019crossweigh,\n  title={CrossWeigh: Training Named Entity Tagger from Imperfect Annotations},\n  author={Wang, Zihan and Shang, Jingbo and Liu, Liyuan and Lu, Lihao and Liu, Jiacheng and Han, Jiawei},\n  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},\n  pages={5157--5166},\n  year={2019}\n}","description":"CoNLLpp is a corrected version of the CoNLL2003 NER dataset where labels of 5.38% of the sentences in the test set\nhave been manually corrected. The training set and development set are included for completeness.\nFor more details see https://www.aclweb.org/anthology/D19-1519/ and https://github.com/ZihanWangKi/CrossWeigh","paperswithcode_id":"conll","key":""},{"id":"conv_ai","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-scoring","task_ids:text-scoring-other-evaluating-dialogue-systems"],"description":"ConvAI is a dataset of human-to-bot conversations labelled for quality. This data can be used to train a metric for evaluating dialogue systems. Moreover, it can be used in the development of chatbots themselves: it contains the information on the quality of utterances and entire dialogues, that can guide a dialogue system in search of better answers.","key":""},{"id":"conv_ai_2","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-scoring","task_ids:text-scoring-other-evaluating-dialogue-systems"],"citation":"@misc{dinan2019second,\n      title={The Second Conversational Intelligence Challenge (ConvAI2)},\n      author={Emily Dinan and Varvara Logacheva and Valentin Malykh and Alexander Miller and Kurt Shuster and Jack Urbanek and Douwe Kiela and Arthur Szlam and Iulian Serban and Ryan Lowe and Shrimai Prabhumoye and Alan W Black and Alexander Rudnicky and Jason Williams and Joelle Pineau and Mikhail Burtsev and Jason Weston},\n      year={2019},\n      eprint={1902.00098},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}","description":"ConvAI is a dataset of human-to-bot conversations labelled for quality. This data can be used to train a metric for evaluating dialogue systems. Moreover, it can be used in the development of chatbots themselves: it contains the information on the quality of utterances and entire dialogues, that can guide a dialogue system in search of better answers.","paperswithcode_id":"convai2","key":""},{"id":"conv_ai_3","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-scoring","task_ids:text-scoring-other-evaluating-dialogue-systems"],"citation":"@misc{aliannejadi2020convai3,\n      title={ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue Systems (ClariQ)},\n      author={Mohammad Aliannejadi and Julia Kiseleva and Aleksandr Chuklin and Jeff Dalton and Mikhail Burtsev},\n      year={2020},\n      eprint={2009.11352},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"The Conv AI 3 challenge is organized as part of the Search-oriented Conversational AI (SCAI) EMNLP workshop in 2020. The main aim of the conversational systems is to return an appropriate answer in response to the user requests. However, some user requests might be ambiguous. In Information Retrieval (IR) settings such a situation is handled mainly through the diversification of search result page. It is however much more challenging in dialogue settings. Hence, we aim to study the following situation for dialogue settings:\n- a user is asking an ambiguous question (where ambiguous question is a question to which one can return > 1 possible answers)\n- the system must identify that the question is ambiguous, and, instead of trying to answer it directly, ask a good clarifying question.","key":""},{"id":"conv_questions","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en-US","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_categories:sequence-modeling","task_ids:open-domain-qa","task_ids:dialogue-modeling"],"citation":"@InProceedings{christmann2019look,\n  title={Look before you hop: Conversational question answering over knowledge graphs using judicious context expansion},\n  author={Christmann, Philipp and Saha Roy, Rishiraj and Abujabal, Abdalghani and Singh, Jyotsna and Weikum, Gerhard},\n  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},\n  pages={729--738},\n  year={2019}\n}","description":"ConvQuestions is the first realistic benchmark for conversational question answering over knowledge graphs.\nIt contains 11,200 conversations which can be evaluated over Wikidata. The questions feature a variety of complex\nquestion phenomena like comparisons, aggregations, compositionality, and temporal reasoning.","key":""},{"id":"coqa","tags":["languages:en"],"citation":"@InProceedings{SivaAndAl:Coca,\n       author = {Siva, Reddy and Danqi, Chen and  Christopher D., Manning},\n        title = {WikiQA: A Challenge Dataset for Open-Domain Question Answering},\n      journal = { arXiv},\n         year = {2018},\n\n}","description":"CoQA: A Conversational Question Answering Challenge","paperswithcode_id":"coqa","key":""},{"id":"cord19","tags":["annotations_creators:no-annotation","language_creators:found","languages:en","licenses:cc-by-nc-sa-2.0","licenses:cc-by-nc-2.0","licenses:cc-by-nd-2.0","licenses:cc-by-sa-2.0","licenses:cc-by-nc-nd-2.0","licenses:cc-by-nc-1.0","licenses:other-cc0","licenses:other-hybrid-oa","licenses:other-els-covid","licenses:other-no-cc","licenses:other-gold-oa","licenses:other-green-oa","licenses:other-bronze-oa","licenses:other-biorxiv","licenses:other-arxiv","licenses:other-medrxiv","licenses:other-unk","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:other","task_ids:other-other-knowledge-extraction"],"citation":"@article{Wang2020CORD19TC,\n  title={CORD-19: The Covid-19 Open Research Dataset},\n  author={Lucy Lu Wang and Kyle Lo and Yoganand Chandrasekhar and Russell Reas and Jiangjiang Yang and Darrin Eide and\n  K. Funk and Rodney Michael Kinney and Ziyang Liu and W. Merrill and P. Mooney and D. Murdick and Devvret Rishi and\n  Jerry Sheehan and Zhihong Shen and B. Stilson and A. Wade and K. Wang and Christopher Wilhelm and Boya Xie and\n  D. Raymond and Daniel S. Weld and Oren Etzioni and Sebastian Kohlmeier},\n  journal={ArXiv},\n  year={2020}\n}","description":"The Covid-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on Covid-19 and related\nhistorical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information\nretrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19\nhas been downloaded over 75K times and has served as the basis of many Covid-19 text mining and discovery systems.\n\nThe dataset itself isn't defining a specific task, but there is a Kaggle challenge that define 17 open research\nquestions to be solved with the dataset: https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks","paperswithcode_id":"cord-19","key":""},{"id":"cornell_movie_dialog","tags":["languages:en"],"citation":"  @InProceedings{Danescu-Niculescu-Mizil+Lee:11a,\n\n  author={Cristian Danescu-Niculescu-Mizil and Lillian Lee},\n\n  title={Chameleons in imagined conversations:\n  A new approach to understanding coordination of linguistic style in dialogs.},\n\n  booktitle={Proceedings of the\n\n        Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011},\n\n  year={2011}\n\n}","description":"This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:\n- 220,579 conversational exchanges between 10,292 pairs of movie characters\n- involves 9,035 characters from 617 movies\n- in total 304,713 utterances\n- movie metadata included:\n    - genres\n    - release year\n    - IMDB rating\n    - number of IMDB votes\n    - IMDB rating\n- character metadata included:\n    - gender (for 3,774 characters)\n    - position on movie credits (3,321 characters)","paperswithcode_id":"cornell-movie-dialogs-corpus","key":""},{"id":"cos_e","tags":["languages:en"],"citation":"@inproceedings{rajani2019explain,\n     title = {Explain Yourself! Leveraging Language models for Commonsense Reasoning},\n    author = {Rajani, Nazneen Fatema  and\n      McCann, Bryan  and\n      Xiong, Caiming  and\n      Socher, Richard}\n      year={2019}\n    booktitle = {Proceedings of the 2019 Conference of the Association for Computational Linguistics (ACL2019)}\n    url ={https://arxiv.org/abs/1906.02361}\n}","description":"Common Sense Explanations (CoS-E) allows for training language models to\nautomatically generate explanations that can be used during training and\ninference in a novel Commonsense Auto-Generated Explanation (CAGE) framework.","paperswithcode_id":"cos-e","key":""},{"id":"cosmos_qa","tags":["languages:en"],"citation":"@inproceedings{cosmos,\n    title={COSMOS QA: Machine Reading Comprehension\n    with Contextual Commonsense Reasoning},\n    author={Lifu Huang and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},\n    booktitle ={arXiv:1909.00277v2},\n    year={2019}\n}","description":"Cosmos QA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people's everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context","paperswithcode_id":"cosmosqa","key":""},{"id":"counter","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:ur","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_categories:text-scoring","task_ids:semantic-similarity-scoring","task_ids:topic-classification"],"citation":"@Article{Sharjeel2016,\nauthor=\"Sharjeel, Muhammad\nand Nawab, Rao Muhammad Adeel\nand Rayson, Paul\",\ntitle=\"COUNTER: corpus of Urdu news text reuse\",\njournal=\"Language Resources and Evaluation\",\nyear=\"2016\",\npages=\"1--27\",\nissn=\"1574-0218\",\ndoi=\"10.1007/s10579-016-9367-2\",\nurl=\"http://dx.doi.org/10.1007/s10579-016-9367-2\"\n}","description":" The COrpus of Urdu News TExt Reuse (COUNTER) corpus contains 1200 documents with real examples of text reuse from the field of journalism. It has been manually annotated at document level with three levels of reuse: wholly derived, partially derived and non derived.","paperswithcode_id":"counter","key":""},{"id":"covid_qa_castorini","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa","task_ids:extractive-qa"],"citation":"@article{tang2020rapidly,\n  title={Rapidly Bootstrapping a Question Answering Dataset for COVID-19},\n  author={Tang, Raphael and Nogueira, Rodrigo and Zhang, Edwin and Gupta, Nikhil and Cam, Phuong and Cho, Kyunghyun and Lin, Jimmy},\n  journal={arXiv preprint arXiv:2004.11339},\n  year={2020}\n}","description":"CovidQA is the beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge.","paperswithcode_id":"covidqa","key":""},{"id":"covid_qa_deepset","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:closed-domain-qa","task_ids:extractive-qa"],"citation":"@inproceedings{moller2020covid,\n  title={COVID-QA: A Question Answering Dataset for COVID-19},\n  author={M{\\\"o}ller, Timo and Reina, Anthony and Jayakumar, Raghavan and Pietsch, Malte},\n  booktitle={Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020},\n  year={2020}\n}","description":"COVID-QA is a Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19.","key":""},{"id":"covid_qa_ucsd","tags":["annotations_creators:found","language_creators:expert-generated","language_creators:found","languages:en","languages:zh","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:closed-domain-qa"],"citation":"@article{ju2020CovidDialog,\n  title={CovidDialog: Medical Dialogue Datasets about COVID-19},\n  author={Ju, Zeqian and Chakravorty, Subrato and He, Xuehai and Chen, Shu and Yang, Xingyi and Xie, Pengtao},\n  journal={ https://github.com/UCSD-AI4H/COVID-Dialogue},\n  year={2020}\n}","key":""},{"id":"covid_tweets_japanese","tags":["annotations_creators:crowdsourced","language_creators:found","languages:ja","licenses:cc-by-nd-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:fact-checking"],"citation":"No paper about this dataset is published yet. Please cite this dataset as \"鈴木 優: COVID-19 日本語 Twitter データセット （http://www.db.info.gifu-u.ac.jp/covid-19-twitter-dataset/）\"","description":"53,640 Japanese tweets with annotation if a tweet is related to COVID-19 or not. The annotation is by majority decision by 5 - 10 crowd workers. Target tweets include \"COVID\" or \"コロナ\". The period of the tweets is from around January 2020 to around June 2020. The original tweets are not contained. Please use Twitter API to get them, for example.","key":""},{"id":"covost2","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","language_creators:expert-generated","languages:fr","languages:de","languages:es","languages:ca","languages:it","languages:ru","languages:zh-CN","languages:pt","languages:fa","languages:et","languages:mn","languages:nl","languages:tr","languages:ar","languages:sv-SE","languages:lv","languages:sl","languages:ta","languages:ja","languages:id","languages:cy","licenses:cc-by-nc-4.0","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:extended|other-common-voice","task_categories:other","task_ids:other-other-speech-translation"],"citation":"@misc{wang2020covost,\n    title={CoVoST 2: A Massively Multilingual Speech-to-Text Translation Corpus},\n    author={Changhan Wang and Anne Wu and Juan Pino},\n    year={2020},\n    eprint={2007.10310},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}","description":"CoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. The dataset is created using Mozilla’s open source Common Voice database of crowdsourced voice recordings.\n\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .mp3 format and is not converted to a float32 array. To convert, the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n\n```python\nimport torchaudio\n\ndef map_to_array(batch):\n    speech_array, _ = torchaudio.load(batch[\"file\"])\n    batch[\"speech\"] = speech_array.numpy()\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```","key":""},{"id":"craigslist_bargains","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:dialogue-modeling"],"citation":"@misc{he2018decoupling,\n    title={Decoupling Strategy and Generation in Negotiation Dialogues},\n    author={He He and Derek Chen and Anusha Balakrishnan and Percy Liang},\n    year={2018},\n    eprint={1808.09637},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"We study negotiation dialogues where two agents, a buyer and a seller,\nnegotiate over the price of an time for sale. We collected a dataset of more\nthan 6K negotiation dialogues over multiple categories of products scraped from Craigslist.\nOur goal is to develop an agent that negotiates with humans through such conversations.\nThe challenge is to handle both the negotiation strategy and the rich language for bargaining.","paperswithcode_id":"craigslistbargains","key":""},{"id":"crawl_domain","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","language_creators:expert-generated","language_creators:found","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other-Common-Crawl","source_datasets:original","task_categories:other","task_ids:other-other-text-to-speech","task_ids:other-other-web-search"],"citation":"@inproceedings{zrs2020urlsegmentation,\n  title={Semi-supervised URL Segmentation with Recurrent Neural Networks Pre-trained on Knowledge Graph Entities},\n  author={Hao Zhang and Jae Ro and Richard William Sproat},\n  booktitle={The 28th International Conference on Computational Linguistics (COLING 2020)},\n  year={2020}\n}","description":"Corpus of domain names scraped from Common Crawl and manually annotated to add word boundaries (e.g. \"commoncrawl\" to \"common crawl\"). Breaking domain names such as \"openresearch\" into component words \"open\" and \"research\" is important for applications such as Text-to-Speech synthesis and web search. Common Crawl is an open repository of web crawl data that can be accessed and analyzed by anyone. Specifically, we scraped the plaintext (WET) extracts for domain names from URLs that contained diverse letter casing (e.g. \"OpenBSD\"). Although in the previous example, segmentation is trivial using letter casing, this was not always the case (e.g. \"NASA\"), so we had to manually annotate the data. The dataset is stored as plaintext file where each line is an example of space separated segments of a domain name. The examples are stored in their original letter casing, but harder and more interesting examples can be generated by lowercasing the input first.","paperswithcode_id":"common-crawl-domain-names","key":""},{"id":"crd3","tags":["pretty_name:CRD3 (Critical Role Dungeons and Dragons Dataset)","annotations_creators:no-annotation","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_ids:summarization","task_ids:dialogue-modeling","size_categories:10K<n<100K"],"citation":"@inproceedings{\ntitle = {Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset},\nauthor = {Rameshkumar, Revanth  and Bailey, Peter},\nyear = {2020},\npublisher = {Association for Computational Linguistics},\nconference = {ACL}\n}","description":"Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset.\nCritical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game.\nThe dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding\nabstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player\ncollaboration and spoken interaction. For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail,\nand semantic ties to the previous dialogues.","paperswithcode_id":"crd3","key":""},{"id":"crime_and_punish","tags":["languages:en"],"description":"\\","key":""},{"id":"crows_pairs","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-scoring","task_ids:text-scoring-other-bias-evaluation"],"citation":"@inproceedings{nangia2020crows,\n    title = \"{CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models}\",\n    author = \"Nangia, Nikita  and\n      Vania, Clara  and\n      Bhalerao, Rasika  and\n      Bowman, Samuel R.\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\"\n}","description":"CrowS-Pairs, a challenge dataset for measuring the degree to which U.S. stereotypical biases present in the masked language models (MLMs).","paperswithcode_id":"crows-pairs","key":""},{"id":"cryptonite","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@misc{efrat2021cryptonite,\n      title={Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language},\n      author={Avia Efrat and Uri Shaham and Dan Kilman and Omer Levy},\n      year={2021},\n      eprint={2103.01242},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language\nCurrent NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite,\na large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each\nexample in Cryptonite is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving\nrequires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a\nchallenge even for experienced solvers, though top-tier experts can solve them with almost 100% accuracy. Cryptonite\nis a challenging task for current models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6% accuracy, on\npar with the accuracy of a rule-based clue solver (8.6%).","key":""},{"id":"cs_restaurants","tags":["annotations_creators:found","language_creators:expert-generated","language_creators:machine-generated","languages:cs","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-san-francisco-restaurants","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_ids:dialogue-modeling","task_ids:language-modeling","task_ids:other-stuctured-to-text"],"citation":"@article{DBLP:journals/corr/abs-1910-05298,\n  author    = {Ondrej Dusek and\n               Filip Jurcicek},\n  title     = {Neural Generation for Czech: Data and Baselines},\n  journal   = {CoRR},\n  volume    = {abs/1910.05298},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.05298},\n  archivePrefix = {arXiv},\n  eprint    = {1910.05298},\n  timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-05298.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"This is a dataset for NLG in task-oriented spoken dialogue systems with Czech as the target language. It originated as\na translation of the English San Francisco Restaurants dataset by Wen et al. (2015).","paperswithcode_id":"czech-restaurant-information","key":""},{"id":"cuad","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:closed-domain-qa","task_ids:extractive-qa"],"citation":"@article{hendrycks2021cuad,\n      title={CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review},\n      author={Dan Hendrycks and Collin Burns and Anya Chen and Spencer Ball},\n      journal={arXiv preprint arXiv:2103.06268},\n      year={2021}\n}","description":"Contract Understanding Atticus Dataset (CUAD) v1 is a corpus of more than 13,000 labels in 510\ncommercial legal contracts that have been manually labeled to identify 41 categories of important\nclauses that lawyers look for when reviewing contracts in connection with corporate transactions.","paperswithcode_id":"cuad","key":""},{"id":"curiosity_dialogs","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:dialogue-modeling","task_ids:sequence-modeling-other-conversational-curiosity"],"citation":"@inproceedings{rodriguez2020curiosity,\n    title = {Information Seeking in the Spirit of Learning: a Dataset for Conversational Curiosity},\n    author = {Pedro Rodriguez and Paul Crook and Seungwhan Moon and Zhiguang Wang},\n    year = 2020,\n    booktitle = {Empirical Methods in Natural Language Processing}\n}","description":"This dataset contains 14K dialogs (181K utterances) where users and assistants converse about geographic topics like\ngeopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog\nacts, grounding to Wikipedia, and user reactions to messages.","paperswithcode_id":"curiosity","key":""},{"id":"daily_dialog","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-label-classification","task_ids:text-classification-other-emotion-classification","task_ids:text-classification-other-dialog-act-classification"],"citation":"@InProceedings{li2017dailydialog,\n    author = {Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi},\n    title = {DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset},\n    booktitle = {Proceedings of The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017)},\n    year = {2017}\n}","description":"We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects.\nThe language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way\nand cover various topics about our daily life. We also manually label the developed dataset with communication\nintention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it\nbenefit the research field of dialog systems.","paperswithcode_id":"dailydialog","key":""},{"id":"dane","tags":["annotations_creators:expert-generated","language_creators:found","languages:da","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","task_categories:structure-prediction","task_ids:named-entity-recognition","task_ids:part-of-speech-tagging"],"citation":"@inproceedings{hvingelby-etal-2020-dane,\n    title = \"{D}a{NE}: A Named Entity Resource for {D}anish\",\n    author = \"Hvingelby, Rasmus  and\n      Pauli, Amalie Brogaard  and\n      Barrett, Maria  and\n      Rosted, Christina  and\n      Lidegaard, Lasse Malm  and\n      Søgaard, Anders\",\n    booktitle = \"Proceedings of the 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.565\",\n    pages = \"4597--4604\",\n    abstract = \"We present a named entity annotation for the Danish Universal Dependencies treebank using the CoNLL-2003 annotation scheme: DaNE. It is the largest publicly available, Danish named entity gold annotation. We evaluate the quality of our annotations intrinsically by double annotating the entire treebank and extrinsically by comparing our annotations to a recently released named entity annotation of the validation and test sections of the Danish Universal Dependencies treebank. We benchmark the new resource by training and evaluating competitive architectures for supervised named entity recognition (NER), including FLAIR, monolingual (Danish) BERT and multilingual BERT. We explore cross-lingual transfer in multilingual BERT from five related languages in zero-shot and direct transfer setups, and we show that even with our modestly-sized training set, we improve Danish NER over a recent cross-lingual approach, as well as over zero-shot transfer from five related languages. Using multilingual BERT, we achieve higher performance by fine-tuning on both DaNE and a larger Bokm{\\aa}l (Norwegian) training set compared to only using DaNE. However, the highest performance isachieved by using a Danish BERT fine-tuned on DaNE. Our dataset enables improvements and applicability for Danish NER beyond cross-lingual methods. We employ a thorough error analysis of the predictions of the best models for seen and unseen entities, as well as their robustness on un-capitalized text. The annotated dataset and all the trained models are made publicly available.\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}","description":"The DaNE dataset has been annotated with Named Entities for PER, ORG and LOC\nby the Alexandra Institute.\nIt is a reannotation of the UD-DDT (Universal Dependency - Danish Dependency Treebank)\nwhich has annotations for dependency parsing and part-of-speech (POS) tagging.\nThe Danish UD treebank (Johannsen et al., 2015, UD-DDT) is a conversion of\nthe Danish Dependency Treebank (Buch-Kromann et al. 2003) based on texts\nfrom Parole (Britt, 1998).","paperswithcode_id":"dane","key":""},{"id":"danish_political_comments","tags":["annotations_creators:expert-generated","language_creators:other","languages:da","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"description":"The dataset consists of 9008 sentences that are labelled with fine-grained polarity in the range from -2 to 2 (negative to postive). The quality of the fine-grained is not cross validated and is therefore subject to uncertainties; however, the simple polarity has been cross validated and therefore is considered to be more correct.","key":""},{"id":"dart","tags":["annotations_creators:crowdsourced","annotations_creators:machine-generated","language_creators:crowdsourced","language_creators:machine-generated","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|wikitable_questions","source_datasets:extended|wikisql","source_datasets:extended|web_nlg","source_datasets:extended|cleaned_e2e","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-rdf-to-text"],"citation":"@article{radev2020dart,\n  title={DART: Open-Domain Structured Data Record to Text Generation},\n  author={Dragomir Radev and Rui Zhang and Amrit Rau and Abhinand Sivaprasad and Chiachun Hsieh and Nazneen Fatema Rajani and Xiangru Tang and Aadit Vyas and Neha Verma and Pranav Krishna and Yangxiaokang Liu and Nadia Irwanto and Jessica Pan and Faiaz Rahman and Ahmad Zaidi and Murori Mutuma and Yasin Tarabar and Ankit Gupta and Tao Yu and Yi Chern Tan and Xi Victoria Lin and Caiming Xiong and Richard Socher},\n  journal={arXiv preprint arXiv:2007.02871},\n  year={2020}","description":"DART is a large and open-domain structured DAta Record to Text generation corpus with high-quality\nsentence annotations with each input being a set of entity-relation triples following a tree-structured ontology.\nIt consists of 82191 examples across different domains with each input being a semantic RDF triple set derived\nfrom data records in tables and the tree ontology of table schema, annotated with sentence description that\ncovers all facts in the triple set.\n\nDART is released in the following paper where you can find more details and baseline results:\nhttps://arxiv.org/abs/2007.02871","paperswithcode_id":"dart","key":""},{"id":"datacommons_factcheck","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:1K<n<10K","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_ids:fact-checking"],"citation":"@InProceedings{huggingface:dataset,\ntitle = {Data Commons 2019 Fact Checks},\nauthors={datacommons.org},\nyear={2019}\n}","description":"A dataset of fact checked claims by news media maintained by datacommons.org","key":""},{"id":"dbpedia_14","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:topic-classification"],"citation":"@article{lehmann2015dbpedia,\n  title={DBpedia--a large-scale, multilingual knowledge base extracted from Wikipedia},\n  author={Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas,\n  Dimitris and Mendes, Pablo N and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef,\n  Patrick and Auer, S{\\\"o}ren and others},\n  journal={Semantic web},\n  volume={6},\n  number={2},\n  pages={167--195},\n  year={2015},\n  publisher={IOS Press}\n}","description":"The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes\nfrom DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we\nrandomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size\nof the training dataset is 560,000 and testing dataset 70,000.\nThere are 3 columns in the dataset (same for train and test splits), corresponding to class index\n(1 to 14), title and content. The title and content are escaped using double quotes (\"), and any\ninternal double quote is escaped by 2 double quotes (\"\"). There are no new lines in title or content.","paperswithcode_id":"dbpedia","key":""},{"id":"dbrd","tags":["annotations_creators:found","language_creators:found","languages:nl","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_categories:text-classification","task_ids:language-modeling","task_ids:sentiment-classification"],"citation":"@article{DBLP:journals/corr/abs-1910-00896,\n  author    = {Benjamin van der Burgh and\n               Suzan Verberne},\n  title     = {The merits of Universal Language Model Fine-tuning for Small Datasets\n               - a case with Dutch book reviews},\n  journal   = {CoRR},\n  volume    = {abs/1910.00896},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.00896},\n  archivePrefix = {arXiv},\n  eprint    = {1910.00896},\n  timestamp = {Fri, 04 Oct 2019 12:28:06 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-00896.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"The Dutch Book Review Dataset (DBRD) contains over 110k book reviews of which 22k have associated binary sentiment polarity labels. It is intended as a benchmark for sentiment classification in Dutch and created due to a lack of annotated datasets in Dutch that are suitable for this task.","paperswithcode_id":"dbrd","key":""},{"id":"deal_or_no_dialog","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-dialogue-generation"],"citation":"@article{lewis2017deal,\n  title={Deal or no deal? end-to-end learning for negotiation dialogues},\n  author={Lewis, Mike and Yarats, Denis and Dauphin, Yann N and Parikh, Devi and Batra, Dhruv},\n  journal={arXiv preprint arXiv:1706.05125},\n  year={2017}\n}","description":"A large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other’s reward functions must reach anagreement (o a deal) via natural language dialogue.","paperswithcode_id":"negotiation-dialogues-dataset","key":""},{"id":"definite_pronoun_resolution","tags":["languages:en"],"citation":"@inproceedings{rahman2012resolving,\n  title={Resolving complex cases of definite pronouns: the winograd schema challenge},\n  author={Rahman, Altaf and Ng, Vincent},\n  booktitle={Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},\n  pages={777--789},\n  year={2012},\n  organization={Association for Computational Linguistics}\n}","description":"Composed by 30 students from one of the author's undergraduate classes. These\nsentence pairs cover topics ranging from real events (e.g., Iran's plan to\nattack the Saudi ambassador to the U.S.) to events/characters in movies (e.g.,\nBatman) and purely imaginary situations, largely reflecting the pop culture as\nperceived by the American kids born in the early 90s. Each annotated example\nspans four lines: the first line contains the sentence, the second line contains\nthe target pronoun, the third line contains the two candidate antecedents, and\nthe fourth line contains the correct antecedent. If the target pronoun appears\nmore than once in the sentence, its first occurrence is the one to be resolved.","paperswithcode_id":"definite-pronoun-resolution-dataset","key":""},{"id":"dengue_filipino","tags":["annotations_creators:crowdsourced","annotations_creators:machine-generated","language_creators:crowdsourced","languages:tl","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"citation":"    @INPROCEEDINGS{8459963,\n      author={E. D. {Livelo} and C. {Cheng}},\n      booktitle={2018 IEEE International Conference on Agents (ICA)},\n      title={Intelligent Dengue Infoveillance Using Gated Recurrent Neural Learning and Cross-Label Frequencies},\n      year={2018},\n      volume={},\n      number={},\n      pages={2-7},\n      doi={10.1109/AGENTS.2018.8459963}}\n    }","description":"    Benchmark dataset for low-resource multiclass classification, with 4,015 training, 500 testing, and 500 validation examples, each labeled as part of five classes. Each sample can be a part of multiple classes. Collected as tweets.","paperswithcode_id":"dengue","key":""},{"id":"dialog_re","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:other","task_categories:sequence-modeling","task_ids:other-other-relation-extraction","task_ids:dialogue-modeling"],"citation":"@inproceedings{yu2020dialogue,\n  title={Dialogue-Based Relation Extraction},\n  author={Yu, Dian and Sun, Kai and Cardie, Claire and Yu, Dong},\n  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  year={2020},\n  url={https://arxiv.org/abs/2004.08056v1}\n}","description":"DialogRE is the first human-annotated dialogue based relation extraction (RE) dataset aiming\nto support the prediction of relation(s) between two arguments that appear in a dialogue.\nThe dataset annotates all occurrences of 36 possible relation types that exist between pairs\nof arguments in the 1,788 dialogues originating from the complete transcripts of Friends.","paperswithcode_id":"dialogre","key":""},{"id":"diplomacy_detection","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification"],"citation":"@inproceedings{peskov-etal-2020-takes,\n    title = \"It Takes Two to Lie: One to Lie, and One to Listen\",\n    author = \"Peskov, Denis  and\n      Cheng, Benny  and\n      Elgohary, Ahmed  and\n      Barrow, Joe  and\n      Danescu-Niculescu-Mizil, Cristian  and\n      Boyd-Graber, Jordan\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.353\",\n    doi = \"10.18653/v1/2020.acl-main.353\",\n    pages = \"3811--3854\",\n    abstract = \"Trust is implicit in many online text conversations{---}striking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.\",\n}","key":""},{"id":"disaster_response_messages","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","languages:es","languages:fr","languages:ht","languages:ur","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-classification","task_ids:intent-classification","task_ids:sentiment-classification","task_ids:text-simplification"],"citation":"@inproceedings{title={Multilingual Disaster Response Messages}\n}","description":"This dataset contains 30,000 messages drawn from events including an earthquake in Haiti in 2010, an earthquake in Chile in 2010, floods in Pakistan in 2010, super-storm Sandy in the U.S.A. in 2012, and news articles spanning a large number of years and 100s of different disasters.\nThe data has been encoded with 36 different categories related to disaster response and has been stripped of messages with sensitive information in their entirety.\nUpon release, this is the featured dataset of a new Udacity course on Data Science and the AI4ALL summer school and is especially utile for text analytics and natural language processing (NLP) tasks and models.\nThe input data in this job contains thousands of untranslated disaster-related messages and their English translations.","key":""},{"id":"discofuse","tags":["languages:en"],"citation":"@InProceedings{GevaEtAl2019,\n  title = {DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion},\n  author = {Geva, Mor and Malmi, Eric and Szpektor, Idan and Berant, Jonathan},\n  booktitle = {Proceedings of the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics},\n  note = {arXiv preprint arXiv:1902.10526},\n  year = {2019}\n}","description":" DISCOFUSE is a large scale dataset for discourse-based sentence fusion.","paperswithcode_id":"discofuse","key":""},{"id":"discovery","tags":["annotations_creators:other","language_creators:other","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1M<n<10M","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-discourse-marker-prediction"],"citation":"@inproceedings{sileo-etal-2019-mining,\n    title = \"Mining Discourse Markers for Unsupervised Sentence Representation Learning\",\n    author = \"Sileo, Damien  and\n      Van De Cruys, Tim  and\n      Pradel, Camille  and\n      Muller, Philippe\",\n    booktitle = \"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\",\n    month = jun,\n    year = \"2019\",\n    address = \"Minneapolis, Minnesota\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/N19-1351\",\n    pages = \"3477--3486\",\n    abstract = \"Current state of the art systems in NLP heavily rely on manually annotated datasets, which are expensive to construct. Very little work adequately exploits unannotated data {--} such as discourse markers between sentences {--} mainly because of data sparseness and ineffective extraction methods. In the present work, we propose a method to automatically discover sentence pairs with relevant discourse markers, and apply it to massive amounts of data. Our resulting dataset contains 174 discourse markers with at least 10k examples each, even for rare markers such as {``}coincidentally{''} or {``}amazingly{''}. We use the resulting data as supervision for learning transferable sentence embeddings. In addition, we show that even though sentence representation learning through prediction of discourse marker yields state of the art results across different transfer tasks, it{'}s not clear that our models made use of the semantic relation between sentences, thus leaving room for further improvements.\",\n}","paperswithcode_id":"discovery","key":""},{"id":"doc2dial","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:closed-domain-qa"],"citation":"@inproceedings{feng-etal-2020-doc2dial,\n    title = \"doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset\",\n    author = \"Feng, Song  and Wan, Hui  and Gunasekara, Chulaka  and Patel, Siva  and Joshi, Sachindra  and Lastras, Luis\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.652\",\n}","description":"Doc2dial is dataset of goal-oriented dialogues that are grounded in the associated documents. It includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains. Compared to the prior document-grounded dialogue datasets this dataset covers a variety of dialogue scenes in information-seeking conversations.","paperswithcode_id":"doc2dial","key":""},{"id":"docred","tags":["languages:en"],"citation":"@inproceedings{yao2019DocRED,\n  title={{DocRED}: A Large-Scale Document-Level Relation Extraction Dataset},\n  author={Yao, Yuan and Ye, Deming and Li, Peng and Han, Xu and Lin, Yankai and Liu, Zhenghao and Liu,   Zhiyuan and Huang, Lixin and Zhou, Jie and Sun, Maosong},\n  booktitle={Proceedings of ACL 2019},\n  year={2019}\n}","description":"Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features:\n    - DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text.\n    - DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document.\n    - Along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios.","paperswithcode_id":"docred","key":""},{"id":"doqa","tags":["languages:en"],"citation":"@misc{campos2020doqa,\n    title={DoQA -- Accessing Domain-Specific FAQs via Conversational QA},\n    author={Jon Ander Campos and Arantxa Otegi and Aitor Soroa and Jan Deriu and Mark Cieliebak and Eneko Agirre},\n    year={2020},\n    eprint={2005.01328},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"DoQA is a dataset for accessing Domain Specific FAQs via conversational QA that contains 2,437 information-seeking question/answer dialogues\n(10,917 questions in total) on three different domains: cooking, travel and movies. Note that we include in the generic concept of FAQs also\nCommunity Question Answering sites, as well as corporate information in intranets which is maintained in textual form similar to FAQs, often\nreferred to as internal “knowledge bases”.\n\nThese dialogues are created by crowd workers that play the following two roles: the user who asks questions about a given topic posted in Stack\nExchange (https://stackexchange.com/), and the domain expert who replies to the questions by selecting a short span of text from the long textual\nreply in the original post. The expert can rephrase the selected span, in order to make it look more natural. The dataset covers unanswerable\nquestions and some relevant dialogue acts.\n\nDoQA enables the development and evaluation of conversational QA systems that help users access the knowledge buried in domain specific FAQs.","paperswithcode_id":"doqa","key":""},{"id":"dream","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@article{sundream2018,\n  title={{DREAM}: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension},\n  author={Sun, Kai and Yu, Dian and Chen, Jianshu and Yu, Dong and Choi, Yejin and Cardie, Claire},\n  journal={Transactions of the Association for Computational Linguistics},\n  year={2019},\n  url={https://arxiv.org/abs/1902.00164v1}\n}","description":"DREAM is a multiple-choice Dialogue-based REAding comprehension exaMination dataset. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding.","paperswithcode_id":"dream","key":""},{"id":"drop","tags":["pretty_name:DROP","annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa","task_ids:abstractive-qa"],"citation":"@inproceedings{Dua2019DROP,\n  author={Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},\n  title={DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},\n  booktitle={Proc. of NAACL},\n  year={2019}\n}","description":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs.\n. DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a\nquestion, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or\n sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was\n necessary for prior datasets.","paperswithcode_id":"drop","key":""},{"id":"duorc","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:mit","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:abstractive-qa","task_ids:extractive-qa"],"citation":"@inproceedings{DuoRC,\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\nyear = {2018}\n}","description":"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.","paperswithcode_id":"duorc","key":""},{"id":"dutch_social","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:en","languages:nl","licenses:cc-by-nc-4.0","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification","task_ids:multi-label-classification"],"citation":"@data{FK2/MTPTL7_2020,\nauthor = {Gupta, Aakash},\npublisher = {COVID-19 Data Hub},\ntitle = {{Dutch social media collection}},\nyear = {2020},\nversion = {DRAFT VERSION},\ndoi = {10.5072/FK2/MTPTL7},\nurl = {https://doi.org/10.5072/FK2/MTPTL7}\n}","description":"The dataset contains around 271,342 tweets. The tweets are filtered via the official Twitter API to\ncontain tweets in Dutch language or by users who have specified their location information within Netherlands\ngeographical boundaries. Using natural language processing we have classified the tweets for their HISCO codes.\nIf the user has provided their location within Dutch boundaries, we have also classified them to their respective\nprovinces The objective of this dataset is to make research data available publicly in a FAIR (Findable, Accessible,\nInteroperable, Reusable) way. Twitter's Terms of Service Licensed under Attribution-NonCommercial 4.0 International\n(CC BY-NC 4.0) (2020-10-27)","key":""},{"id":"dyk","tags":["annotations_creators:expert-generated","language_creators:other","languages:pl","licenses:bsd-3-clause","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@inproceedings{marcinczuk2013open,\ntitle={Open dataset for development of Polish Question Answering systems},\nauthor={Marcinczuk, Michal and Ptak, Marcin and Radziszewski, Adam and Piasecki, Maciej},\nbooktitle={Proceedings of the 6th Language & Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics, Wydawnictwo Poznanskie, Fundacja Uniwersytetu im. Adama Mickiewicza},\nyear={2013}\n}","description":"The Did You Know (pol. Czy wiesz?) dataset consists of human-annotated question-answer pairs. The task is to predict if the answer is correct. We chose the negatives which have the largest token overlap with a question.","key":""},{"id":"e2e_nlg","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-meaning-representtion-to-text"],"citation":"@article{dusek.etal2020:csl,\n  title = {Evaluating the {{State}}-of-the-{{Art}} of {{End}}-to-{{End Natural Language Generation}}: {{The E2E NLG Challenge}}},\n  author = {Du{\\v{s}}ek, Ond\\v{r}ej and Novikova, Jekaterina and Rieser, Verena},\n  year = {2020},\n  month = jan,\n  volume = {59},\n  pages = {123--156},\n  doi = {10.1016/j.csl.2019.06.009},\n  archivePrefix = {arXiv},\n  eprint = {1901.11528},\n  eprinttype = {arxiv},\n  journal = {Computer Speech & Language}\n}","description":"The E2E dataset is used for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area.\nThe E2E dataset poses new challenges:\n(1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena;\n(2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances.\n\nE2E is released in the following paper where you can find more details and baseline results:\nhttps://arxiv.org/abs/1706.09254","paperswithcode_id":"e2e","key":""},{"id":"e2e_nlg_cleaned","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-meaning-representtion-to-text"],"citation":"@inproceedings{dusek-etal-2019-semantic,\n    title = \"Semantic Noise Matters for Neural Natural Language Generation\",\n    author = \"Du{\\v{s}}ek, Ond{\\v{r}}ej  and\n      Howcroft, David M.  and\n      Rieser, Verena\",\n    booktitle = \"Proceedings of the 12th International Conference on Natural Language Generation\",\n    month = oct # \"{--}\" # nov,\n    year = \"2019\",\n    address = \"Tokyo, Japan\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W19-8652\",\n    doi = \"10.18653/v1/W19-8652\",\n    pages = \"421--426\"\n}","description":"An update release of E2E NLG Challenge data with cleaned MRs and scripts, accompanying the following paper:\n\nOndřej Dušek, David M. Howcroft, and Verena Rieser (2019): Semantic Noise Matters for Neural Natural Language Generation. In INLG, Tokyo, Japan.","key":""},{"id":"ecb","tags":["annotations_creators:found","language_creators:found","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:fi","languages:fr","languages:hu","languages:it","languages:lt","languages:lv","languages:mt","languages:nl","languages:pl","languages:pt","languages:sk","languages:sl","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J�rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"Original source: Website and documentatuion from the European Central Bank, compiled and made available by Alberto Simoes (thank you very much!)\n19 languages, 170 bitexts\ntotal number of files: 340\ntotal number of tokens: 757.37M\ntotal number of sentence fragments: 30.55M","paperswithcode_id":"ecb","key":""},{"id":"ecthr_cases","tags":["annotations_creators:expert-generated","annotations_creators:found","language_creators:found","languages:en","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-label-classification","task_ids:text-classification-other-rationale-extraction","task_ids:text-classification-other-legal-judgment-prediction"],"citation":"@InProceedings{chalkidis-et-al-2021-ecthr,\n    title = \"Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases\",\n    author = \"Chalkidis, Ilias and Fergadiotis, Manos and Tsarapatsanis, Dimitrios and Aletras, Nikolaos and Androutsopoulos, Ion and Malakasiotis, Prodromos\",\n    booktitle = \"Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics\",\n    year = \"2021\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\"\n}","description":"The ECtHR Cases dataset is designed for experimentation of neural judgment prediction and rationale extraction considering ECtHR cases.","paperswithcode_id":"ecthr","key":""},{"id":"eduge","tags":["pretty_name:Eduge","annotations_creators:expert-generated","language_creators:expert-generated","languages:mn","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"description":"Eduge news classification dataset is provided by Bolorsoft LLC. It is used for training the Eduge.mn production news classifier\n75K news articles in 9 categories: урлаг соёл, эдийн засаг, эрүүл мэнд, хууль, улс төр, спорт, технологи, боловсрол and байгал орчин","key":""},{"id":"ehealth_kd","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:es","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition","task_ids:structure-prediction-other-relation-prediction"],"citation":"@inproceedings{overview_ehealthkd2020,\n  author    = {Piad{-}Morffis, Alejandro and\n               Guti{\\'{e}}rrez, Yoan and\n               Cañizares-Diaz, Hian and\n               Estevez{-}Velarde, Suilan and\n               Almeida{-}Cruz, Yudivi{\\'{a}}n and\n               Muñoz, Rafael and\n               Montoyo, Andr{\\'{e}}s},\n  title     = {Overview of the eHealth Knowledge Discovery Challenge at IberLEF 2020},\n  booktitle = ,\n  year      = {2020},\n}","description":"Dataset of the eHealth Knowledge Discovery Challenge at IberLEF 2020. It is designed for\nthe identification of semantic entities and relations in Spanish health documents.","key":""},{"id":"eitb_parcc","tags":["annotations_creators:found","language_creators:found","languages:es","languages:eu","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"EiTB-ParCC: Parallel Corpus of Comparable News. A Basque-Spanish parallel corpus provided by Vicomtech (https://www.vicomtech.org), extracted from comparable news produced by the Basque public broadcasting group Euskal Irrati Telebista.","paperswithcode_id":"eitb-parcc","key":""},{"id":"eli5","tags":["annotations_creators:no-annotation","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:question-answering","task_ids:abstractive-qa","task_ids:open-domain-qa"],"citation":"@inproceedings{DBLP:conf/acl/FanJPGWA19,\n  author    = {Angela Fan and\n               Yacine Jernite and\n               Ethan Perez and\n               David Grangier and\n               Jason Weston and\n               Michael Auli},\n  editor    = {Anna Korhonen and\n               David R. Traum and\n               Lluis Marquez},\n  title     = {{ELI5:} Long Form Question Answering},\n  booktitle = {Proceedings of the 57th Conference of the Association for Computational\n               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,\n               Volume 1: Long Papers},\n  pages     = {3558--3567},\n  publisher = {Association for Computational Linguistics},\n  year      = {2019},\n  url       = {https://doi.org/10.18653/v1/p19-1346},\n  doi       = {10.18653/v1/p19-1346},\n}","description":"Explain Like I'm 5 long form QA dataset","paperswithcode_id":"eli5","key":""},{"id":"emea","tags":["annotations_creators:found","language_creators:found","languages:bg","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:fi","languages:fr","languages:hu","languages:it","languages:lt","languages:lv","languages:mt","languages:nl","languages:pl","languages:pt","languages:ro","languages:sk","languages:sl","languages:sv","licenses:unknown","multilinguality:multilingual","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation","pretty_name:EMEA"],"citation":"J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)","description":"This is a parallel corpus made out of PDF documents from the European Medicines Agency. All files are automatically converted from PDF to plain text using pdftotext with the command line arguments -layout -nopgbrk -eol unix. There are some known problems with tables and multi-column layouts - some of them are fixed in the current version.\n\nsource: http://www.emea.europa.eu/\n\n22 languages, 231 bitexts\ntotal number of files: 41,957\ntotal number of tokens: 311.65M\ntotal number of sentence fragments: 26.51M","key":""},{"id":"emo","tags":["languages:en"],"citation":"@inproceedings{chatterjee-etal-2019-semeval,\n    title={SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text},\n    author={Ankush Chatterjee and Kedhar Nath Narahari and Meghana Joshi and Puneet Agrawal},\n    booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation},\n    year={2019},\n    address={Minneapolis, Minnesota, USA},\n    publisher={Association for Computational Linguistics},\n    url={https://www.aclweb.org/anthology/S19-2005},\n    doi={10.18653/v1/S19-2005},\n    pages={39--48},\n    abstract={In this paper, we present the SemEval-2019 Task 3 - EmoContext: Contextual Emotion Detection in Text. Lack of facial expressions and voice modulations make detecting emotions in text a challenging problem. For instance, as humans, on reading ''Why don't you ever text me!'' we can either interpret it as a sad or angry emotion and the same ambiguity exists for machines. However, the context of dialogue can prove helpful in detection of the emotion. In this task, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry and Others. To facilitate the participation in this task, textual dialogues from user interaction with a conversational agent were taken and annotated for emotion classes after several data processing steps. A training data set of 30160 dialogues, and two evaluation data sets, Test1 and Test2, containing 2755 and 5509 dialogues respectively were released to the participants. A total of 311 teams made submissions to this task. The final leader-board was evaluated on Test2 data set, and the highest ranked submission achieved 79.59 micro-averaged F1 score. Our analysis of systems submitted to the task indicate that Bi-directional LSTM was the most common choice of neural architecture used, and most of the systems had the best performance for the Sad emotion class, and the worst for the Happy emotion class}\n}","description":"In this dataset, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry and Others.","paperswithcode_id":"emocontext","key":""},{"id":"emotion","tags":["pretty_name:Emotion","annotations_creators:machine-generated","language_creators:machine-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification","task_ids:text-classification-other-emotion-classification"],"citation":"@inproceedings{saravia-etal-2018-carer,\n    title = \"{CARER}: Contextualized Affect Representations for Emotion Recognition\",\n    author = \"Saravia, Elvis  and\n      Liu, Hsien-Chi Toby  and\n      Huang, Yen-Hao  and\n      Wu, Junlin  and\n      Chen, Yi-Shin\",\n    booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\",\n    month = oct # \"-\" # nov,\n    year = \"2018\",\n    address = \"Brussels, Belgium\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D18-1404\",\n    doi = \"10.18653/v1/D18-1404\",\n    pages = \"3687--3697\",\n    abstract = \"Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand emotion, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. We propose a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text. The pattern-based representations are further enriched with word embeddings and evaluated through several emotion recognition tasks. Our experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on emotion recognition tasks.\",\n}","description":"Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper.","paperswithcode_id":"emotion","key":""},{"id":"emotone_ar","tags":["annotations_creators:found","language_creators:found","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:emotion-classification"],"citation":"@inbook{inbook,\nauthor = {Al-Khatib, Amr and El-Beltagy, Samhaa},\nyear = {2018},\nmonth = {01},\npages = {105-114},\ntitle = {Emotional Tone Detection in Arabic Tweets: 18th International Conference, CICLing 2017, Budapest, Hungary, April 17–23, 2017, Revised Selected Papers, Part II},\nisbn = {978-3-319-77115-1},\ndoi = {10.1007/978-3-319-77116-8_8}\n}","description":"Dataset of 10065 tweets in Arabic for Emotion detection in Arabic text","key":""},{"id":"empathetic_dialogues","tags":["languages:en"],"citation":"@inproceedings{rashkin2019towards,\n  title = {Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset},\n  author = {Hannah Rashkin and Eric Michael Smith and Margaret Li and Y-Lan Boureau},\n  booktitle = {ACL},\n  year = {2019},\n}","description":"PyTorch original implementation of Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset","paperswithcode_id":"empatheticdialogues","key":""},{"id":"enriched_web_nlg","tags":["annotations_creators:found","language_creators:crowdsourced","languages:de","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-web-nlg","task_categories:conditional-text-generation","task_ids:other-stuctured-to-text"],"citation":"@InProceedings{ferreiraetal2018,\n  author = \t\"Castro Ferreira, Thiago and Moussallem, Diego and Wubben, Sander and Krahmer, Emiel\",\n  title = \t\"Enriching the WebNLG corpus\",\n  booktitle = \t\"Proceedings of the 11th International Conference on Natural Language Generation\",\n  year = \t\"2018\",\n  series = {INLG'18},\n  publisher = \t\"Association for Computational Linguistics\",\n  address = \t\"Tilburg, The Netherlands\",\n}","description":"WebNLG is a valuable resource and benchmark for the Natural Language Generation (NLG) community. However, as other NLG benchmarks, it only consists of a collection of parallel raw representations and their corresponding textual realizations. This work aimed to provide intermediate representations of the data for the development and evaluation of popular tasks in the NLG pipeline architecture (Reiter and Dale, 2000), such as Discourse Ordering, Lexicalization, Aggregation and Referring Expression Generation.","key":""},{"id":"eraser_multi_rc","tags":["languages:en"],"citation":"@unpublished{eraser2019,\n    title = {ERASER: A Benchmark to Evaluate Rationalized NLP Models},\n    author = {Jay DeYoung and Sarthak Jain and Nazneen Fatema Rajani and Eric Lehman and Caiming Xiong and Richard Socher and Byron C. Wallace}\n}\n@inproceedings{MultiRC2018,\n    author = {Daniel Khashabi and Snigdha Chaturvedi and Michael Roth and Shyam Upadhyay and Dan Roth},\n    title = {Looking Beyond the Surface:A Challenge Set for Reading Comprehension over Multiple Sentences},\n    booktitle = {NAACL},\n    year = {2018}\n}","description":"Eraser Multi RC is a dataset for queries over multi-line passages, along with\nanswers and a rationalte. Each example in this dataset has the following 5 parts\n1. A Mutli-line Passage\n2. A Query about the passage\n3. An Answer to the query\n4. A Classification as to whether the answer is right or wrong\n5. An Explanation justifying the classification","key":""},{"id":"esnli","tags":["languages:en"],"citation":"@incollection{NIPS2018_8163,\ntitle = {e-SNLI: Natural Language Inference with Natural Language Explanations},\nauthor = {Camburu, Oana-Maria and Rockt\\\"{a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},\nbooktitle = {Advances in Neural Information Processing Systems 31},\neditor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},\npages = {9539--9549},\nyear = {2018},\npublisher = {Curran Associates, Inc.},\nurl = {http://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations.pdf}\n}","description":"The e-SNLI dataset extends the Stanford Natural Language Inference Dataset to\ninclude human-annotated natural language explanations of the entailment\nrelations.","paperswithcode_id":"e-snli","key":""},{"id":"eth_py150_open","tags":["annotations_creators:no-annotations","language_creators:machine-generated","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:other","task_ids:other-other-contextual-embeddings"],"citation":"@inproceedings{kanade2020learning,\n  title={Learning and Evaluating Contextual Embedding of Source Code},\n  author={Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},\n  booktitle={International Conference on Machine Learning},\n  pages={5110--5121},\n  year={2020},\n  organization={PMLR}\n}","description":"A redistributable subset of the ETH Py150 corpus, introduced in the ICML 2020 paper 'Learning and Evaluating Contextual Embedding of Source Code'","paperswithcode_id":"eth-py150-open","key":""},{"id":"ethos","tags":["annotations_creators:crowdsourced","annotations_creators:expert-generated","language_creators:found, other","languages:en","licenses:agpl-3.0-or-later","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification","task_ids:text-classification-other-Hate Speech Detection","task_ids:multi-label-classification"],"citation":"@misc{mollas2020ethos,\n      title={ETHOS: an Online Hate Speech Detection Dataset},\n      author={Ioannis Mollas and Zoe Chrysopoulou and Stamatis Karlos and Grigorios Tsoumakas},\n      year={2020},\n      eprint={2006.08328},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"ETHOS: onlinE haTe speecH detectiOn dataSet. This repository contains a dataset for hate speech\ndetection on social media platforms, called Ethos. There are two variations of the dataset:\n\nEthos_Dataset_Binary: contains 998 comments in the dataset alongside with a label\nabout hate speech presence or absence. 565 of them do not contain hate speech,\nwhile the rest of them, 433, contain.\n\nEthos_Dataset_Multi_Label: which contains 8 labels for the 433 comments with hate speech content.\nThese labels are violence (if it incites (1) or not (0) violence), directed_vs_general (if it is\ndirected to a person (1) or a group (0)), and 6 labels about the category of hate speech like,\ngender, race, national_origin, disability, religion and sexual_orientation.","paperswithcode_id":"ethos","key":""},{"id":"eu_regulatory_ir","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-retrieval","task_ids:document-retrieval","task_ids:text-retrieval-other-document-to-document-retrieval"],"citation":"@inproceedings{chalkidis-etal-2021-regir,\n    title = \"Regulatory Compliance through Doc2Doc Information Retrieval: A case study in EU/UK legislation where text similarity has limitations\",\n    author = \"Chalkidis, Ilias  and Fergadiotis, Emmanouil and Manginas, Nikos and Katakalou, Eva,  and Malakasiotis, Prodromos\",\n    booktitle = \"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2021)\",\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/2101.10726\",\n}","description":"EURegIR: Regulatory Compliance IR (EU/UK)","key":""},{"id":"eurlex","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-label-classification","task_ids:text-classification-legal-topic-classification"],"citation":"@inproceedings{chalkidis-etal-2019-large,\n    title = \"Large-Scale Multi-Label Text Classification on {EU} Legislation\",\n    author = \"Chalkidis, Ilias  and Fergadiotis, Emmanouil  and Malakasiotis, Prodromos  and Androutsopoulos, Ion\",\n    booktitle = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\",\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P19-1636\",\n    doi = \"10.18653/v1/P19-1636\",\n    pages = \"6314--6322\"\n}","description":"EURLEX57K contains 57k legislative documents in English from EUR-Lex portal, annotated with EUROVOC concepts.","paperswithcode_id":"eurlex57k","key":""},{"id":"euronews","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:de","languages:fr","languages:nl","licenses:cc0-1.0","multilinguality:multilingual","size_categories:n<1K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@InProceedings{NEUDECKER16.110,\n  author = {Clemens Neudecker},\n  title = {An Open Corpus for Named Entity Recognition in Historic Newspapers},\n  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},\n  year = {2016},\n  month = {may},\n  date = {23-28},\n  location = {Portorož, Slovenia},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Helene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  address = {Paris, France},\n  isbn = {978-2-9517408-9-1},\n  language = {english}\n }","description":"The corpora comprise of files per data provider that are encoded in the IOB format (Ramshaw & Marcus, 1995). The IOB format is a simple text chunking format that divides texts into single tokens per line, and, separated by a whitespace, tags to mark named entities. The most commonly used categories for tags are PER (person), LOC (location) and ORG (organization). To mark named entities that span multiple tokens, the tags have a prefix of either B- (beginning of named entity) or I- (inside of named entity). O (outside of named entity) tags are used to mark tokens that are not a named entity.","paperswithcode_id":"europeana-newspapers","key":""},{"id":"europa_eac_tm","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:bg","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:fi","languages:fr","languages:hr","languages:hu","languages:is","languages:it","languages:lt","languages:lv","languages:mt","languages:nl","languages:no","languages:pl","languages:pt","languages:ro","languages:sk","languages:sl","languages:sv","languages:tr","licenses:cc-by-4.0","multilinguality:translation","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@Article{Steinberger2014,\n        author={Steinberger, Ralf\n                and Ebrahim, Mohamed\n                and Poulis, Alexandros\n                and Carrasco-Benitez, Manuel\n                and Schl{\\\"u}ter, Patrick\n                and Przybyszewski, Marek\n                and Gilbro, Signe},\n        title={An overview of the European Union's highly multilingual parallel corpora},\n        journal={Language Resources and Evaluation},\n        year={2014},\n        month={Dec},\n        day={01},\n        volume={48},\n        number={4},\n        pages={679-707},\n        issn={1574-0218},\n        doi={10.1007/s10579-014-9277-0},\n        url={https://doi.org/10.1007/s10579-014-9277-0}\n}","description":"In October 2012, the European Union's (EU) Directorate General for Education and Culture ( DG EAC) released a translation memory (TM), i.e. a collection of sentences and their professionally produced translations, in twenty-six languages. This resource bears the name EAC Translation Memory, short EAC-TM.\n\nEAC-TM covers up to 26 languages: 22 official languages of the EU (all except Irish) plus Icelandic, Croatian, Norwegian and Turkish. EAC-TM thus contains translations from English into the following 25 languages: Bulgarian, Czech, Danish, Dutch, Estonian, German, Greek, Finnish, French, Croatian, Hungarian, Icelandic, Italian, Latvian, Lithuanian, Maltese, Norwegian, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish, Swedish and Turkish.\n\nAll documents and sentences were originally written in English (source language is English) and then translated into the other languages. The texts were translated by staff of the National Agencies of the Lifelong Learning and Youth in Action programmes. They are typically professionals in the field of education/youth and EU programmes. They are thus not professional translators, but they are normally native speakers of the target language.","key":""},{"id":"europa_ecdc_tm","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:bg","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:fi","languages:fr","languages:ga","languages:hu","languages:is","languages:it","languages:lt","languages:lv","languages:mt","languages:nl","languages:no","languages:pl","languages:pt","languages:ro","languages:sk","languages:sl","languages:sv","licenses:cc-by-sa-4.0","multilinguality:translation","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@Article{Steinberger2014,\n        author={Steinberger, Ralf\n                and Ebrahim, Mohamed\n                and Poulis, Alexandros\n                and Carrasco-Benitez, Manuel\n                and Schl{\\\"u}ter, Patrick\n                and Przybyszewski, Marek\n                and Gilbro, Signe},\n        title={An overview of the European Union's highly multilingual parallel corpora},\n        journal={Language Resources and Evaluation},\n        year={2014},\n        month={Dec},\n        day={01},\n        volume={48},\n        number={4},\n        pages={679-707},\n        issn={1574-0218},\n        doi={10.1007/s10579-014-9277-0},\n        url={https://doi.org/10.1007/s10579-014-9277-0}\n}","description":"In October 2012, the European Union (EU) agency 'European Centre for Disease Prevention and Control' (ECDC) released a translation memory (TM), i.e. a collection of sentences and their professionally produced translations, in twenty-five languages. This resource bears the name EAC Translation Memory, short EAC-TM.\nECDC-TM covers 25 languages: the 23 official languages of the EU plus Norwegian (Norsk) and Icelandic. ECDC-TM was created by translating from English into the following 24 languages: Bulgarian, Czech, Danish, Dutch, English, Estonian, Gaelige (Irish), German, Greek, Finnish, French, Hungarian, Icelandic, Italian, Latvian, Lithuanian, Maltese, Norwegian (NOrsk), Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish and Swedish.\nAll documents and sentences were thus originally written in English. They were then translated into the other languages by professional translators from the Translation Centre CdT in Luxembourg.","key":""},{"id":"europarl_bilingual","tags":["annotations_creators:found","language_creators:found","languages:bg","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:fi","languages:fr","languages:hu","languages:it","languages:lt","languages:lv","languages:nl","languages:pl","languages:pt","languages:ro","languages:sk","languages:sl","languages:sv","licenses:unknown","multilinguality:translation","size_categories:100K<n<1M","source_datasets:original","task_categories:other","task_ids:machine-translation"],"description":"A parallel corpus extracted from the European Parliament web site by Philipp Koehn (University of Edinburgh). The main intended use is to aid statistical machine translation research.","key":""},{"id":"event2Mind","tags":["languages:en"],"citation":"@inproceedings{event2Mind,\n    title={Event2Mind: Commonsense Inference on Events, Intents, and Reactions},\n    author={Hannah Rashkin and Maarten Sap and Emily Allaway and Noah A. Smith† Yejin Choi},\n    year={2018}\n}","description":"In Event2Mind, we explore the task of understanding stereotypical intents and reactions to events. Through crowdsourcing, we create a large corpus with 25,000 events and free-form descriptions of their intents and reactions, both of the event's subject and (potentially implied) other participants.","paperswithcode_id":"event2mind","key":""},{"id":"evidence_infer_treatment","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-retrieval","task_ids:fact-checking-retrieval"],"citation":"@inproceedings{lehman-etal-2019-inferring,\n    title = \"Inferring Which Medical Treatments Work from Reports of Clinical Trials\",\n    author = \"Lehman, Eric  and\n      DeYoung, Jay  and\n      Barzilay, Regina  and\n      Wallace, Byron C.\",\n    booktitle = \"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\",\n    month = jun,\n    year = \"2019\",\n    address = \"Minneapolis, Minnesota\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/N19-1371\",\n    pages = \"3705--3717\",\n}","description":"Data and code from our \"Inferring Which Medical Treatments Work from Reports of Clinical Trials\", NAACL 2019. This work concerns inferring the results reported in clinical trials from text.\n\nThe dataset consists of biomedical articles describing randomized control trials (RCTs) that compare multiple treatments. Each of these articles will have multiple questions, or 'prompts' associated with them. These prompts will ask about the relationship between an intervention and comparator with respect to an outcome, as reported in the trial. For example, a prompt may ask about the reported effects of aspirin as compared to placebo on the duration of headaches. For the sake of this task, we assume that a particular article will report that the intervention of interest either significantly increased, significantly decreased or had significant effect on the outcome, relative to the comparator.\n\nThe dataset could be used for automatic data extraction of the results of a given RCT. This would enable readers to discover the effectiveness of different treatments without needing to read the paper.","key":""},{"id":"exams","tags":["annotations_creators:found","language_creators:found","languages:ar","languages:bg","languages:de","languages:es","languages:fr","languages:hr","languages:hu","languages:it","languages:lt","languages:mk","languages:pl","languages:pt","languages:sq","languages:sr","languages:tr","languages:vi","licenses:cc-by-sa-4.0","multilinguality:multilingual","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:1K<n<10K","size_categories:n<1K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@article{hardalov2020exams,\n  title={EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering},\n  author={Hardalov, Momchil and Mihaylov, Todor and Dimitrina Zlatkova and Yoan Dinkov and Ivan Koychev and Preslav Nvakov},\n  journal={arXiv preprint arXiv:2011.03080},\n  year={2020}\n}","description":"EXAMS is a benchmark dataset for multilingual and cross-lingual question answering from high school examinations.\nIt consists of more than 24,000 high-quality high school exam questions in 16 languages,\ncovering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.","paperswithcode_id":"exams","key":""},{"id":"factckbr","tags":["annotations_creators:expert-generated","language_creators:found","languages:pt","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:fact-checking"],"citation":"@inproceedings{10.1145/3323503.3361698,\n    author = {Moreno, Jo\\\\~{a}o and Bressan, Gra\\\\c{c}a},\n    title = {FACTCK.BR: A New Dataset to Study Fake News},\n    year = {2019},\n    isbn = {9781450367639},\n    publisher = {Association for Computing Machinery},\n    address = {New York, NY, USA},\n    url = {https://doi.org/10.1145/3323503.3361698},\n    doi = {10.1145/3323503.3361698},\n    abstract = {Machine learning algorithms can be used to combat fake news propagation. For the news classification, labeled datasets are required, however, among the existing datasets, few separate verified false from skewed ones with a good variety of sources. This work presents FACTCK.BR, a new dataset to study Fake News in Portuguese, presenting a supposedly false News along with their respective fact check and classification. The data is collected from the ClaimReview, a structured data schema used by fact check agencies to share their results in search engines, enabling data collect in real time.},\n    booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},\n    pages = {525–527},\n    numpages = {3},\n    keywords = {fake news, fact check, information extraction, dataset},\n    location = {Rio de Janeiro, Brazil},\n    series = {WebMedia '19}\n}","description":"A dataset to study Fake News in Portuguese, presenting a supposedly false News along with their respective fact check and classification.\nThe data is collected from the ClaimReview, a structured data schema used by fact check agencies to share their results in search engines, enabling data collect in real time.\nThe FACTCK.BR dataset contains 1309 claims with its corresponding label.","key":""},{"id":"fake_news_english","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_ids:multi-label-classification"],"citation":"@inproceedings{inproceedings,\nauthor = {Golbeck, Jennifer and Everett, Jennine and Falak, Waleed and Gieringer, Carl and Graney, Jack and Hoffman, Kelly and Huth, Lindsay and Ma, Zhenya and Jha, Mayanka and Khan, Misbah and Kori, Varsha and Mauriello, Matthew and Lewis, Elo and Mirano, George and IV, William and Mussenden, Sean and Nelson, Tammie and Mcwillie, Sean and Pant, Akshat and Cheakalos, Paul},\nyear = {2018},\nmonth = {05},\npages = {17-21},\ntitle = {Fake News vs Satire: A Dataset and Analysis},\ndoi = {10.1145/3201064.3201100}\n}","description":"Fake news has become a major societal issue and a technical challenge for social media companies to identify. This content is difficult to identify because the term \"fake news\" covers intentionally false, deceptive stories as well as factual errors, satire, and sometimes, stories that a person just does not like. Addressing the problem requires clear definitions and examples. In this work, we present a dataset of fake news and satire stories that are hand coded, verified, and, in the case of fake news, include rebutting stories. We also include a thematic content analysis of the articles, identifying major themes that include hyperbolic support or condemnation of a gure, conspiracy theories, racist themes, and discrediting of reliable sources. In addition to releasing this dataset for research use, we analyze it and show results based on language that are promising for classification purposes. Overall, our contribution of a dataset and initial analysis are designed to support future work by fake news researchers.","key":""},{"id":"fake_news_filipino","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:tl","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:fact-checking"],"citation":"    @inproceedings{cruz2020localization,\n      title={Localization of Fake News Detection via Multitask Transfer Learning},\n      author={Cruz, Jan Christian Blaise and Tan, Julianne Agatha and Cheng, Charibeth},\n      booktitle={Proceedings of The 12th Language Resources and Evaluation Conference},\n      pages={2596--2604},\n      year={2020}\n    }","description":"    Low-Resource Fake News Detection Corpora in Filipino. The first of its kind. Contains 3,206 expertly-labeled news samples, half of which are real and half of which are fake.","paperswithcode_id":"fake-news-filipino-dataset","key":""},{"id":"farsi_news","tags":["annotations_creators:found","language_creators:found","languages:fa","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"\\","description":"Contains Farsi (Persian) datasets for Machine Learning tasks, particularly NLP.\nThese datasets have been extracted from the RSS feed of two Farsi news agency websites:\n\n- Hamshahri\n- RadioFarda","key":""},{"id":"fashion_mnist","tags":["annotations_creators:expert-generated","licenses:mit","size_categories:10K<n<100K","source_datasets:original","task_categories:other","task_ids:other-other-image-classification"],"citation":"@article{DBLP:journals/corr/abs-1708-07747,\n  author    = {Han Xiao and\n               Kashif Rasul and\n               Roland Vollgraf},\n  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n               Algorithms},\n  journal   = {CoRR},\n  volume    = {abs/1708.07747},\n  year      = {2017},\n  url       = {http://arxiv.org/abs/1708.07747},\n  archivePrefix = {arXiv},\n  eprint    = {1708.07747},\n  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.","paperswithcode_id":"fashion-mnist","key":""},{"id":"fever","tags":["languages:en","annotations_creators:crowdsourced","language_creators:found","licenses:cc-by-sa-3.0","licenses:gpl-3.0","multilinguality:monolingual","pretty_name:FEVER","size_categories:100K<n<1M","source_datasets:extended|wikipedia","task_categories:text-classification","task_ids:text-classification-other-knowledge-verification"],"citation":"@inproceedings{Thorne18Fever,\n    author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},\n    title = {{FEVER}: a Large-scale Dataset for Fact Extraction and VERification},\n    booktitle = {NAACL-HLT},\n    year = {2018}\n}\n}","description":"With billions of individual pages on the web providing information on almost every conceivable topic, we should have the ability to collect facts that answer almost every conceivable question. However, only a small fraction of this information is contained in structured sources (Wikidata, Freebase, etc.) – we are therefore limited by our ability to transform free-form text to structured knowledge. There is, however, another problem that has become the focus of a lot of recent research and media coverage: false information coming from unreliable sources. [1] [2]\n\nThe FEVER workshops are a venue for work in verifiable knowledge extraction and to stimulate progress in this direction.","paperswithcode_id":"fever","key":""},{"id":"few_rel","tags":["annotations_creators:crowdsourced","annotations_creators:machine-generated","language_creators:found","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:n<1K","source_datasets:original","task_categories:other","task_ids:other-other-relation-extraction"],"citation":"@inproceedings{han-etal-2018-fewrel,\n    title = \"{F}ew{R}el: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation\",\n    author = \"Han, Xu and Zhu, Hao and Yu, Pengfei and Wang, Ziyun and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong\",\n    booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\",\n    month = oct # \"-\" # nov,\n    year = \"2018\",\n    address = \"Brussels, Belgium\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D18-1514\",\n    doi = \"10.18653/v1/D18-1514\",\n    pages = \"4803--4809\"\n}\n\n@inproceedings{gao-etal-2019-fewrel,\n    title = \"{F}ew{R}el 2.0: Towards More Challenging Few-Shot Relation Classification\",\n    author = \"Gao, Tianyu and Han, Xu and Zhu, Hao and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\",\n    month = nov,\n    year = \"2019\",\n    address = \"Hong Kong, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D19-1649\",\n    doi = \"10.18653/v1/D19-1649\",\n    pages = \"6251--6256\"\n}","description":"FewRel is a large-scale few-shot relation extraction dataset, which contains more than one hundred relations and tens of thousands of annotated instances cross different domains.","paperswithcode_id":"fewrel","key":""},{"id":"financial_phrasebank","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-nc-sa-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification","task_ids:sentiment-classification"],"citation":"@article{Malo2014GoodDO,\n  title={Good debt or bad debt: Detecting semantic orientations in economic texts},\n  author={P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},\n  journal={Journal of the Association for Information Science and Technology},\n  year={2014},\n  volume={65}\n}","description":"The key arguments for the low utilization of statistical techniques in\nfinancial sentiment analysis have been the difficulty of implementation for\npractical applications and the lack of high quality training data for building\nsuch models. Especially in the case of finance and economic texts, annotated\ncollections are a scarce resource and many are reserved for proprietary use\nonly. To resolve the missing training data problem, we present a collection of\n∼ 5000 sentences to establish human-annotated standards for benchmarking\nalternative modeling techniques.\n\nThe objective of the phrase level annotation task was to classify each example\nsentence into a positive, negative or neutral category by considering only the\ninformation explicitly available in the given sentence. Since the study is\nfocused only on financial and economic domains, the annotators were asked to\nconsider the sentences from the view point of an investor only; i.e. whether\nthe news may have positive, negative or neutral influence on the stock price.\nAs a result, sentences which have a sentiment that is not relevant from an\neconomic or financial perspective are considered neutral.\n\nThis release of the financial phrase bank covers a collection of 4840\nsentences. The selected collection of phrases was annotated by 16 people with\nadequate background knowledge on financial markets. Three of the annotators\nwere researchers and the remaining 13 annotators were master’s students at\nAalto University School of Business with majors primarily in finance,\naccounting, and economics.\n\nGiven the large number of overlapping annotations (5 to 8 annotations per\nsentence), there are several ways to define a majority vote based gold\nstandard. To provide an objective comparison, we have formed 4 alternative\nreference datasets based on the strength of majority agreement: all annotators\nagree, >=75% of annotators agree, >=66% of annotators agree and >=50% of\nannotators agree.","key":""},{"id":"finer","tags":["annotations_creators:expert-generated","language_creators:other","languages:fi","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@article{ruokolainen2019finnish,\n  title={A finnish news corpus for named entity recognition},\n  author={Ruokolainen, Teemu and Kauppinen, Pekka and Silfverberg, Miikka and Lind{\\'e}n, Krister},\n  journal={Language Resources and Evaluation},\n  pages={1--26},\n  year={2019},\n  publisher={Springer}\n}","description":"The directory data contains a corpus of Finnish technology related news articles with a manually prepared\nnamed entity annotation (digitoday.2014.csv). The text material was extracted from the archives of Digitoday,\na Finnish online technology news source (www.digitoday.fi). The corpus consists of 953 articles\n(193,742 word tokens) with six named entity classes (organization, location, person, product, event, and date).\nThe corpus is available for research purposes and can be readily used for development of NER systems for Finnish.","paperswithcode_id":"finer","key":""},{"id":"flores","tags":["annotations_creators:found","language_creators:found","languages:en","languages:ne","languages:si","licenses:cc-by-4.0","multilinguality:translation","size_categories:1K<n<10K","source_datasets:extended|wikipedia","source_datasets:extended|opus_gnome","source_datasets:extended|opus_ubuntu","source_datasets:extended|open_subtitles","source_datasets:extended|paracrawl","source_datasets:extended|bible_para","source_datasets:extended|kde4","source_datasets:extended|other-global-voices","source_datasets:extended|other-common-crawl","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@misc{guzmn2019new,\n    title={Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English},\n    author={Francisco Guzman and Peng-Jen Chen and Myle Ott and Juan Pino and Guillaume Lample and Philipp Koehn and Vishrav Chaudhary and Marc'Aurelio Ranzato},\n    year={2019},\n    eprint={1902.01382},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"Evaluation datasets for low-resource machine translation: Nepali-English and Sinhala-English.","paperswithcode_id":"flores","key":""},{"id":"flue","tags":["annotations_creators:crowdsourced","annotations_creators:machine-generated","language_creators:crowdsourced","languages:fr","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification","task_ids:semantic-similarity-classification","task_ids:sentiment-classification","task_ids:text-classification-other-Word Sense Disambiguation for Verbs"],"citation":"@misc{le2019flaubert,\n    title={FlauBERT: Unsupervised Language Model Pre-training for French},\n    author={Hang Le and Loïc Vial and Jibril Frej and Vincent Segonne and Maximin Coavoux and Benjamin Lecouteux and Alexandre Allauzen and Benoît Crabbé and Laurent Besacier and Didier Schwab},\n    year={2019},\n    eprint={1912.05372},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"FLUE is an evaluation setup for French NLP systems similar to the popular GLUE benchmark. The goal is to enable further reproducible experiments in the future and to share models and progress on the French language.","key":""},{"id":"fquad","tags":["annotations_creators:crowdsourced","extended:original","language_creators:crowdsourced","language_creators:found","languages:fr","licenses:cc-by-nc-sa-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_categories:text-retrieval","task_ids:extractive-qa","task_ids:closed-domain-qa"],"citation":"@ARTICLE{2020arXiv200206071\n       author = {Martin, d'Hoffschmidt and Maxime, Vidal and\n         Wacim, Belblidia and Tom, Brendlé},\n        title = \"{FQuAD: French Question Answering Dataset}\",\n      journal = {arXiv e-prints},\n     keywords = {Computer Science - Computation and Language},\n         year = \"2020\",\n        month = \"Feb\",\n          eid = {arXiv:2002.06071},\n        pages = {arXiv:2002.06071},\narchivePrefix = {arXiv},\n       eprint = {2002.06071},\n primaryClass = {cs.CL}\n}","description":"FQuAD: French Question Answering Dataset\nWe introduce FQuAD, a native French Question Answering Dataset. FQuAD contains 25,000+ question and answer pairs.\nFinetuning CamemBERT on FQuAD yields a F1 score of 88% and an exact match of 77.9%.","paperswithcode_id":"fquad","key":""},{"id":"freebase_qa","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|trivia_qa","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@article{jiang2019freebaseqa,\n  title={FreebaseQA: A New Factoid QA Dataset Matching Trivia-Style Question-Answer Pairs with Freebase},\n  author={Jiang, Kelvin and Wu, Dekun and Jiang, Hui},\n  journal={north american chapter of the association for computational linguistics},\n  year={2019}\n}","description":"FreebaseQA is for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase The data set is generated by matching trivia-type question-answer pairs with subject-predicateobject triples in Freebase.","paperswithcode_id":"freebaseqa","key":""},{"id":"gap","tags":["languages:en"],"citation":"@article{DBLP:journals/corr/abs-1810-05201,\n  author    = {Kellie Webster and\n               Marta Recasens and\n               Vera Axelrod and\n               Jason Baldridge},\n  title     = {Mind the {GAP:} {A} Balanced Corpus of Gendered Ambiguous Pronouns},\n  journal   = {CoRR},\n  volume    = {abs/1810.05201},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.05201},\n  archivePrefix = {arXiv},\n  eprint    = {1810.05201},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-05201},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"GAP is a gender-balanced dataset containing 8,908 coreference-labeled pairs of\n(ambiguous pronoun, antecedent name), sampled from Wikipedia and released by\nGoogle AI Language for the evaluation of coreference resolution in practical\napplications.","paperswithcode_id":"gap","key":""},{"id":"gem","tags":["annotations_creators:crowdsourced","annotations_creators:found","language_creators:found","language_creators:crowdsourced","language_creators:machine-generated","languages:en","languages:cs","languages:de","languages:es","languages:ru","languages:tr","languages:vi","licenses:other-research-only","multilinguality:monolingual","multilinguality:multilingual","size_categories:10K<n<100K","size_categories:1K<n<10K","size_categories:100K<n<1M","source_datasets:extended|other-vision-datasets","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_ids:other-stuctured-to-text","task_ids:summarization","task_ids:dialogue-modeling","task_ids:table-to-text","task_ids:text-simplification"],"citation":"@article{gem_benchmark,\n  author    = {Sebastian Gehrmann and\n               Tosin P. Adewumi and\n               Karmanya Aggarwal and\n               Pawan Sasanka Ammanamanchi and\n               Aremu Anuoluwapo and\n               Antoine Bosselut and\n               Khyathi Raghavi Chandu and\n               Miruna{-}Adriana Clinciu and\n               Dipanjan Das and\n               Kaustubh D. Dhole and\n               Wanyu Du and\n               Esin Durmus and\n               Ondrej Dusek and\n               Chris Emezue and\n               Varun Gangal and\n               Cristina Garbacea and\n               Tatsunori Hashimoto and\n               Yufang Hou and\n               Yacine Jernite and\n               Harsh Jhamtani and\n               Yangfeng Ji and\n               Shailza Jolly and\n               Dhruv Kumar and\n               Faisal Ladhak and\n               Aman Madaan and\n               Mounica Maddela and\n               Khyati Mahajan and\n               Saad Mahamood and\n               Bodhisattwa Prasad Majumder and\n               Pedro Henrique Martins and\n               Angelina McMillan{-}Major and\n               Simon Mille and\n               Emiel van Miltenburg and\n               Moin Nadeem and\n               Shashi Narayan and\n               Vitaly Nikolaev and\n               Rubungo Andre Niyongabo and\n               Salomey Osei and\n               Ankur P. Parikh and\n               Laura Perez{-}Beltrachini and\n               Niranjan Ramesh Rao and\n               Vikas Raunak and\n               Juan Diego Rodriguez and\n               Sashank Santhanam and\n               Joao Sedoc and\n               Thibault Sellam and\n               Samira Shaikh and\n               Anastasia Shimorina and\n               Marco Antonio Sobrevilla Cabezudo and\n               Hendrik Strobelt and\n               Nishant Subramani and\n               Wei Xu and\n               Diyi Yang and\n               Akhila Yerukola and\n               Jiawei Zhou},\n  title     = {The {GEM} Benchmark: Natural Language Generation, its Evaluation and\n               Metrics},\n  journal   = {CoRR},\n  volume    = {abs/2102.01672},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2102.01672},\n  archivePrefix = {arXiv},\n  eprint    = {2102.01672}\n}","description":"GEM is a benchmark environment for Natural Language Generation with a focus on its Evaluation,\nboth through human annotations and automated Metrics.\n\nGEM aims to:\n- measure NLG progress across 13 datasets spanning many NLG tasks and languages.\n- provide an in-depth analysis of data and models presented via data statements and challenge sets.\n- develop standards for evaluation of generated text using both automated and human metrics.\n\nIt is our goal to regularly update GEM and to encourage toward more inclusive practices in dataset development\nby extending existing data or developing datasets for additional languages.","paperswithcode_id":"gem","key":""},{"id":"generated_reviews_enth","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:machine-generated","languages:en","languages:th","licenses:cc-by-sa-4.0","multilinguality:translation","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-classification","task_ids:machine-translation","task_ids:multi-class-classification","task_ids:semantic-similarity-classification"],"citation":"@article{lowphansirikul2020scb,\n  title={scb-mt-en-th-2020: A Large English-Thai Parallel Corpus},\n  author={Lowphansirikul, Lalita and Polpanumas, Charin and Rutherford, Attapol T and Nutanong, Sarana},\n  journal={arXiv preprint arXiv:2007.03541},\n  year={2020}\n}","description":" `generated_reviews_enth`\n Generated product reviews dataset for machine translation quality prediction, part of [scb-mt-en-th-2020](https://arxiv.org/pdf/2007.03541.pdf)\n `generated_reviews_enth` is created as part of [scb-mt-en-th-2020](https://arxiv.org/pdf/2007.03541.pdf) for machine translation task.\n This dataset (referred to as `generated_reviews_yn` in [scb-mt-en-th-2020](https://arxiv.org/pdf/2007.03541.pdf)) are English product reviews\n generated by [CTRL](https://arxiv.org/abs/1909.05858), translated by Google Translate API and annotated as accepted or rejected (`correct`)\n based on fluency and adequacy of the translation by human annotators.\n This allows it to be used for English-to-Thai translation quality esitmation (binary label), machine translation, and sentiment analysis.","key":""},{"id":"generics_kb","tags":["annotations_creators:machine-generated","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1M<n<10M","size_categories:10K<n<100K","source_datasets:original","task_categories:other","task_ids:other-other-knowledge-base"],"citation":"@InProceedings{huggingface:dataset,\ntitle = {GenericsKB: A Knowledge Base of Generic Statements},\nauthors={Sumithra Bhakthavatsalam, Chloe Anastasiades, Peter Clark},\nyear={2020},\npublisher = {Allen Institute for AI},\n}","description":"The GenericsKB contains 3.4M+ generic sentences about the world, i.e., sentences expressing general truths such as \"Dogs bark,\" and \"Trees remove carbon dioxide from the atmosphere.\" Generics are potentially useful as a knowledge source for AI systems requiring general world knowledge. The GenericsKB is the first large-scale resource containing naturally occurring generic sentences (as opposed to extracted or crowdsourced triples), and is rich in high-quality, general, semantically complete statements. Generics were primarily extracted from three large text sources, namely the Waterloo Corpus, selected parts of Simple Wikipedia, and the ARC Corpus. A filtered, high-quality subset is also available in GenericsKB-Best, containing 1,020,868 sentences. We recommend you start with GenericsKB-Best.","paperswithcode_id":"genericskb","key":""},{"id":"german_legal_entity_recognition","tags":["annotations_creators:expert-generated","language_creators:found","languages:de","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{leitner2019fine,\n  author = {Elena Leitner and Georg Rehm and Julian Moreno-Schneider},\n  title = {{Fine-grained Named Entity Recognition in Legal Documents}},\n  booktitle = {Semantic Systems. The Power of AI and Knowledge\n                  Graphs. Proceedings of the 15th International Conference\n                  (SEMANTiCS 2019)},\n  year = 2019,\n  editor = {Maribel Acosta and Philippe Cudré-Mauroux and Maria\n                  Maleshkova and Tassilo Pellegrini and Harald Sack and York\n                  Sure-Vetter},\n  keywords = {aip},\n  publisher = {Springer},\n  series = {Lecture Notes in Computer Science},\n  number = {11702},\n  address = {Karlsruhe, Germany},\n  month = 9,\n  note = {10/11 September 2019},\n  pages = {272--287},\n  pdf = {https://link.springer.com/content/pdf/10.1007%2F978-3-030-33220-4_20.pdf}}","description":"\\","paperswithcode_id":"legal-documents-entity-recognition","key":""},{"id":"germaner","tags":["annotations_creators:crowdsourced","language_creators:found","languages:de","licenses:apache-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{Benikova2015GermaNERFO,\n  title={GermaNER: Free Open German Named Entity Recognition Tool},\n  author={Darina Benikova and S. Yimam and Prabhakaran Santhanam and Chris Biemann},\n  booktitle={GSCL},\n  year={2015}\n}","description":"GermaNER is a freely available statistical German Named Entity Tagger based on conditional random fields(CRF). The tagger is trained and evaluated on the NoSta-D Named Entity dataset, which was used in the GermEval 2014 for named entity recognition. The tagger comes close to the performance of the best (proprietary) system in the competition with 77% F-measure (this is the latest result; the one reported in the paper is 76%) test set performance on the four standard NER classes (PERson, LOCation, ORGanisation and OTHer).\n\nWe describe a range of features and their influence on German NER classification and provide a comparative evaluation and some analysis of the results. The software components, the training data and all data used for feature generation are distributed under permissive licenses, thus this tagger can be used in academic and commercial settings without restrictions or fees. The tagger is available as a command-line tool and as an Apache UIMA component.","key":""},{"id":"germeval_14","tags":[],"citation":"@inproceedings{benikova-etal-2014-nosta,\n    title = {NoSta-D Named Entity Annotation for German: Guidelines and Dataset},\n    author = {Benikova, Darina  and\n      Biemann, Chris  and\n      Reznicek, Marc},\n    booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},\n    month = {may},\n    year = {2014},\n    address = {Reykjavik, Iceland},\n    publisher = {European Language Resources Association (ELRA)},\n    url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/276_Paper.pdf},\n    pages = {2524--2531},\n}","description":"The GermEval 2014 NER Shared Task builds on a new dataset with German Named Entity annotation with the following properties:    - The data was sampled from German Wikipedia and News Corpora as a collection of citations.    - The dataset covers over 31,000 sentences corresponding to over 590,000 tokens.    - The NER annotation uses the NoSta-D guidelines, which extend the Tübingen Treebank guidelines,      using four main NER categories with sub-structure, and annotating embeddings among NEs      such as [ORG FC Kickers [LOC Darmstadt]].","key":""},{"id":"giga_fren","tags":["annotations_creators:found","language_creators:found","languages:en","languages:fr","licenses:unknown","multilinguality:multilingual","size_categories:10M<n<100M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"Giga-word corpus for French-English from WMT2010 collected by Chris Callison-Burch\n2 languages, total number of files: 452\ntotal number of tokens: 1.43G\ntotal number of sentence fragments: 47.55M","key":""},{"id":"gigaword","tags":["languages:en"],"citation":"@article{graff2003english,\n  title={English gigaword},\n  author={Graff, David and Kong, Junbo and Chen, Ke and Maeda, Kazuaki},\n  journal={Linguistic Data Consortium, Philadelphia},\n  volume={4},\n  number={1},\n  pages={34},\n  year={2003}\n}\n\n@article{Rush_2015,\n   title={A Neural Attention Model for Abstractive Sentence Summarization},\n   url={http://dx.doi.org/10.18653/v1/D15-1044},\n   DOI={10.18653/v1/d15-1044},\n   journal={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},\n   publisher={Association for Computational Linguistics},\n   author={Rush, Alexander M. and Chopra, Sumit and Weston, Jason},\n   year={2015}\n}","description":"Headline-generation on a corpus of article pairs from Gigaword consisting of\naround 4 million articles. Use the 'org_data' provided by\nhttps://github.com/microsoft/unilm/ which is identical to\nhttps://github.com/harvardnlp/sent-summary but with better format.\n\nThere are two features:\n  - document: article.\n  - summary: headline.","key":""},{"id":"glucose","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other-ROC-stories","task_categories:sequence-modeling","task_ids:sequence-modeling-other-common-sense-inference"],"citation":"@inproceedings{mostafazadeh2020glucose,\n      title={GLUCOSE: GeneraLized and COntextualized Story Explanations},\n      author={Nasrin Mostafazadeh and Aditya Kalyanpur and Lori Moon and David Buchanan and Lauren Berkowitz and Or Biran and Jennifer Chu-Carroll},\n      year={2020},\n      booktitle={The Conference on Empirical Methods in Natural Language Processing},\n      publisher={Association for Computational Linguistics}\n}","description":"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context.","paperswithcode_id":"glucose","key":""},{"id":"glue","tags":["annotations_creators:unknown","language_creators:unknown","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:unknown","task_categories:text-classification","task_categories:text-scoring","task_ids:natural-language-inference","task_ids:acceptability-classification","task_ids:text-classification-other-paraphrase-identification","task_ids:text-classification-other-qa-nli","task_ids:sentiment-classification","task_ids:semantic-similarity-scoring","task_ids:text-classification-other-coreference-nli"],"citation":"@inproceedings{wang2019glue,\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n  note={In the Proceedings of ICLR.},\n  year={2019}\n}","description":"GLUE, the General Language Understanding Evaluation benchmark\n(https://gluebenchmark.com/) is a collection of resources for training,\nevaluating, and analyzing natural language understanding systems.","paperswithcode_id":"glue","key":""},{"id":"gnad10","tags":["annotations_creators:crowdsourced","language_creators:found","languages:de","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other-from-One-Million-Posts-Corpus","task_categories:text-classification","task_ids:topic-classification"],"description":"This dataset is intended to advance topic classification for German texts. A classifier that is efffective in\nEnglish may not be effective in German dataset because it has a higher inflection and longer compound words.\nThe 10kGNAD dataset contains 10273 German news articles from an Austrian online newspaper categorized into\n9 categories. Article titles and text are concatenated together and authors are removed to avoid a keyword-like\nclassification on authors that write frequently about one category. This dataset can be used as a benchmark\nfor German topic classification.","key":""},{"id":"go_emotions","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification","task_ids:multi-label-classification","task_ids:text-classification-other-emotion"],"citation":"@inproceedings{demszky2020goemotions,\n author = {Demszky, Dorottya and Movshovitz-Attias, Dana and Ko, Jeongwoo and Cowen, Alan and Nemade, Gaurav and Ravi, Sujith},\n booktitle = {58th Annual Meeting of the Association for Computational Linguistics (ACL)},\n title = {{GoEmotions: A Dataset of Fine-Grained Emotions}},\n year = {2020}\n}","description":"The GoEmotions dataset contains 58k carefully curated Reddit comments labeled for 27 emotion categories or Neutral.\nThe emotion categories are admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire,\ndisappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness,\noptimism, pride, realization, relief, remorse, sadness, surprise.","paperswithcode_id":"goemotions","key":""},{"id":"gooaq","tags":["annotations_creators:expert-generated","language_creators:machine-generated","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@article{gooaq2021,\n  title={GooAQ: Open Question Answering with Diverse Answer Types},\n  author={Khashabi, Daniel and Ng, Amos and Khot, Tushar and Sabharwal, Ashish and Hajishirzi, Hannaneh and Callison-Burch, Chris},\n  journal={arXiv preprint},\n  year={2021}\n}","description":"GooAQ is a large-scale dataset with a variety of answer types. This dataset contains over\n5 million questions and 3 million answers collected from Google. GooAQ questions are collected\nsemi-automatically from the Google search engine using its autocomplete feature. This results in\nnaturalistic questions of practical interest that are nonetheless short and expressed using simple\nlanguage. GooAQ answers are mined from Google's responses to our collected questions, specifically from\nthe answer boxes in the search results. This yields a rich space of answer types, containing both\ntextual answers (short and long) as well as more structured ones such as collections.","paperswithcode_id":"gooaq","key":""},{"id":"google_wellformed_query","tags":["task_categories:text-scoring","multilinguality:monolingual","task_ids:other","languages:en","annotations_creators:crowdsourced","source_datasets:extended","size_categories:10K<n<100K","licenses:CC-BY-SA-4.0"],"citation":"@misc{faruqui2018identifying,\n      title={Identifying Well-formed Natural Language Questions},\n      author={Manaal Faruqui and Dipanjan Das},\n      year={2018},\n      eprint={1808.09419},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Google's query wellformedness dataset was created by crowdsourcing well-formedness annotations for 25,100 queries from the Paralex corpus. Every query was annotated by five raters each with 1/0 rating of whether or not the query is well-formed.","key":""},{"id":"grail_qa","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:question-answering-other-knowledge-base-qa"],"citation":"@misc{gu2020iid,\n    title={Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases},\n    author={Yu Gu and Sue Kase and Michelle Vanni and Brian Sadler and Percy Liang and Xifeng Yan and Yu Su},\n    year={2020},\n    eprint={2011.07743},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"Strongly Generalizable Question Answering (GrailQA) is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: i.i.d., compositional, and zero-shot.","key":""},{"id":"great_code","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_ids:table-to-text"],"citation":"@inproceedings{DBLP:conf/iclr/HellendoornSSMB20,\n  author    = {Vincent J. Hellendoorn and\n               Charles Sutton and\n               Rishabh Singh and\n               Petros Maniatis and\n               David Bieber},\n  title     = {Global Relational Models of Source Code},\n  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,\n               Addis Ababa, Ethiopia, April 26-30, 2020},\n  publisher = {OpenReview.net},\n  year      = {2020},\n  url       = {https://openreview.net/forum?id=B1lnbRNtwr},\n  timestamp = {Thu, 07 May 2020 17:11:47 +0200},\n  biburl    = {https://dblp.org/rec/conf/iclr/HellendoornSSMB20.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"The dataset for the variable-misuse task, described in the ICLR 2020 paper 'Global Relational Models of Source Code' [https://openreview.net/forum?id=B1lnbRNtwr]\n\nThis is the public version of the dataset used in that paper. The original, used to produce the graphs in the paper, could not be open-sourced due to licensing issues. See the public associated code repository [https://github.com/VHellendoorn/ICLR20-Great] for results produced from this dataset.\n\nThis dataset was generated synthetically from the corpus of Python code in the ETH Py150 Open dataset [https://github.com/google-research-datasets/eth_py150_open].","key":""},{"id":"guardian_authorship","tags":["languages:en"],"citation":"@article{article,\n    author = {Stamatatos, Efstathios},\n    year = {2013},\n    month = {01},\n    pages = {421-439},\n    title = {On the robustness of authorship attribution based on character n-gram features},\n    volume = {21},\n    journal = {Journal of Law and Policy}\n}\n\n@inproceedings{stamatatos2017authorship,\n    title={Authorship attribution using text distortion},\n    author={Stamatatos, Efstathios},\n    booktitle={Proc. of the 15th Conf. of the European Chapter of the Association for Computational Linguistics},\n    volume={1}\n    pages={1138--1149},\n    year={2017}\n}","description":"A dataset cross-topic authorship attribution. The dataset is provided by Stamatatos 2013.\n1- The cross-topic scenarios are based on Table-4 in Stamatatos 2017 (Ex. cross_topic_1 => row 1:P S U&W ).\n2- The cross-genre scenarios are based on Table-5 in the same paper. (Ex. cross_genre_1 => row 1:B P S&U&W).\n\n3- The same-topic/genre scenario is created by grouping all the datasts as follows.\nFor ex., to use same_topic and split the data 60-40 use:\ntrain_ds = load_dataset('guardian_authorship', name=\"cross_topic_<<#>>\",\n                        split='train[:60%]+validation[:60%]+test[:60%]')\ntests_ds = load_dataset('guardian_authorship', name=\"cross_topic_<<#>>\",\n                        split='train[-40%:]+validation[-40%:]+test[-40%:]')\n\nIMPORTANT: train+validation+test[:60%] will generate the wrong splits becasue the data is imbalanced\n\n* See https://huggingface.co/docs/datasets/splits.html for detailed/more examples","key":""},{"id":"gutenberg_time","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"citation":"@misc{kim2020time,\n      title={What time is it? Temporal Analysis of Novels},\n      author={Allen Kim and Charuta Pethe and Steven Skiena},\n      year={2020},\n      eprint={2011.04124},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"A clean data resource containing all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg.","paperswithcode_id":"gutenberg-time-dataset","key":""},{"id":"hans","tags":["languages:en"],"citation":"@article{DBLP:journals/corr/abs-1902-01007,\n  author    = {R. Thomas McCoy and\n               Ellie Pavlick and\n               Tal Linzen},\n  title     = {Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural\n               Language Inference},\n  journal   = {CoRR},\n  volume    = {abs/1902.01007},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1902.01007},\n  archivePrefix = {arXiv},\n  eprint    = {1902.01007},\n  timestamp = {Tue, 21 May 2019 18:03:36 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-01007.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"The HANS dataset is an NLI evaluation set that tests specific hypotheses about invalid heuristics that NLI models are likely to learn.","paperswithcode_id":"hans","key":""},{"id":"hansards","tags":[],"citation":"","description":"This release contains 1.3 million pairs of aligned text chunks (sentences or smaller fragments)\nfrom the official records (Hansards) of the 36th Canadian Parliament.\n\nThe complete Hansards of the debates in the House and Senate of the 36th Canadian Parliament,\nas far as available, were aligned. The corpus was then split into 5 sets of sentence pairs:\ntraining (80% of the sentence pairs), two sets of sentence pairs for testing (5% each), and\ntwo sets of sentence pairs for final evaluation (5% each). The current release consists of the\ntraining and testing sets. The evaluation sets are reserved for future MT evaluation purposes\nand currently not available.\n\nCaveats\n1. This release contains only sentence pairs. Even though the order of the sentences is the same\nas in the original, there may be gaps resulting from many-to-one, many-to-many, or one-to-many\nalignments that were filtered out. Therefore, this release may not be suitable for\ndiscourse-related research.\n2. Neither the sentence splitting nor the alignments are perfect. In particular, watch out for\npairs that differ considerably in length. You may want to filter these out before you do\nany statistical training.\n\nThe alignment of the Hansards was performed as part of the ReWrite project under funding\nfrom the DARPA TIDES program.","key":""},{"id":"hard","tags":["annotations_creators:found","language_creators:found","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"citation":"@incollection{elnagar2018hotel,\n  title={Hotel Arabic-reviews dataset construction for sentiment analysis applications},\n  author={Elnagar, Ashraf and Khalifa, Yasmin S and Einea, Anas},\n  booktitle={Intelligent Natural Language Processing: Trends and Applications},\n  pages={35--52},\n  year={2018},\n  publisher={Springer}\n}","description":"This dataset contains 93700 hotel reviews in Arabic language.The hotel reviews were collected from Booking.com website during June/July 2016.The reviews are expressed in Modern Standard Arabic as well as dialectal Arabic.The following table summarize some tatistics on the HARD Dataset.","paperswithcode_id":"hard","key":""},{"id":"harem","tags":["annotations_creators:expert-generated","language_creators:found","languages:pt","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{santos2006harem,\n  title={Harem: An advanced ner evaluation contest for portuguese},\n  author={Santos, Diana and Seco, Nuno and Cardoso, Nuno and Vilela, Rui},\n  booktitle={quot; In Nicoletta Calzolari; Khalid Choukri; Aldo Gangemi; Bente Maegaard; Joseph Mariani; Jan Odjik; Daniel Tapias (ed) Proceedings of the 5 th International Conference on Language Resources and Evaluation (LREC'2006)(Genoa Italy 22-28 May 2006)},\n  year={2006}\n}","description":"The HAREM is a Portuguese language corpus commonly used for Named Entity Recognition tasks. It includes about 93k words, from 129 different texts,\nfrom several genres, and language varieties. The split of this dataset version follows the division made by [1], where 7% HAREM\ndocuments are the validation set and the miniHAREM corpus (with about 65k words) is the test set. There are two versions of the dataset set,\na version that has a total of 10 different named entity classes (Person, Organization, Location, Value, Date, Title, Thing, Event,\nAbstraction, and Other) and a \"selective\" version with only 5 classes (Person, Organization, Location, Value, and Date).\n\nIt's important to note that the original version of the HAREM dataset has 2 levels of NER details, namely \"Category\" and \"Sub-type\".\nThe dataset version processed here ONLY USE the \"Category\" level of the original dataset.\n\n[1] Souza, Fábio, Rodrigo Nogueira, and Roberto Lotufo. \"BERTimbau: Pretrained BERT Models for Brazilian Portuguese.\" Brazilian Conference on Intelligent Systems. Springer, Cham, 2020.","key":""},{"id":"has_part","tags":["annotations_creators:machine-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other-Generics-KB","task_categories:text-scoring","task_ids:text-scoring-other-Meronym-Prediction"],"citation":"@misc{bhakthavatsalam2020dogs,\n      title={Do Dogs have Whiskers? A New Knowledge Base of hasPart Relations},\n      author={Sumithra Bhakthavatsalam and Kyle Richardson and Niket Tandon and Peter Clark},\n      year={2020},\n      eprint={2006.07510},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"This dataset is a new knowledge-base (KB) of hasPart relationships, extracted from a large corpus of generic statements. Complementary to other resources available, it is the first which is all three of: accurate (90% precision), salient (covers relationships a person may mention), and has high coverage of common terms (approximated as within a 10 year old’s vocabulary), as well as having several times more hasPart entries than in the popular ontologies ConceptNet and WordNet. In addition, it contains information about quantifiers, argument modifiers, and links the entities to appropriate concepts in Wikipedia and WordNet.","paperswithcode_id":"haspart-kb","key":""},{"id":"hate_offensive","tags":["annotations_creators:crowdsourced","language_creators:machine-generated","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification","task_ids:text-classification-other-hate-speech-detection"],"citation":"@article{article,\nauthor = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},\nyear = {2017},\nmonth = {03},\npages = {},\ntitle = {Automated Hate Speech Detection and the Problem of Offensive Language}\n}","paperswithcode_id":"hate-speech-and-offensive-language","key":""},{"id":"hate_speech18","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification"],"citation":"@inproceedings{gibert2018hate,\n    title = \"{Hate Speech Dataset from a White Supremacy Forum}\",\n    author = \"de Gibert, Ona  and\n      Perez, Naiara  and\n      Garcia-Pablos, Aitor  and\n      Cuadros, Montse\",\n    booktitle = \"Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2)\",\n    month = oct,\n    year = \"2018\",\n    address = \"Brussels, Belgium\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W18-5102\",\n    doi = \"10.18653/v1/W18-5102\",\n    pages = \"11--20\",\n}","description":"These files contain text extracted from Stormfront, a white supremacist forum. A random set of\nforums posts have been sampled from several subforums and split into sentences. Those sentences\nhave been manually labelled as containing hate speech or not, according to certain annotation guidelines.","paperswithcode_id":"hate-speech","key":""},{"id":"hate_speech_filipino","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:tl","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other-twitter-data-philippine-election","task_categories:text-classification","task_ids:sentiment-analysis"],"citation":"@article{Cabasag-2019-hate-speech,\n  title={Hate speech in Philippine election-related tweets: Automatic detection and classification using natural language processing.},\n  author={Neil Vicente Cabasag, Vicente Raphael Chan, Sean Christian Lim, Mark Edward Gonzales, and Charibeth Cheng},\n  journal={Philippine Computing Journal},\n  volume={XIV},\n  number={1},\n  month={August},\n  year={2019}\n}","description":"    Contains 10k tweets (training set) that are labeled as hate speech or non-hate speech. Released with 4,232 validation and 4,232 testing samples. Collected during the 2016 Philippine Presidential Elections.","key":""},{"id":"hate_speech_offensive","tags":["annotations_creators:expert-generated","annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-hate-speech-detection"],"citation":"@inproceedings{hateoffensive,\ntitle = {Automated Hate Speech Detection and the Problem of Offensive Language},\nauthor = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},\nbooktitle = {Proceedings of the 11th International AAAI Conference on Web and Social Media},\nseries = {ICWSM '17},\nyear = {2017},\nlocation = {Montreal, Canada},\npages = {512-515}\n}","description":"An annotated dataset for hate speech and offensive language detection on tweets.","paperswithcode_id":"hate-speech-and-offensive-language","key":""},{"id":"hate_speech_pl","tags":["annotations_creators:expert-generated","language_creators:found","languages:pl","licenses:cc-by-nc-sa-1.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_categories:text-scoring","task_ids:multi-class-classification","task_ids:multi-label-classification","task_ids:sentiment-classification","task_ids:sentiment-scoring","task_ids:topic-classification"],"description":"HateSpeech corpus in the current version contains over 2000 posts crawled from public Polish web. They represent various types and degrees of offensive language, expressed toward minorities (eg. ethnical, racial). The data were annotated manually.","key":""},{"id":"hate_speech_portuguese","tags":["annotations_creators:expert-generated","language_creators:found","languages:pt","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-hate-speech-detection"],"citation":"@inproceedings{fortuna-etal-2019-hierarchically,\ntitle = \"A Hierarchically-Labeled {P}ortuguese Hate Speech Dataset\",\nauthor = \"Fortuna, Paula  and\n    Rocha da Silva, Jo{\\\\~a}o  and\n    Soler-Company, Juan  and\n    Wanner, Leo  and\n    Nunes, S{\\'e}rgio\",\nbooktitle = \"Proceedings of the Third Workshop on Abusive Language Online\",\nmonth = aug,\nyear = \"2019\",\naddress = \"Florence, Italy\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/W19-3510\",\ndoi = \"10.18653/v1/W19-3510\",\npages = \"94--104\",\nabstract = \"Over the past years, the amount of online offensive speech has been growing steadily. To successfully cope with it, machine learning are applied. However, ML-based techniques require sufficiently large annotated datasets. In the last years, different datasets were published, mainly for English. In this paper, we present a new dataset for Portuguese, which has not been in focus so far. The dataset is composed of 5,668 tweets. For its annotation, we defined two different schemes used by annotators with different levels of expertise. Firstly, non-experts annotated the tweets with binary labels ({`}hate{'} vs. {`}no-hate{'}). Secondly, expert annotators classified the tweets following a fine-grained hierarchical multiple label scheme with 81 hate speech categories in total. The inter-annotator agreement varied from category to category, which reflects the insight that some types of hate speech are more subtle than others and that their detection depends on personal perception. This hierarchical annotation scheme is the main contribution of the presented work, as it facilitates the identification of different types of hate speech and their intersections. To demonstrate the usefulness of our dataset, we carried a baseline classification experiment with pre-trained word embeddings and LSTM on the binary classified data, with a state-of-the-art outcome.\",\n}","description":"Portuguese dataset for hate speech detection composed of 5,668 tweets with binary annotations (i.e. 'hate' vs. 'no-hate').","key":""},{"id":"hatexplain","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-hate-speech-detection"],"citation":"@misc{mathew2020hatexplain,\n      title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},\n      author={Binny Mathew and Punyajoy Saha and Seid Muhie Yimam and Chris Biemann and Pawan Goyal and Animesh Mukherjee},\n      year={2020},\n      eprint={2012.10289},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Hatexplain is the first benchmark hate speech dataset covering multiple aspects of the issue. Each post in the dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based.","paperswithcode_id":"hatexplain","key":""},{"id":"hausa_voa_ner","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:ha","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{hedderich-etal-2020-transfer,\n    title = \"Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on {A}frican Languages\",\n    author = \"Hedderich, Michael A.  and\n      Adelani, David  and\n      Zhu, Dawei  and\n      Alabi, Jesujoba  and\n      Markus, Udia  and\n      Klakow, Dietrich\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.204\",\n    doi = \"10.18653/v1/2020.emnlp-main.204\",\n    pages = \"2580--2591\",\n}","description":"The Hausa VOA NER dataset is a labeled dataset for named entity recognition in Hausa. The texts were obtained from\nHausa Voice of America News articles https://www.voahausa.com/ . We concentrate on\nfour types of named entities: persons [PER], locations [LOC], organizations [ORG], and dates & time [DATE].\n\nThe Hausa VOA NER data files contain 2 columns separated by a tab ('\\t'). Each word has been put on a separate line and\nthere is an empty line after each sentences i.e the CoNLL format. The first item on each line is a word, the second\nis the named entity tag. The named entity tags have the format I-TYPE which means that the word is inside a phrase\nof type TYPE. For every multi-word expression like 'New York', the first word gets a tag B-TYPE and the subsequent words\nhave tags I-TYPE, a word with tag O is not part of a phrase. The dataset is in the BIO tagging scheme.\n\nFor more details, see https://www.aclweb.org/anthology/2020.emnlp-main.204/","key":""},{"id":"hausa_voa_topics","tags":["annotations_creators:expert-generated","language_creators:found","languages:ha","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:topic-classification"],"citation":"@inproceedings{hedderich-etal-2020-transfer,\n    title = \"Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages\",\n    author = \"Hedderich, Michael A.  and\n      Adelani, David  and\n      Zhu, Dawei  and\n      Alabi, Jesujoba  and\n      Markus, Udia  and\n      Klakow, Dietrich\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.204\",\n    doi = \"10.18653/v1/2020.emnlp-main.204\",\n}","description":"A collection of news article headlines in Hausa from VOA Hausa.\nEach headline is labeled with one of the following classes: Nigeria,\nAfrica, World, Health or Politics.\n\nThe dataset was presented in the paper:\nHedderich, Adelani, Zhu, Alabi, Markus, Klakow: Transfer Learning and\nDistant Supervision for Multilingual Transformer Models: A Study on\nAfrican Languages (EMNLP 2020).","key":""},{"id":"hda_nli_hindi","tags":["annotations_creators:machine-generated","language_creators:found","languages:hi","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|hindi_discourse","task_categories:text-classification","task_ids:natural-language-inference"],"citation":"    @inproceedings{uppal-etal-2020-two,\n    title = \"Two-Step Classification using Recasted Data for Low Resource Settings\",\n    author = \"Uppal, Shagun  and\n      Gupta, Vivek  and\n      Swaminathan, Avinash  and\n      Zhang, Haimin  and\n      Mahata, Debanjan  and\n      Gosangi, Rakesh  and\n      Shah, Rajiv Ratn  and\n      Stent, Amanda\",\n    booktitle = \"Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing\",\n    month = dec,\n    year = \"2020\",\n    address = \"Suzhou, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.aacl-main.71\",\n    pages = \"706--719\",\n    abstract = \"An NLP model{'}s ability to reason should be independent of language. Previous works utilize Natural Language Inference (NLI) to understand the reasoning ability of models, mostly focusing on high resource languages like English. To address scarcity of data in low-resource languages such as Hindi, we use data recasting to create NLI datasets for four existing text classification datasets. Through experiments, we show that our recasted dataset is devoid of statistical irregularities and spurious patterns. We further study the consistency in predictions of the textual entailment models and propose a consistency regulariser to remove pairwise-inconsistencies in predictions. We propose a novel two-step classification method which uses textual-entailment predictions for classification task. We further improve the performance by using a joint-objective for classification and textual entailment. We therefore highlight the benefits of data recasting and improvements on classification performance using our approach with supporting experimental results.\",\n}","description":"This dataset is a recasted version of the Hindi Discourse Analysis Dataset used to train models for Natural Language Inference Tasks in Low-Resource Languages like Hindi.","key":""},{"id":"head_qa","tags":["annotations_creators:no-annotation","language_creators:expert-generated","languages:en","languages:es","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@inproceedings{vilares-gomez-rodriguez-2019-head,\n    title = \"{HEAD}-{QA}: A Healthcare Dataset for Complex Reasoning\",\n    author = \"Vilares, David  and\n      G{\\'o}mez-Rodr{\\'i}guez, Carlos\",\n    booktitle = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P19-1092\",\n    doi = \"10.18653/v1/P19-1092\",\n    pages = \"960--966\",\n    abstract = \"We present HEAD-QA, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work.\",\n}","description":"HEAD-QA is a multi-choice HEAlthcare Dataset. The questions come from exams to access a specialized position in the\nSpanish healthcare system, and are challenging even for highly specialized humans. They are designed by the Ministerio\nde Sanidad, Consumo y Bienestar Social.\n\nThe dataset contains questions about the following topics: medicine, nursing, psychology, chemistry, pharmacology and biology.","paperswithcode_id":"headqa","key":""},{"id":"health_fact","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:fact-checking","task_ids:multi-class-classification"],"citation":"@inproceedings{kotonya-toni-2020-explainable,\n    title = \"Explainable Automated Fact-Checking for Public Health Claims\",\n    author = \"Kotonya, Neema and Toni, Francesca\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods\n    in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.623\",\n    pages = \"7740--7754\",\n}","description":"PUBHEALTH is a comprehensive dataset for explainable automated fact-checking of\npublic health claims. Each instance in the PUBHEALTH dataset has an associated\nveracity label (true, false, unproven, mixture). Furthermore each instance in the\ndataset has an explanation text field. The explanation is a justification for which\nthe claim has been assigned a particular veracity label.\n\nThe dataset was created to explore fact-checking of difficult to verify claims i.e.,\nthose which require expertise from outside of the journalistics domain, in this case\nbiomedical and public health expertise.\n\nIt was also created in response to the lack of fact-checking datasets which provide\ngold standard natural language explanations for verdicts/labels.\n\nNOTE: There are missing labels in the dataset and we have replaced them with -1.","paperswithcode_id":"pubhealth","key":""},{"id":"hebrew_projectbenyehuda","tags":["annotations_creators:expert-generated","language_creators:found","languages:he","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@article{,\n  author = {},\n  title = {Public domain texts from Project Ben-Yehuda},\n  journal = {},\n  url = {https://github.com/projectbenyehuda/public_domain_dump},\n  year = {2020},\n}","description":"This repository contains a dump of thousands of public domain works in Hebrew, from Project Ben-Yehuda, in plaintext UTF-8 files, with and without diacritics (nikkud). The metadata (pseudocatalogue.csv) file is a list of titles, authors, genres, and file paths, to help you process the dump.\nAll these works are in the public domain, so you are free to make any use of them, and do not need to ask for permission.\nThere are 10078 files, 3181136 lines","key":""},{"id":"hebrew_sentiment","tags":["annotations_creators:expert-generated","language_creators:found","languages:he","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{amram-etal-2018-representations,\n    title = \"Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from {M}odern {H}ebrew\",\n    author = \"Amram, Adam  and\n      Ben David, Anat  and\n      Tsarfaty, Reut\",\n    booktitle = \"Proceedings of the 27th International Conference on Computational Linguistics\",\n    month = aug,\n    year = \"2018\",\n    address = \"Santa Fe, New Mexico, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/C18-1190\",\n    pages = \"2242--2252\",\n    abstract = \"This paper empirically studies the effects of representation choices on neural sentiment analysis for Modern Hebrew, a morphologically rich language (MRL) for which no sentiment analyzer currently exists. We study two dimensions of representational choices: (i) the granularity of the input signal (token-based vs. morpheme-based), and (ii) the level of encoding of vocabulary items (string-based vs. character-based). We hypothesise that for MRLs, languages where multiple meaning-bearing elements may be carried by a single space-delimited token, these choices will have measurable effects on task perfromance, and that these effects may vary for different architectural designs {---} fully-connected, convolutional or recurrent. Specifically, we hypothesize that morpheme-based representations will have advantages in terms of their generalization capacity and task accuracy, due to their better OOV coverage. To empirically study these effects, we develop a new sentiment analysis benchmark for Hebrew, based on 12K social media comments, and provide two instances of these data: in token-based and morpheme-based settings. Our experiments show that representation choices empirical effects vary with architecture type. While fully-connected and convolutional networks slightly prefer token-based settings, RNNs benefit from a morpheme-based representation, in accord with the hypothesis that explicit morphological information may help generalize. Our endeavour also delivers the first state-of-the-art broad-coverage sentiment analyzer for Hebrew, with over 89% accuracy, alongside an established benchmark to further study the effects of linguistic representation choices on neural networks{'} task performance.\",\n}","description":"HebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel’s\npresident, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder,\n2013) to scrape all the comments to all of the president’s posts in the period of June – August 2014,\nthe first three months of Rivlin’s presidency.2 While the president’s posts aimed at reconciling tensions\nand called for tolerance and empathy, the sentiment expressed in the comments to the president’s posts\nwas polarized between citizens who warmly thanked the president, and citizens that fiercely critiqued his\npolicy. Of the 12,804 comments, 370 are neutral; 8,512 are positive, 3,922 negative.\n\nData Annotation: A trained researcher examined each comment and determined its sentiment value,\nwhere comments with an overall positive sentiment were assigned the value 1, comments with an overall\nnegative sentiment were assigned the value -1, and comments that are off-topic to the post’s content\nwere assigned the value 0. We validated the coding scheme by asking a second trained researcher to\ncode the same data. There was substantial agreement between raters (N of agreements: 10623, N of\ndisagreements: 2105, Coehn’s Kappa = 0.697, p = 0).","paperswithcode_id":"modern-hebrew-sentiment-dataset","key":""},{"id":"hebrew_this_world","tags":["annotations_creators:expert-generated","language_creators:found","languages:he","licenses:gpl","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"description":"HebrewThisWorld is a data set consists of 2028 issues of the newspaper 'This World' edited by Uri Avnery and were published between 1950 and 1989. Released under the AGPLv3 license.","key":""},{"id":"hellaswag","tags":["languages:en"],"citation":"@inproceedings{zellers2019hellaswag,\n    title={HellaSwag: Can a Machine Really Finish Your Sentence?},\n    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},\n    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},\n    year={2019}\n}","description":"","paperswithcode_id":"hellaswag","key":""},{"id":"hendrycks_test","tags":["annotations_creators:no-annotation","language_creators:expert-generated","languages:en-US","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@article{hendryckstest2021,\n      title={Measuring Massive Multitask Language Understanding},\n      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},\n      journal={Proceedings of the International Conference on Learning Representations (ICLR)},\n      year={2021}\n    }","description":"This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge, covering 57 tasks including elementary mathematics, US history, computer science, law, and more.","key":""},{"id":"hind_encorp","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","language_creators:machine-generated","languages:en","languages:hi","licenses:cc-by-nc-sa-3.0","multilinguality:translation","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{hindencorp05:lrec:2014,\n  author = {Ond{\\v{r}}ej Bojar and Vojt{\\v{e}}ch Diatka\n            and Pavel Rychl{\\'{y}} and Pavel Stra{\\v{n}}{\\'{a}}k\n            and V{\\'{}}t Suchomel and Ale{\\v{s}} Tamchyna and Daniel Zeman},\n  title = \"{HindEnCorp - Hindi-English and Hindi-only Corpus for Machine\n            Translation}\",\n  booktitle = {Proceedings of the Ninth International Conference on Language\n               Resources and Evaluation (LREC'14)},\n  year = {2014},\n  month = {may},\n  date = {26-31},\n  address = {Reykjavik, Iceland},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and\n     Thierry Declerck and Hrafn Loftsson and Bente Maegaard and Joseph Mariani\n     and Asuncion Moreno and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-8-4},\n  language = {english}\n}","description":"HindEnCorp parallel texts (sentence-aligned) come from the following sources:\nTides, which contains 50K sentence pairs taken mainly from news articles. This dataset was originally col- lected for the DARPA-TIDES surprise-language con- test in 2002, later refined at IIIT Hyderabad and provided for the NLP Tools Contest at ICON 2008 (Venkatapathy, 2008).\n\nCommentaries by Daniel Pipes contain 322 articles in English written by a journalist Daniel Pipes and translated into Hindi.\n\nEMILLE. This corpus (Baker et al., 2002) consists of three components: monolingual, parallel and annotated corpora. There are fourteen monolingual sub- corpora, including both written and (for some lan- guages) spoken data for fourteen South Asian lan- guages. The EMILLE monolingual corpora contain in total 92,799,000 words (including 2,627,000 words of transcribed spoken data for Bengali, Gujarati, Hindi, Punjabi and Urdu). The parallel corpus consists of 200,000 words of text in English and its accompanying translations into Hindi and other languages.\n\nSmaller datasets as collected by Bojar et al. (2010) include the corpus used at ACL 2005 (a subcorpus of EMILLE), a corpus of named entities from Wikipedia (crawled in 2009), and Agriculture domain parallel corpus.\n￼\nFor the current release, we are extending the parallel corpus using these sources:\nIntercorp (Čermák and Rosen,2012) is a large multilingual parallel corpus of 32 languages including Hindi. The central language used for alignment is Czech. Intercorp’s core texts amount to 202 million words. These core texts are most suitable for us because their sentence alignment is manually checked and therefore very reliable. They cover predominately short sto- ries and novels. There are seven Hindi texts in Inter- corp. Unfortunately, only for three of them the English translation is available; the other four are aligned only with Czech texts. The Hindi subcorpus of Intercorp contains 118,000 words in Hindi.\n\nTED talks 3 held in various languages, primarily English, are equipped with transcripts and these are translated into 102 languages. There are 179 talks for which Hindi translation is available.\n\nThe Indic multi-parallel corpus (Birch et al., 2011; Post et al., 2012) is a corpus of texts from Wikipedia translated from the respective Indian language into English by non-expert translators hired over Mechanical Turk. The quality is thus somewhat mixed in many respects starting from typesetting and punctuation over capi- talization, spelling, word choice to sentence structure. A little bit of control could be in principle obtained from the fact that every input sentence was translated 4 times. We used the 2012 release of the corpus.\n\nLaunchpad.net is a software collaboration platform that hosts many open-source projects and facilitates also collaborative localization of the tools. We downloaded all revisions of all the hosted projects and extracted the localization (.po) files.\n\nOther smaller datasets. This time, we added Wikipedia entities as crawled in 2013 (including any morphological variants of the named entitity that appears on the Hindi variant of the Wikipedia page) and words, word examples and quotes from the Shabdkosh online dictionary.","paperswithcode_id":"hindencorp","key":""},{"id":"hindi_discourse","tags":["annotations_creators:other","language_creators:found","languages:hi","licenses:other-MIDAS-LAB-IIITD-Delhi","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:sequence-modeling-other-discourse-analysis"],"citation":"@inproceedings{swapnil2020,\n    title={An Annotated Dataset of Discourse Modes in Hindi Stories},\n    author={Swapnil Dhanwal, Hritwik Dutta, Hitesh Nankani, Nilay Shrivastava, Yaman Kumar, Junyi Jessy Li, Debanjan Mahata, Rakesh Gosangi, Haimin Zhang, Rajiv Ratn Shah, Amanda Stent},\n    booktitle={Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)},\n    volume={12},\n    pages={1191–1196},\n    year={2020}","description":"The Hindi Discourse Analysis dataset is a corpus for analyzing discourse modes present in its sentences.\nIt contains sentences from stories written by 11 famous authors from the 20th Century.\n4-5 stories by each author have been selected which were available in the public domain resulting\nin a collection of 53 stories. Most of these short stories were originally written in Hindi\nbut some of them were written in other Indian languages and later translated to Hindi.","key":""},{"id":"hippocorpus","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:other-my-license","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-scoring","task_ids:text-scoring-other-narrative-flow"],"citation":"@inproceedings{sap-etal-2020-recollection,\n    title = \"Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models\",\n    author = \"Sap, Maarten  and\n      Horvitz, Eric  and\n      Choi, Yejin  and\n      Smith, Noah A.  and\n      Pennebaker, James\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.178\",\n    doi = \"10.18653/v1/2020.acl-main.178\",\n    pages = \"1970--1978\",\n    abstract = \"We investigate the use of NLP as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release Hippocorpus, a dataset of 7,000 stories about imagined and recalled events. We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events. Our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected. In addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory. Finally, our measures reveal the effect of narrativization of memories in stories (e.g., stories about frequently recalled memories flow more linearly; Bartlett, 1932). Our findings highlight the potential of using NLP tools to study the traces of human cognition in language.\",\n}","description":"To examine the cognitive processes of remembering and imagining and their traces in language, we introduce Hippocorpus, a dataset of 6,854 English diary-like short stories about recalled and imagined events. Using a crowdsourcing framework, we first collect recalled stories and summaries from workers, then provide these summaries to other workers who write imagined stories. Finally, months later, we collect a retold version of the recalled stories from a subset of recalled authors. Our dataset comes paired with author demographics (age, gender, race), their openness to experience, as well as some variables regarding the author's relationship to the event (e.g., how personal the event is, how often they tell its story, etc.).","key":""},{"id":"hkcancor","tags":["annotations_creators:expert-generated","language_creators:found","languages:yue","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_ids:dialogue-modeling","task_ids:machine-translation"],"citation":"@article{luke2015hong,\n  author={Luke, Kang-Kwong and Wong, May LY},\n  title={The Hong Kong Cantonese corpus: design and uses},\n  journal={Journal of Chinese Linguistics},\n  year={2015},\n  pages={309-330},\n  month={12}\n}\n@misc{lee2020,\n  author = {Lee, Jackson},\n  title = {PyCantonese: Cantonese Linguistics and NLP in Python},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {https://github.com/jacksonllee/pycantonese},\n  commit = {1d58f44e1cb097faa69de6b617e1d28903b84b98}\n}","description":"The Hong Kong Cantonese Corpus (HKCanCor) comprise transcribed conversations\nrecorded between March 1997 and August 1998. It contains recordings of\nspontaneous speech (51 texts) and radio programmes (42 texts),\nwhich involve 2 to 4 speakers, with 1 text of monologue.\n\nIn total, the corpus contains around 230,000 Chinese words.\nThe text is word-segmented, annotated with part-of-speech (POS) tags and\nromanised Cantonese pronunciation.\n\nRomanisation scheme - Linguistic Society of Hong Kong (LSHK)\nPOS scheme - Peita-Fujitsu-Renmin Ribao (PRF) corpus (Duan et al., 2000),\n             with extended tags for Cantonese-specific phenomena added by\n             Luke and Wang (see original paper for details).","paperswithcode_id":"hong-kong-cantonese-corpus","key":""},{"id":"hlgd","tags":["annotations_creators:crowdsourced","language_creators:expert-generated","languages:en","licenses:apache-2.0","multilinguality:monolingual","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-headline-grouping","size_categories:10K<n<100K"],"citation":"@inproceedings{Laban2021NewsHG,\n  title={News Headline Grouping as a Challenging NLU Task},\n  author={Philippe Laban and Lucas Bandarkar},\n  booktitle={NAACL 2021},\n  publisher = {Association for Computational Linguistics},\n  year={2021}\n}","description":"HLGD is a binary classification dataset consisting of 20,056 labeled news headlines pairs indicating\nwhether the two headlines describe the same underlying world event or not.","key":""},{"id":"hope_edi","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","languages:ml","languages:ta","licenses:cc-by-4.0","multilinguality:monolingual","multilinguality:multilingual","size_categories:10K<n<100K","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-hope-speech-classification"],"citation":"@inproceedings{chakravarthi-2020-hopeedi,\ntitle = \"{H}ope{EDI}: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion\",\nauthor = \"Chakravarthi, Bharathi Raja\",\nbooktitle = \"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media\",\nmonth = dec,\nyear = \"2020\",\naddress = \"Barcelona, Spain (Online)\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/2020.peoples-1.5\",\npages = \"41--53\",\nabstract = \"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff{'}s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.\",\n}","description":"A Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not.","paperswithcode_id":"hopeedi","key":""},{"id":"hotpot_qa","tags":["languages:en"],"citation":"@inproceedings{yang2018hotpotqa,\n  title={{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},\n  booktitle={Conference on Empirical Methods in Natural Language Processing ({EMNLP})},\n  year={2018}\n}","description":"HotpotQA is a new dataset with 113k Wikipedia-based question-answer pairs with four key features:\n(1) the questions require finding and reasoning over multiple supporting documents to answer;\n(2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas;\n(3) we provide sentence-level supporting facts required for reasoning, allowingQA systems to reason with strong supervisionand explain the predictions;\n(4) we offer a new type of factoid comparison questions to testQA systems’ ability to extract relevant facts and perform necessary comparison.","paperswithcode_id":"hotpotqa","key":""},{"id":"hover","tags":["annotations_creators:expert-generated","language_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-retrieval","task_ids:fact-checking-retrieval"],"citation":"@inproceedings{jiang2020hover,\n  title={{HoVer}: A Dataset for Many-Hop Fact Extraction And Claim Verification},\n  author={Yichen Jiang and Shikha Bordia and Zheng Zhong and Charles Dognin and Maneesh Singh and Mohit Bansal.},\n  booktitle={Findings of the Conference on Empirical Methods in Natural Language Processing ({EMNLP})},\n  year={2020}\n}","description":"HoVer is an open-domain, many-hop fact extraction and claim verification dataset built upon the Wikipedia corpus. The original 2-hop claims are adapted from question-answer pairs from HotpotQA. It is collected by a team of NLP researchers at UNC Chapel Hill and Verisk Analytics.","paperswithcode_id":"hover","key":""},{"id":"hrenwac_para","tags":["annotations_creators:no-annotation","language_creators:found","languages:en","languages:hr","licenses:cc-by-sa-3.0","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@misc{11356/1058,\n title = {Croatian-English parallel corpus {hrenWaC} 2.0},\n author = {Ljube{\\v s}i{\\'c}, Nikola and Espl{\\'a}-Gomis, Miquel and Ortiz Rojas, Sergio and Klubi{\\v c}ka, Filip and Toral, Antonio},\n url = {http://hdl.handle.net/11356/1058},\n note = {Slovenian language resource repository {CLARIN}.{SI}},\n copyright = {{CLARIN}.{SI} User Licence for Internet Corpora},\n year = {2016} }","description":"The hrenWaC corpus version 2.0 consists of parallel Croatian-English texts crawled from the .hr top-level domain for Croatia.\nThe corpus was built with Spidextor (https://github.com/abumatran/spidextor), a tool that glues together the output of SpiderLing used for crawling and Bitextor used for bitext extraction. The accuracy of the extracted bitext on the segment level is around 80% and on the word level around 84%.","key":""},{"id":"hrwac","tags":["annotations_creators:no-annotation","language_creators:found","languages:hr","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:1B<n<10B","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@misc{11356/1064,\n title = {Croatian web corpus {hrWaC} 2.1},\n author = {Ljube{\\v s}i{\\'c}, Nikola and Klubi{\\v c}ka, Filip},\n url = {http://hdl.handle.net/11356/1064},\n note = {Slovenian language resource repository {CLARIN}.{SI}},\n copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},\n year = {2016} }","description":"The Croatian web corpus hrWaC was built by crawling the .hr top-level domain in 2011 and again in 2014. The corpus was near-deduplicated on paragraph level, normalised via diacritic restoration, morphosyntactically annotated and lemmatised. The corpus is shuffled by paragraphs. Each paragraph contains metadata on the URL, domain and language identification (Croatian vs. Serbian).\n\nVersion 2.0 of this corpus is described in http://www.aclweb.org/anthology/W14-0405. Version 2.1 contains newer and better linguistic annotations.","key":""},{"id":"humicroedit","tags":["annotations_creators:crowdsourced","annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-scoring","task_categories:text-classification","task_ids:text-scoring-other-funniness-score-prediction","task_ids:text-classification-other-funnier-headline-identification"],"citation":"@article{hossain2019president,\n  title={\" President Vows to Cut< Taxes> Hair\": Dataset and Analysis of Creative Text Editing for Humorous Headlines},\n  author={Hossain, Nabil and Krumm, John and Gamon, Michael},\n  journal={arXiv preprint arXiv:1906.00274},\n  year={2019}\n}","description":"This new dataset is designed to assess the funniness of edited news headlines.","paperswithcode_id":"humicroedit","key":""},{"id":"hybrid_qa","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:question-answering-other-multihop-tabular-text-qa"],"citation":"@article{chen2020hybridqa,\n  title={HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data},\n  author={Chen, Wenhu and Zha, Hanwen and Chen, Zhiyu and Xiong, Wenhan and Wang, Hong and Wang, William},\n  journal={Findings of EMNLP 2020},\n  year={2020}\n}","description":"Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable.","paperswithcode_id":"hybridqa","key":""},{"id":"hyperpartisan_news_detection","tags":["languages:en"],"citation":"@article{kiesel2019data,\n  title={Data for pan at semeval 2019 task 4: Hyperpartisan news detection},\n  author={Kiesel, Johannes and Mestre, Maria and Shukla, Rishabh and Vincent, Emmanuel and Corney, David and Adineh, Payam and Stein, Benno and Potthast, Martin},\n  year={2019}\n}","description":"Hyperpartisan News Detection was a dataset created for PAN @ SemEval 2019 Task 4.\nGiven a news article text, decide whether it follows a hyperpartisan argumentation, i.e., whether it exhibits blind, prejudiced, or unreasoning allegiance to one party, faction, cause, or person.\n\nThere are 2 parts:\n- byarticle: Labeled through crowdsourcing on an article basis. The data contains only articles for which a consensus among the crowdsourcing workers existed.\n- bypublisher: Labeled by the overall bias of the publisher as provided by BuzzFeed journalists or MediaBiasFactCheck.com.","key":""},{"id":"iapp_wiki_qa_squad","tags":["annotations_creators:expert-generated","language_creators:found","languages:th","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-iapp-wiki-qa-dataset","task_categories:question-answering","task_ids:extractive-qa","task_ids:open-domain-qa"],"citation":"@dataset{kobkrit_viriyayudhakorn_2021_4539916,\n  author       = {Kobkrit Viriyayudhakorn and\n                  Charin Polpanumas},\n  title        = {iapp_wiki_qa_squad},\n  month        = feb,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = 1,\n  doi          = {10.5281/zenodo.4539916},\n  url          = {https://doi.org/10.5281/zenodo.4539916}\n}","description":"`iapp_wiki_qa_squad` is an extractive question answering dataset from Thai Wikipedia articles.\nIt is adapted from [the original iapp-wiki-qa-dataset](https://github.com/iapp-technology/iapp-wiki-qa-dataset)\nto [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format, resulting in\n5761/742/739 questions from 1529/191/192 articles.","key":""},{"id":"id_clickbait","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:id","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:fact-checking"],"citation":"@inproceedings{id_clickbait,\n  author    = {Andika William, Yunita Sari},\n  title     = {CLICK-ID: A Novel Dataset for Indonesian Clickbait Headlines},\n  year      = {2020},\n  url       = {http://dx.doi.org/10.17632/k42j7x2kpn.1},\n}","description":"The CLICK-ID dataset is a collection of Indonesian news headlines that was collected from 12 local online news\npublishers; detikNews, Fimela, Kapanlagi, Kompas, Liputan6, Okezone, Posmetro-Medan, Republika, Sindonews, Tempo,\nTribunnews, and Wowkeren. This dataset is comprised of mainly two parts; (i) 46,119 raw article data, and (ii)\n15,000 clickbait annotated sample headlines. Annotation was conducted with 3 annotator examining each headline.\nJudgment were based only on the headline. The majority then is considered as the ground truth. In the annotated\nsample, our annotation shows 6,290 clickbait and 8,710 non-clickbait.","key":""},{"id":"id_liputan6","tags":["annotations_creators:no-annotation","language_creators:found","languages:id","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@inproceedings{id_liputan6,\n  author    = {Fajri Koto, Jey Han Lau, Timothy Baldwin},\n  title     = {Liputan6: A Large-scale Indonesian Dataset for Text Summarization},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2011.00679},\n}","description":"In this paper, we introduce a large-scale Indonesian summarization dataset. We harvest articles from this http URL,\nan online news portal, and obtain 215,827 document-summary pairs. We leverage pre-trained language models to develop\nbenchmark extractive and abstractive summarization methods over the dataset with multilingual and monolingual\nBERT-based models. We include a thorough error analysis by examining machine-generated summaries that have\nlow ROUGE scores, and expose both issues with ROUGE it-self, as well as with extractive and abstractive\nsummarization models.","key":""},{"id":"id_nergrit_corpus","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:id","licenses:other-nergrit-license","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{id_nergrit_corpus,\n  author    = {Gria Inovasi Teknologi},\n  title     = {NERGRIT CORPUS},\n  year      = {2019},\n  url       = {https://github.com/grit-id/nergrit-corpus},\n}","description":"Nergrit Corpus is a dataset collection for Indonesian Named Entity Recognition, Statement Extraction, and Sentiment\nAnalysis. id_nergrit_corpus is the Named Entity Recognition of this dataset collection which contains 18 entities as\nfollow:\n    'CRD': Cardinal\n    'DAT': Date\n    'EVT': Event\n    'FAC': Facility\n    'GPE': Geopolitical Entity\n    'LAW': Law Entity (such as Undang-Undang)\n    'LOC': Location\n    'MON': Money\n    'NOR': Political Organization\n    'ORD': Ordinal\n    'ORG': Organization\n    'PER': Person\n    'PRC': Percent\n    'PRD': Product\n    'QTY': Quantity\n    'REG': Religion\n    'TIM': Time\n    'WOA': Work of Art\n    'LAN': Language","paperswithcode_id":"nergrit-corpus","key":""},{"id":"id_newspapers_2018","tags":["annotations_creators:no-annotation","language_creators:found","languages:id","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@inproceedings{id_newspapers_2018,\n  author    = {},\n  title     = {Indonesian Newspapers 2018},\n  year      = {2019},\n  url       = {https://github.com/feryandi/Dataset-Artikel},\n}","description":"The dataset contains around 500K articles (136M of words) from 7 Indonesian newspapers: Detik, Kompas, Tempo,\nCNN Indonesia, Sindo, Republika and Poskota. The articles are dated between 1st January 2018 and 20th August 2018\n(with few exceptions dated earlier). The size of uncompressed 500K json files (newspapers-json.tgz) is around 2.2GB,\nand the cleaned uncompressed in a big text file (newspapers.txt.gz) is about 1GB. The original source in Google Drive\ncontains also a dataset in html format which include raw data (pictures, css, javascript, ...)\nfrom the online news website","key":""},{"id":"id_panl_bppt","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","languages:id","licenses:unknown","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{id_panl_bppt,\n  author    = {PAN Localization - BPPT},\n  title     = {Parallel Text Corpora, English Indonesian},\n  year      = {2009},\n  url       = {http://digilib.bppt.go.id/sampul/p92-budiono.pdf},\n}","description":"Parallel Text Corpora for Multi-Domain Translation System created by BPPT (Indonesian Agency for the Assessment and\nApplication of Technology) for PAN Localization Project (A Regional Initiative to Develop Local Language Computing\nCapacity in Asia). The dataset contains around 24K sentences divided in 4 difference topics (Economic, international,\nScience and Technology and Sport).","key":""},{"id":"id_puisi","tags":["annotations_creators:no-annotation","language_creators:found","languages:id","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_ids:language-modeling","task_ids:other-stuctured-to-text"],"description":"Puisi (poem) is an Indonesian poetic form. The dataset contains 7223 Indonesian puisi with its title and author.","key":""},{"id":"igbo_english_machine_translation","tags":["annotations_creators:found","language_creators:found","languages:en","languages:ig","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@misc{ezeani2020igboenglish,\n    title={Igbo-English Machine Translation: An Evaluation Benchmark},\n    author={Ignatius Ezeani and Paul Rayson and Ikechukwu Onyenwe and Chinedu Uchechukwu and Mark Hepple},\n    year={2020},\n    eprint={2004.00648},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2004.00648}\n}","description":"Parallel Igbo-English Dataset","paperswithcode_id":"igbonlp-datasets","key":""},{"id":"igbo_monolingual","tags":["annotations_creators:found","language_creators:found","languages:ig","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","size_categories:n<1K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@misc{ezeani2020igboenglish,\ntitle={Igbo-English Machine Translation: An Evaluation Benchmark},\nauthor={Ignatius Ezeani and Paul Rayson and Ikechukwu Onyenwe and Chinedu Uchechukwu and Mark Hepple},\nyear={2020},\neprint={2004.00648},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}","description":"A dataset is a collection of Monolingual Igbo sentences.","key":""},{"id":"igbo_ner","tags":["annotations_creators:found","language_creators:found","languages:ig","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@misc{ezeani2020igboenglish,\n    title={Igbo-English Machine Translation: An Evaluation Benchmark},\n    author={Ignatius Ezeani and Paul Rayson and Ikechukwu Onyenwe and Chinedu Uchechukwu and Mark Hepple},\n    year={2020},\n    eprint={2004.00648},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"Igbo Named Entity Recognition Dataset","key":""},{"id":"ilist","tags":["task_categories:text-classification","multilinguality:multilingual","task_ids:text-classification-other-language-identification","languages:hi","languages:awa","languages:bho","languages:mag","languages:bra","annotations_creators:unknown","source_datasets:original","size_categories:10K<n<100K","licenses:unknown"],"citation":"@proceedings{ws-2018-nlp-similar,\n    title = \"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)\",\n    editor = {Zampieri, Marcos  and\n      Nakov, Preslav  and\n      Ljube{\\v{s}}i{\\'c}, Nikola  and\n      Tiedemann, J{\\\"o}rg  and\n      Malmasi, Shervin  and\n      Ali, Ahmed},\n    month = aug,\n    year = \"2018\",\n    address = \"Santa Fe, New Mexico, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W18-3900\",\n}","description":"This dataset is introduced in a task which aimed at identifying 5 closely-related languages of Indo-Aryan language family –\nHindi (also known as Khari Boli), Braj Bhasha, Awadhi, Bhojpuri, and Magahi.","key":""},{"id":"imdb","tags":["languages:en"],"citation":"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n  title     = {Learning Word Vectors for Sentiment Analysis},\n  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n  month     = {June},\n  year      = {2011},\n  address   = {Portland, Oregon, USA},\n  publisher = {Association for Computational Linguistics},\n  pages     = {142--150},\n  url       = {http://www.aclweb.org/anthology/P11-1015}\n}","description":"Large Movie Review Dataset.\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\\","paperswithcode_id":"imdb-movie-reviews","key":""},{"id":"imdb_urdu_reviews","tags":["annotations_creators:found","language_creators:machine-generated","languages:ur","licenses:odbl-1.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n  author    = {Maas, Andrew L. and Daly,nRaymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y...},\n  title     = {Learning Word Vectors for Sentiment Analysis},\n  month     = {June},\n  year      = {2011},\n  address   = {Portland, Oregon, USA},\n  publisher = {Association for Computational Linguistics},\n  pages     = {142--150},\n  url       = {http://www.aclweb.org/anthology/P11-1015}\n}","description":"Large Movie translated Urdu Reviews Dataset.\nThis is a dataset for binary sentiment classification containing substantially more data than previous\nbenchmark datasets. We provide a set of 40,000 highly polar movie reviews for training, and 10,000 for testing.\nTo increase the availability of sentiment analysis dataset for a low recourse language like Urdu,\nwe opted to use the already available IMDB Dataset. we have translated this dataset using google translator.\nThis is a binary classification dataset having two classes as positive and negative.\nThe reason behind using this dataset is high polarity for each class.\nIt contains 50k samples equally divided in two classes.","key":""},{"id":"imppres","tags":["annotations_creators:machine-generated","language_creators:machine-generated","languages:en","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:natural-language-inference"],"citation":"@inproceedings{jeretic-etal-2020-natural,\n    title = \"Are Natural Language Inference Models {IMPPRESsive}? {L}earning {IMPlicature} and {PRESupposition}\",\n    author = \"Jereti\\v{c}, Paloma  and\n      Warstadt, Alex  and\n      Bhooshan, Suvrat  and\n      Williams, Adina\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.768\",\n    doi = \"10.18653/v1/2020.acl-main.768\",\n    pages = \"8690--8705\",\n    abstract = \"Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by {``}some{''} as entailments. For some presupposition triggers like {``}only{''}, BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.\",\n}","description":"Over >25k semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types. IMPPRES is an NLI dataset following the format of SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018) and XNLI (Conneau et al., 2018), which was created to evaluate how well trained NLI models recognize several classes of presuppositions and scalar implicatures.","paperswithcode_id":"imppres","key":""},{"id":"indic_glue","tags":[],"citation":"    @inproceedings{kakwani2020indicnlpsuite,\n    title={{IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages}},\n    author={Divyanshu Kakwani and Anoop Kunchukuttan and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n    year={2020},\n    booktitle={Findings of EMNLP},\n}","description":"    IndicGLUE is a natural language understanding benchmark for Indian languages. It contains a wide\n    variety of tasks and covers 11 major Indian languages - as, bn, gu, hi, kn, ml, mr, or, pa, ta, te.","key":""},{"id":"indonlu","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:id","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:1K<n<10K","size_categories:n<1K","source_datasets:original","task_categories:structure-prediction","task_categories:text-classification","task_categories:question-answering","task_ids:part-of-speech-tagging","task_ids:text-classification-other-aspect-based-sentiment-analysis","task_ids:multi-class-classification","task_ids:closed-domain-qa","task_ids:structure-prediction-other-keyphrase-extraction","task_ids:named-entity-recognition","task_ids:sentiment-classification","task_ids:structure-prediction-other-span-extraction","task_ids:semantic-similarity-classification"],"citation":"@inproceedings{wilie2020indonlu,\ntitle = {{IndoNLU}: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding},\nauthors={Bryan Wilie and Karissa Vincentio and Genta Indra Winata and Samuel Cahyawijaya and X. Li and Zhi Yuan Lim and S. Soleman and R. Mahendra and Pascale Fung and Syafri Bahar and A. Purwarianti},\nbooktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},\nyear={2020}\n}","description":"The IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for Bahasa Indonesia.","paperswithcode_id":"indonlu-benchmark","key":""},{"id":"inquisitive_qg","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-question-generation"],"citation":"@InProceedings{ko2020inquisitive,\n  author    = {Ko, Wei-Jen and Chen, Te-Yuan and Huang, Yiyan and Durrett, Greg and Li, Junyi Jessy},\n  title     = {Inquisitive Question Generation for High Level Text Comprehension},\n  booktitle = {Proceedings of EMNLP},\n  year      = {2020},\n}","description":"A dataset of about 20k questions that are elicited from readers as they naturally read through a document sentence by sentence. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. Because these questions are generated while the readers are processing the information, the questions directly communicate gaps between the reader’s and writer’s knowledge about the events described in the text, and are not necessarily answered in the document itself. This type of question reflects a real-world scenario: if one has questions during reading, some of them are answered by the text later on, the rest are not, but any of them would help further the reader’s understanding at the particular point when they asked it. This resource could enable question generation models to simulate human-like curiosity and cognitive processing, which may open up a new realm of applications.","paperswithcode_id":"inquisitive","key":""},{"id":"interpress_news_category_tr","tags":["annotations_creators:found","language_creators:found","languages:tr","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-news-category-classification"],"description":"It is a Turkish news data set consisting of 273601 news in 17 categories, compiled from print media and news websites between 2010 and 2017 by the Interpress (https://www.interpress.com/) media monitoring company.","key":""},{"id":"interpress_news_category_tr_lite","tags":["annotations_creators:found","language_creators:found","languages:tr","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:extended|interpress_news_category_tr","task_categories:text-classification","task_ids:text-classification-other-news-category-classification"],"description":"It is a Turkish news data set consisting of 273601 news in 10 categories, compiled from print media and news websites between 2010 and 2017 by the Interpress (https://www.interpress.com/) media monitoring company. It has been rearranged as easily separable and with fewer classes.","key":""},{"id":"irc_disentangle","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_categories:structure-prediction","task_ids:coreference-resolution","task_ids:dialogue-modeling"],"citation":"@InProceedings{acl19disentangle,\nauthor    = {Jonathan K. Kummerfeld and Sai R. Gouravajhala and Joseph Peper and Vignesh Athreya and Chulaka Gunasekara and Jatin Ganhotra and Siva Sankalp Patel and Lazaros Polymenakos and Walter S. Lasecki},\ntitle     = {A Large-Scale Corpus for Conversation Disentanglement},\nbooktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},\nlocation  = {Florence, Italy},\nmonth     = {July},\nyear      = {2019},\ndoi       = {10.18653/v1/P19-1374},\npages     = {3846--3856},\nurl       = {https://aclweb.org/anthology/papers/P/P19/P19-1374/},\narxiv     = {https://arxiv.org/abs/1810.11118},\nsoftware  = {https://jkk.name/irc-disentanglement},\ndata      = {https://jkk.name/irc-disentanglement},\n}","description":"Disentangling conversations mixed together in a single stream of messages is\na difficult task, made harder by the lack of large manually annotated\ndatasets. This new dataset of 77,563 messages manually annotated with\nreply-structure graphs that both disentangle conversations and define\ninternal conversation structure. The dataset is 16 times larger than all\npreviously released datasets combined, the first to include adjudication of\nannotation disagreements, and the first to include context.","paperswithcode_id":"irc-disentanglement","key":""},{"id":"isixhosa_ner_corpus","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:xh","licenses:other-Creative Commons Attribution 2.5 South Africa License","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{isixhosa_ner_corpus,\n  author    = {K. Podile and\n              Roald Eiselen},\n  title     = {NCHLT isiXhosa Named Entity Annotated Corpus},\n  booktitle = {Eiselen, R. 2016. Government domain named entity recognition for South African languages. Proceedings of the 10th      Language Resource and Evaluation Conference, Portorož, Slovenia.},\n  year      = {2016},\n  url       = {https://repo.sadilar.org/handle/20.500.12185/312},\n}","description":"Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.","key":""},{"id":"isizulu_ner_corpus","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:zu","licenses:other-Creative Commons Attribution 2.5 South Africa","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{isizulu_ner_corpus,\n  author    = {A.N. Manzini and\n              Roald Eiselen},\n  title     = {NCHLT isiZulu Named Entity Annotated Corpus},\n  booktitle = {Eiselen, R. 2016. Government domain named entity recognition for South African languages. Proceedings of the 10th      Language Resource and Evaluation Conference, Portorož, Slovenia.},\n  year      = {2016},\n  url       = {https://repo.sadilar.org/handle/20.500.12185/319},\n}","description":"Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.","key":""},{"id":"iwslt2017","tags":[],"citation":"@inproceedings{cettoloEtAl:EAMT2012,\nAddress = {Trento, Italy},\nAuthor = {Mauro Cettolo and Christian Girardi and Marcello Federico},\nBooktitle = {Proceedings of the 16$^{th}$ Conference of the European Association for Machine Translation (EAMT)},\nDate = {28-30},\nMonth = {May},\nPages = {261--268},\nTitle = {WIT$^3$: Web Inventory of Transcribed and Translated Talks},\nYear = {2012}}","description":"The IWSLT 2017 Evaluation Campaign includes a multilingual TED Talks MT task. The languages involved are five:\n\n  German, English, Italian, Dutch, Romanian.\n\nFor each language pair, training and development sets are available through the entry of the table below: by clicking, an archive will be downloaded which contains the sets and a README file. Numbers in the table refer to millions of units (untokenized words) of the target side of all parallel training sets.","key":""},{"id":"jeopardy","tags":["languages:en"],"citation":"","description":"Dataset containing 216,930 Jeopardy questions, answers and other data.\n\nThe json file is an unordered list of questions where each question has\n'category' : the question category, e.g. \"HISTORY\"\n'value' : integer $ value of the question as string, e.g. \"200\"\nNote: This is \"None\" for Final Jeopardy! and Tiebreaker questions\n'question' : text of question\nNote: This sometimes contains hyperlinks and other things messy text such as when there's a picture or video question\n'answer' : text of answer\n'round' : one of \"Jeopardy!\",\"Double Jeopardy!\",\"Final Jeopardy!\" or \"Tiebreaker\"\nNote: Tiebreaker questions do happen but they're very rare (like once every 20 years)\n'show_number' : int of show number, e.g '4680'\n'air_date' : string of the show air date in format YYYY-MM-DD","key":""},{"id":"jfleg","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","multilinguality:other-language-learner","size_categories:1K<n<10K","source_datasets:extended|other-GUG-grammaticality-judgements","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-grammatical-error-correction"],"citation":"@InProceedings{napoles-sakaguchi-tetreault:2017:EACLshort,\n  author    = {Napoles, Courtney\n               and  Sakaguchi, Keisuke\n               and  Tetreault, Joel},\n  title     = {JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction},\n  booktitle = {Proceedings of the 15th Conference of the European Chapter of the\n               Association for Computational Linguistics: Volume 2, Short Papers},\n  month     = {April},\n  year      = {2017},\n  address   = {Valencia, Spain},\n  publisher = {Association for Computational Linguistics},\n  pages     = {229--234},\n  url       = {http://www.aclweb.org/anthology/E17-2037}\n}\n@InProceedings{heilman-EtAl:2014:P14-2,\n  author    = {Heilman, Michael\n               and  Cahill, Aoife\n               and  Madnani, Nitin\n               and  Lopez, Melissa\n               and  Mulholland, Matthew\n               and  Tetreault, Joel},\n  title     = {Predicting Grammaticality on an Ordinal Scale},\n  booktitle = {Proceedings of the 52nd Annual Meeting of the\n               Association for Computational Linguistics (Volume 2: Short Papers)},\n  month     = {June},\n  year      = {2014},\n  address   = {Baltimore, Maryland},\n  publisher = {Association for Computational Linguistics},\n  pages     = {174--180},\n  url       = {http://www.aclweb.org/anthology/P14-2029}\n}","description":"JFLEG (JHU FLuency-Extended GUG) is an English grammatical error correction (GEC) corpus.\nIt is a gold standard benchmark for developing and evaluating GEC systems with respect to\nfluency (extent to which a text is native-sounding) as well as grammaticality.\n\nFor each source document, there are four human-written corrections (ref0 to ref3).","paperswithcode_id":"jfleg","key":""},{"id":"jigsaw_toxicity_pred","tags":["annotations_creators:crowdsourced","language_creators:other","languages:en","licenses:cc0-1.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:multi-label-classification"],"description":"This dataset consists of a large number of Wikipedia comments which have been labeled by human raters for toxic behavior.","key":""},{"id":"jnlpba","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other-genia-v3.02","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{kim2004introduction,\n               title={Introduction to the bio-entity recognition task at JNLPBA},\n               author={Kim, Jin-Dong and Ohta, Tomoko and Tsuruoka, Yoshimasa and Tateisi, Yuka and Collier, Nigel},\n               booktitle={Proceedings of the international joint workshop on natural language processing in biomedicine and its applications},\n               pages={70--75},\n               year={2004},\n               organization={Citeseer}\n}","description":"The data came from the GENIA version 3.02 corpus (Kim et al., 2003). This was formed from a controlled search\non MEDLINE using the MeSH terms \u0018human\u0019, \u0018blood cells\u0019 and \u0018transcription factors\u0019. From this search 2,000 abstracts\nwere selected and hand annotated according to a small taxonomy of 48 classes based on a chemical classification.\nAmong the classes, 36 terminal classes were used to annotate the GENIA corpus.","key":""},{"id":"journalists_questions","tags":["annotations_creators:crowdsourced","language_creators:other","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-question-identification"],"citation":"\\\r\n@inproceedings{hasanain2016questions,\r\n  title={What Questions Do Journalists Ask on Twitter?},\r\n  author={Hasanain, Maram and Bagdouri, Mossaab and Elsayed, Tamer and Oard, Douglas W},\r\n  booktitle={Tenth International AAAI Conference on Web and Social Media},\r\n  year={2016}\r\n}","description":"\\\r\nThe journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\r\ntweets manually labeled for question identification over Arabic tweets posted by journalists.","key":""},{"id":"kannada_news","tags":["annotations_creators:other","language_creators:other","languages:kn","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:topic-classification"],"description":"The Kannada news dataset contains only the headlines of news article in three categories:\nEntertainment, Tech, and Sports.\n\nThe data set contains around 6300 news article headlines which collected from Kannada news websites.\nThe data set has been cleaned and contains train and test set using which can be used to benchmark\nclassification models in Kannada.","key":""},{"id":"kd_conv","tags":["annotations_creators:crowdsourced","annotations_creators:machine-generated","language_creators:crowdsourced","languages:zh","licenses:apache-2.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:dialogue-modeling","task_ids:other-multi-turn"],"citation":"@inproceedings{zhou-etal-2020-kdconv,\n    title = \"{K}d{C}onv: A {C}hinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation\",\n    author = \"Zhou, Hao  and\n      Zheng, Chujie  and\n      Huang, Kaili  and\n      Huang, Minlie  and\n      Zhu, Xiaoyan\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.635\",\n    doi = \"10.18653/v1/2020.acl-main.635\",\n    pages = \"7098--7108\",\n}","description":"KdConv is a Chinese multi-domain Knowledge-driven Conversionsation dataset, grounding the topics in multi-turn conversations to knowledge graphs. KdConv contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics, while the corpus can also used for exploration of transfer learning and domain adaptation.\\","paperswithcode_id":"kdconv","key":""},{"id":"kde4","tags":["annotations_creators:found","language_creators:found","languages:af","languages:ar","languages:as","languages:ast","languages:be","languages:bg","languages:bn","languages:bn_IN","languages:br","languages:ca","languages:crh","languages:cs","languages:csb","languages:cy","languages:da","languages:de","languages:el","languages:en","languages:en_GB","languages:eo","languages:es","languages:et","languages:eu","languages:fa","languages:fi","languages:fr","languages:fy","languages:ga","languages:gl","languages:gu","languages:ha","languages:he","languages:hi","languages:hne","languages:hr","languages:hsb","languages:hu","languages:hy","languages:id","languages:is","languages:it","languages:ja","languages:ka","languages:kk","languages:km","languages:kn","languages:ko","languages:ku","languages:lb","languages:lt","languages:lv","languages:mai","languages:mk","languages:ml","languages:mr","languages:ms","languages:mt","languages:nb","languages:nds","languages:ne","languages:nl","languages:nn","languages:nso","languages:oc","languages:or","languages:pa","languages:pl","languages:ps","languages:pt","languages:pt_BR","languages:ro","languages:ru","languages:rw","languages:se","languages:si","languages:sk","languages:sl","languages:sr","languages:sv","languages:ta","languages:te","languages:tg","languages:th","languages:tr","languages:uk","languages:uz","languages:vi","languages:wa","languages:xh","languages:zh_CN","languages:zh_HK","languages:zh_TW","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"A parallel corpus of KDE4 localization files (v.2).\n\n92 languages, 4,099 bitexts\ntotal number of files: 75,535\ntotal number of tokens: 60.75M\ntotal number of sentence fragments: 8.89M","key":""},{"id":"kelm","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:cc-by-sa-2.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:other","task_ids:other-other-data-to-text-generation"],"citation":"@misc{agarwal2020large,\n      title={Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training},\n      author={Oshin Agarwal and Heming Ge and Siamak Shakeri and Rami Al-Rfou},\n      year={2020},\n      eprint={2010.12688},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Data-To-Text Generation involves converting knowledge graph (KG) triples of the form (subject, relation, object) into\na natural language sentence(s). This dataset consists of English KG data converted into paired natural language text.\nThe generated corpus consists of ∼18M sentences spanning ∼45M triples with ∼1500 distinct relations.","paperswithcode_id":"kelm","key":""},{"id":"kilt_tasks","tags":["annotations_creators:crowdsourced","annotations_creators:found","annotations_creators:machine-generated","language_creators:crowdsourced","language_creators:found","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:100K<n<1M","size_categories:1M<n<10M","size_categories:1K<n<10K","source_datasets:extended|other-aidayago","source_datasets:original","source_datasets:extended|other-wned-cweb","source_datasets:extended|other-hotpotqa","source_datasets:extended|other-fever","source_datasets:extended|natural_questions","source_datasets:extended|other-zero-shot-re","source_datasets:extended|other-trex","source_datasets:extended|other-triviaqa","source_datasets:extended|other-wned-wiki","source_datasets:extended|other-wizardsofwikipedia","task_categories:text-retrieval","task_categories:question-answering","task_categories:text-classification","task_categories:sequence-modeling","task_ids:document-retrieval","task_ids:entity-linking-retrieval","task_ids:abstractive-qa","task_ids:open-domain-qa","task_ids:fact-checking","task_ids:fact-checking-retrieval","task_ids:extractive-qa","task_ids:slot-filling","task_ids:dialogue-modeling"],"citation":"@inproceedings{fb_kilt,\n    author    = {Fabio Petroni and\n                 Aleksandra Piktus and\n                 Angela Fan and\n                 Patrick Lewis and\n                 Majid Yazdani and\n                 Nicola De Cao and\n                 James Thorne and\n                 Yacine Jernite and\n                 Vassilis Plachouras and\n                 Tim Rockt\\\"aschel and\n                 Sebastian Riedel},\n    title     = {{KILT:} a {B}enchmark for {K}nowledge {I}ntensive {L}anguage {T}asks},\n    journal   = {CoRR},\n    archivePrefix = {arXiv},\n    year      = {2020},","description":"KILT tasks training and evaluation data.\n- [FEVER](https://fever.ai) | Fact Checking | fever\n- [AIDA CoNLL-YAGO](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) | Entity Linking | aidayago2\n- [WNED-WIKI](https://github.com/U-Alberta/wned) | Entity Linking | wned\n- [WNED-CWEB](https://github.com/U-Alberta/wned) | Entity Linking | cweb\n- [T-REx](https://hadyelsahar.github.io/t-rex) | Slot Filling | trex\n- [Zero-Shot RE](http://nlp.cs.washington.edu/zeroshot) | Slot Filling | structured_zeroshot\n- [Natural Questions](https://ai.google.com/research/NaturalQuestions) | Open Domain QA  | nq\n- [HotpotQA](https://hotpotqa.github.io) | Open Domain QA | hotpotqa\n- [TriviaQA](http://nlp.cs.washington.edu/triviaqa) | Open Domain QA | triviaqa\n- [ELI5](https://facebookresearch.github.io/ELI5/explore.html) | Open Domain QA | eli5\n- [Wizard of Wikipedia](https://parl.ai/projects/wizard_of_wikipedia) | Dialogue | wow\n\nTo finish linking TriviaQA questions to the IDs provided, follow the instructions [here](http://github.com/huggingface/datasets/datasets/kilt_tasks/README.md).","paperswithcode_id":"kilt","key":""},{"id":"kilt_wikipedia","tags":[],"citation":"@inproceedings{fb_kilt,\n    author    = {Fabio Petroni and\n                 Aleksandra Piktus and\n                 Angela Fan and\n                 Patrick Lewis and\n                 Majid Yazdani and\n                 Nicola De Cao and\n                 James Thorne and\n                 Yacine Jernite and\n                 Vassilis Plachouras and\n                 Tim Rockt\\\"aschel and\n                 Sebastian Riedel},\n    title     = {{KILT:} a {B}enchmark for {K}nowledge {I}ntensive {L}anguage {T}asks},\n    journal   = {CoRR},\n    archivePrefix = {arXiv},\n    year      = {2020},","description":"KILT-Wikipedia: Wikipedia pre-processed for KILT.","key":""},{"id":"kinnews_kirnews","tags":["annotations_creators:expert-generated","language_creators:found","languages:rn","languages:rw","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification","task_ids:topic-classification"],"citation":"@article{niyongabo2020kinnews,\n  title={KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for Kinyarwanda and Kirundi},\n  author={Niyongabo, Rubungo Andre and Qu, Hong and Kreutzer, Julia and Huang, Li},\n  journal={arXiv preprint arXiv:2010.12174},\n  year={2020}\n}","description":"Kinyarwanda and Kirundi news classification datasets","paperswithcode_id":"kinnews-and-kirnews","key":""},{"id":"klue","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:ko","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_categories:text-scoring","task_categories:structure-prediction","task_categories:question-answering","task_categories:sequence-modeling","task_ids:topic-classification","task_ids:semantic-similarity-scoring","task_ids:natural-language-inference","task_ids:named-entity-recognition","task_ids:other-relation-extraction","task_ids:parsing","task_ids:extractive-qa","task_ids:other-dialogue-state-tracking"],"citation":"@misc{park2021klue,\n      title={KLUE: Korean Language Understanding Evaluation},\n      author={Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n      year={2021},\n      eprint={2105.09680},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"KLUE (Korean Language Understanding Evaluation)\nKorean Language Understanding Evaluation (KLUE) benchmark is a series of datasets to evaluate natural language\nunderstanding capability of Korean language models. KLUE consists of 8 diverse and representative tasks, which are accessible\nto anyone without any restrictions. With ethical considerations in mind, we deliberately design annotation guidelines to obtain\nunambiguous annotations for all datasets. Futhermore, we build an evaluation system and carefully choose evaluations metrics\nfor every task, thus establishing fair comparison across Korean language models.","paperswithcode_id":"klue","key":""},{"id":"kor_3i4k","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:ko","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification"],"citation":"@article{cho2018speech,\n    title={Speech Intention Understanding in a Head-final Language: A Disambiguation Utilizing Intonation-dependency},\n    author={Cho, Won Ik and Lee, Hyeon Seung and Yoon, Ji Won and Kim, Seok Min and Kim, Nam Soo},\n    journal={arXiv preprint arXiv:1811.04231},\n    year={2018}\n}","description":"This dataset is designed to identify speaker intention based on real-life spoken utterance in Korean into one of\n7 categories: fragment, description, question, command, rhetorical question, rhetorical command, utterances.","key":""},{"id":"kor_hate","tags":["annotations_creators:crowdsourced","annotations_creators:expert-generated","language_creators:found","languages:ko","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:multi-label-classification"],"citation":"@inproceedings{moon-etal-2020-beep,\n    title = \"{BEEP}! {K}orean Corpus of Online News Comments for Toxic Speech Detection\",\n    author = \"Moon, Jihyung  and\n      Cho, Won Ik  and\n      Lee, Junbum\",\n    booktitle = \"Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.socialnlp-1.4\",\n    pages = \"25--31\",\n    abstract = \"Toxic comments in online platforms are an unavoidable social issue under the cloak of anonymity. Hate speech detection has been actively done for languages such as English, German, or Italian, where manually labeled corpus has been released. In this work, we first present 9.4K manually labeled entertainment news comments for identifying Korean toxic speech, collected from a widely used online news platform in Korea. The comments are annotated regarding social bias and hate speech since both aspects are correlated. The inter-annotator agreement Krippendorff{'}s alpha score is 0.492 and 0.496, respectively. We provide benchmarks using CharCNN, BiLSTM, and BERT, where BERT achieves the highest score on all tasks. The models generally display better performance on bias identification, since the hate speech detection is a more subjective issue. Additionally, when BERT is trained with bias label for hate speech detection, the prediction score increases, implying that bias and hate are intertwined. We make our dataset publicly available and open competitions with the corpus and benchmarks.\",\n}","description":"Human-annotated Korean corpus collected from a popular domestic entertainment news aggregation platform\nfor toxic speech detection. Comments are annotated for gender bias, social bias and hate speech.","paperswithcode_id":"korean-hatespeech-dataset","key":""},{"id":"kor_ner","tags":["annotations_creators:expert-generated","language_creators:other","languages:ko","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@InProceedings{Kim:2016,\n  title     = \"Korean Named Entity Recognition Dataset\",\n  authors   = \"Jae-Hoon Kim\",\n  publisher = \"GitHub\",\n  year      = \"2016\"\n}","description":"Korean named entity recognition dataset","key":""},{"id":"kor_nli","tags":[],"citation":"@article{ham2020kornli,\n  title={KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding},\n  author={Ham, Jiyeon and Choe, Yo Joong and Park, Kyubyong and Choi, Ilji and Soh, Hyungjoon},\n  journal={arXiv preprint arXiv:2004.03289},\n  year={2020}\n}","description":"Korean Natural  Language Inference datasets","paperswithcode_id":"kornli","key":""},{"id":"kor_nlu","tags":["annotations_creators:found","language_creators:expert-generated","language_creators:found","language_creators:machine-generated","languages:ko","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:extended|snli","task_categories:text-classification","task_categories:text-scoring","task_ids:natural-language-inference","task_ids:semantic-similarity-scoring"],"description":"    The dataset contains data for bechmarking korean models on NLI and STS","key":""},{"id":"kor_qpair","tags":["annotations_creators:expert-generated","language_creators:other","languages:ko","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:semantic-similarity-classification"],"citation":"@misc{Song:2018,\n  title     = \"Paired Question v.2\",\n  authors   = \"Youngsook Song\",\n  publisher = \"GitHub\",\n  year      = \"2018\"\n}","description":"This is a Korean paired question dataset containing labels indicating whether two questions in a given pair are semantically identical. This dataset was used to evaluate the performance of [KoGPT2](https://github.com/SKT-AI/KoGPT2#subtask-evaluations) on a phrase detection downstream task.","key":""},{"id":"kor_sae","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:ko","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification"],"citation":"@article{cho2019machines,\n  title={Machines Getting with the Program: Understanding Intent Arguments of Non-Canonical Directives},\n  author={Cho, Won Ik and Moon, Young Ki and Moon, Sangwhan and Kim, Seok Min and Kim, Nam Soo},\n  journal={arXiv preprint arXiv:1912.00342},\n  year={2019}\n}","description":"This new dataset is designed to extract intent from non-canonical directives which will help dialog managers\nextract intent from user dialog that may have no clear objective or are paraphrased forms of utterances.","key":""},{"id":"kor_sarcasm","tags":["annotations_creators:expert-generated","language_creators:found","languages:ko","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-sarcasm-detection"],"description":"This is a dataset designed to detect sarcasm in Korean because it distorts the literal meaning of a sentence\nand is highly related to sentiment classification.","key":""},{"id":"labr","tags":["annotations_creators:found","language_creators:found","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"citation":"@inproceedings{aly2013labr,\n  title={Labr: A large scale arabic book reviews dataset},\n  author={Aly, Mohamed and Atiya, Amir},\n  booktitle={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},\n  pages={494--498},\n  year={2013}\n}","description":"This dataset contains over 63,000 book reviews in Arabic.It is the largest sentiment analysis dataset for Arabic to-date.The book reviews were harvested from the website Goodreads during the month or March 2013.Each book review comes with the goodreads review id, the user id, the book id, the rating (1 to 5) and the text of the review.","paperswithcode_id":"labr","key":""},{"id":"lama","tags":["annotations_creators:crowdsourced","annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:crowdsourced","language_creators:expert-generated","language_creators:machine-generated","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:1K<n<10K","size_categories:n<1K","size_categories:1M<n<10M","source_datasets:extended|conceptnet5","source_datasets:extended|squad","task_categories:text-retrieval","task_categories:text-scoring","task_ids:fact-checking-retrieval","task_ids:text-scoring-other-probing"],"citation":"@inproceedings{petroni2019language,\r\n  title={Language Models as Knowledge Bases?},\r\n  author={F. Petroni, T. Rockt{\\\"{a}}schel, A. H. Miller, P. Lewis, A. Bakhtin, Y. Wu and S. Riedel},\r\n  booktitle={In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019},\r\n  year={2019}\r\n}\r\n@inproceedings{petroni2020how,\r\n  title={How Context Affects Language Models' Factual Predictions},\r\n  author={Fabio Petroni and Patrick Lewis and Aleksandra Piktus and Tim Rockt{\\\"a}schel and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},\r\n  booktitle={Automated Knowledge Base Construction},\r\n  year={2020},\r\n  url={https://openreview.net/forum?id=025X0zPfn}\r\n}","description":"LAMA is a dataset used to probe and analyze the factual and commonsense knowledge contained in pretrained language models. See https://github.com/facebookresearch/LAMA.","paperswithcode_id":"lama","key":""},{"id":"lambada","tags":["task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-long-range-dependency","multilinguality:monolingual","languages:en","language_creators:found","annotations_creators:expert-generated","source_datasets:extended|bookcorpus","size_categories:10K<n<100K","licenses:cc-by-4.0"],"citation":"@InProceedings{paperno-EtAl:2016:P16-1,\n  author    = {Paperno, Denis  and  Kruszewski, Germ\\'{a}n  and  Lazaridou,\nAngeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,\nSandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},\n  title     = {The {LAMBADA} dataset: Word prediction requiring a broad\ndiscourse context},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1525--1534},\n  url       = {http://www.aclweb.org/anthology/P16-1144}\n}","description":"The LAMBADA evaluates the capabilities of computational models\nfor text understanding by means of a word prediction task.\nLAMBADA is a collection of narrative passages sharing the characteristic\nthat human subjects are able to guess their last word if\nthey are exposed to the whole passage, but not if they\nonly see the last sentence preceding the target word.\nTo succeed on LAMBADA, computational models cannot\nsimply rely on local context, but must be able to\nkeep track of information in the broader discourse.\n\nThe LAMBADA dataset is extracted from BookCorpus and\nconsists of 10'022 passages, divided into 4'869 development\nand 5'153 test passages. The training data for language\nmodels to be tested on LAMBADA include the full text\nof 2'662 novels (disjoint from those in dev+test),\ncomprising 203 million words.","paperswithcode_id":"lambada","key":""},{"id":"large_spanish_corpus","tags":["annotations_creators:no-annotation","language_creators:expert-generated","languages:es","licenses:mit","multilinguality:monolingual","size_categories:1M<n<10M","size_categories:10M<n<100M","size_categories:100K<n<1M","size_categories:100M<n<1B","size_categories:10K<n<100K","source_datasets:original","task_categories:other","task_ids:other-other-pretraining-language-models"],"citation":"@dataset{jose_canete_2019_3247731,\n  author       = {José Cañete},\n  title        = {Compilation of Large Spanish Unannotated Corpora},\n  month        = may,\n  year         = 2019,\n  publisher    = {Zenodo},\n  doi          = {10.5281/zenodo.3247731},\n  url          = {https://doi.org/10.5281/zenodo.3247731}\n}","description":"The Large Spanish Corpus is a compilation of 15 unlabelled Spanish corpora spanning Wikipedia to European parliament notes. Each config contains the data corresponding to a different corpus. For example, \"all_wiki\" only includes examples from Spanish Wikipedia. By default, the config is set to \"combined\" which loads all the corpora; with this setting you can also specify the number of samples to return per corpus by configuring the \"split\" argument.","key":""},{"id":"laroseda","tags":["annotations_creators:found","language_creators:found","languages:ro","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@article{\n    tache2101clustering,\n    title={Clustering Word Embeddings with Self-Organizing Maps. Application on LaRoSeDa -- A Large Romanian Sentiment Data Set},\n    author={Anca Maria Tache and Mihaela Gaman and Radu Tudor Ionescu},\n    journal={ArXiv},\n    year = {2021}\n}","description":"        LaRoSeDa (A Large Romanian Sentiment Data Set) contains 15,000 reviews written in Romanian, of which 7,500 are positive and 7,500 negative.\n        Star ratings of 1 and 2 and of 4 and 5 are provided for negative and positive reviews respectively.\n        The current dataset uses star rating as the label for multi-class classification.","key":""},{"id":"lc_quad","tags":["languages:en"],"citation":"@inproceedings{dubey2017lc2,\ntitle={LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and DBpedia},\nauthor={Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},\nbooktitle={Proceedings of the 18th International Semantic Web Conference (ISWC)},\nyear={2019},\norganization={Springer}\n}","description":"LC-QuAD 2.0 is a Large Question Answering dataset with 30,000 pairs of question and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version. Please see our paper for details about the dataset creation process and framework.","paperswithcode_id":"lc-quad-2-0","key":""},{"id":"lener_br","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:pt","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{luz_etal_propor2018,\n    author = {Pedro H. {Luz de Araujo} and Te\\'{o}filo E. {de Campos} and\n    Renato R. R. {de Oliveira} and Matheus Stauffer and\n    Samuel Couto and Paulo Bermejo},\n    title = {{LeNER-Br}: a Dataset for Named Entity Recognition in {Brazilian} Legal Text},\n    booktitle = {International Conference on the Computational Processing of Portuguese ({PROPOR})},\n    publisher = {Springer},\n    series = {Lecture Notes on Computer Science ({LNCS})},\n    pages = {313--323},\n    year = {2018},\n    month = {September 24-26},\n    address = {Canela, RS, Brazil},\n    doi = {10.1007/978-3-319-99722-3_32},\n    url = {https://cic.unb.br/~teodecampos/LeNER-Br/},\n}","description":"LeNER-Br is a Portuguese language dataset for named entity recognition\napplied to legal documents. LeNER-Br consists entirely of manually annotated\nlegislation and legal cases texts and contains tags for persons, locations,\ntime entities, organizations, legislation and legal cases.\nTo compose the dataset, 66 legal documents from several Brazilian Courts were\ncollected. Courts of superior and state levels were considered, such as Supremo\nTribunal Federal, Superior Tribunal de Justiça, Tribunal de Justiça de Minas\nGerais and Tribunal de Contas da União. In addition, four legislation documents\nwere collected, such as \"Lei Maria da Penha\", giving a total of 70 documents","paperswithcode_id":"lener-br","key":""},{"id":"liar","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-fake-news-detection"],"citation":"@inproceedings{wang-2017-liar,\ntitle = \"{``}Liar, Liar Pants on Fire{''}: A New Benchmark Dataset for Fake News Detection\",\nauthor = \"Wang, William Yang\",\nbooktitle = \"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\nmonth = jul,\nyear = \"2017\",\naddress = \"Vancouver, Canada\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/P17-2067\",\ndoi = \"10.18653/v1/P17-2067\",\npages = \"422--426\",\nabstract = \"Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.\",\n}","description":"LIAR is a dataset for fake news detection with 12.8K human labeled short statements from politifact.com's API, and each statement is evaluated by a politifact.com editor for its truthfulness. The distribution of labels in the LIAR dataset is relatively well-balanced: except for 1,050 pants-fire cases, the instances for all other labels range from 2,063 to 2,638. In each case, the labeler provides a lengthy analysis report to ground each judgment.","paperswithcode_id":"liar","key":""},{"id":"librispeech_asr","tags":["pretty_name:LibriSpeech","annotations_creators:expert-generated","language_creators:crowdsourced","language_creators:expert-generated","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:speech-processing","task_ids:automatic-speech-recognition"],"citation":"@inproceedings{panayotov2015librispeech,\n  title={Librispeech: an ASR corpus based on public domain audio books},\n  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},\n  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},\n  pages={5206--5210},\n  year={2015},\n  organization={IEEE}\n}","description":"LibriSpeech is a corpus of approximately 1000 hours of read English speech with sampling rate of 16 kHz,\nprepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read\naudiobooks from the LibriVox project, and has been carefully segmented and aligned.87\n\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .flac format and is not converted to a float32 array. To convert, the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```","paperswithcode_id":"librispeech-1","key":""},{"id":"librispeech_lm","tags":[],"citation":"@inproceedings{panayotov2015librispeech,\n  title={Librispeech: an ASR corpus based on public domain audio books},\n  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},\n  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},\n  pages={5206--5210},\n  year={2015},\n  organization={IEEE}\n}","description":"Language modeling resources to be used in conjunction with the LibriSpeech ASR corpus.","key":""},{"id":"limit","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|net-activities-captions","source_datasets:original","task_categories:structure-prediction","task_categories:text-classification","task_ids:multi-class-classification","task_ids:named-entity-recognition"],"citation":"@inproceedings{manotas-etal-2020-limit,\n    title = \"{L}i{M}i{T}: The Literal Motion in Text Dataset\",\n    author = \"Manotas, Irene  and\n      Vo, Ngoc Phuoc An  and\n      Sheinin, Vadim\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.findings-emnlp.88\",\n    doi = \"10.18653/v1/2020.findings-emnlp.88\",\n    pages = \"991--1000\",\n    abstract = \"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) dataset, a large human-annotated collection of English text sentences describing physical occurrence of motion, with annotated physical entities in motion. We describe the annotation process for the dataset, analyze its scale and diversity, and report results of several baseline models. We also present future research directions and applications of the LiMiT dataset and share it publicly as a new resource for the research community.\",\n}","description":"Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. Literal-Motion-in-Text (LiMiT) dataset, is a large human-annotated collection of English text sentences describing physical occurrence of motion, with annotated physical entities in motion.","paperswithcode_id":"limit","key":""},{"id":"lince","tags":[],"citation":"@inproceedings{aguilar-etal-2020-lince,\n    title = \"{L}in{CE}: A Centralized Benchmark for Linguistic Code-switching Evaluation\",\n    author = \"Aguilar, Gustavo  and\n      Kar, Sudipta  and\n      Solorio, Thamar\",\n    booktitle = \"Proceedings of The 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.223\",\n    pages = \"1803--1813\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}\n\nNote that each LinCE dataset has its own citation. Please see the source to see\nthe correct citation for each contained dataset.","description":"LinCE is a centralized Linguistic Code-switching Evaluation benchmark\n(https://ritual.uh.edu/lince/) that contains data for training and evaluating\nNLP systems on code-switching tasks.","paperswithcode_id":"lince","key":""},{"id":"linnaeus","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@article{gerner2010linnaeus,\n         title={LINNAEUS: a species name identification system for biomedical literature},\n         author={Gerner, Martin and Nenadic, Goran and Bergman, Casey M},\n         journal={BMC bioinformatics},\n         volume={11},\n         number={1},\n         pages={85},\n         year={2010},\n         publisher={Springer}\n}","description":"A novel corpus of full-text documents manually annotated for species mentions.","paperswithcode_id":"linnaeus","key":""},{"id":"liveqa","tags":["annotations_creators:found","language_creators:found","languages:zh","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@inproceedings{qianying-etal-2020-liveqa,\n    title = \"{L}ive{QA}: A Question Answering Dataset over Sports Live\",\n    author = \"Qianying, Liu  and\n      Sicong, Jiang  and\n      Yizhong, Wang  and\n      Sujian, Li\",\n    booktitle = \"Proceedings of the 19th Chinese National Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2020\",\n    address = \"Haikou, China\",\n    publisher = \"Chinese Information Processing Society of China\",\n    url = \"https://www.aclweb.org/anthology/2020.ccl-1.98\",\n    pages = \"1057--1067\"\n}","description":"This is LiveQA, a Chinese dataset constructed from play-by-play live broadcast.\nIt contains 117k multiple-choice questions written by human commentators for over 1,670 NBA games,\nwhich are collected from the Chinese Hupu website.","paperswithcode_id":"liveqa","key":""},{"id":"lj_speech","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:other-public-domain","multilinguality:monolingual","pretty_name:LJ Speech","size_categories:10K<n<100K","source_datasets:original","task_categories:speech-processing","task_ids:automatic-speech-recognition"],"citation":"@misc{ljspeech17,\n  author       = {Keith Ito and Linda Johnson},\n  title        = {The LJ Speech Dataset},\n  howpublished = {\\\\url{https://keithito.com/LJ-Speech-Dataset/}},\n  year         = 2017\n}","description":"This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading\npassages from 7 non-fiction books in English. A transcription is provided for each clip. Clips vary in length\nfrom 1 to 10 seconds and have a total length of approximately 24 hours.\n\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .wav format and is not converted to a float32 array. To convert the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```","paperswithcode_id":"ljspeech","key":""},{"id":"lm1b","tags":[],"citation":"@article{DBLP:journals/corr/ChelbaMSGBK13,\n  author    = {Ciprian Chelba and\n               Tomas Mikolov and\n               Mike Schuster and\n               Qi Ge and\n               Thorsten Brants and\n               Phillipp Koehn},\n  title     = {One Billion Word Benchmark for Measuring Progress in Statistical Language\n               Modeling},\n  journal   = {CoRR},\n  volume    = {abs/1312.3005},\n  year      = {2013},\n  url       = {http://arxiv.org/abs/1312.3005},\n  archivePrefix = {arXiv},\n  eprint    = {1312.3005},\n  timestamp = {Mon, 13 Aug 2018 16:46:16 +0200},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/ChelbaMSGBK13},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"A benchmark corpus to be used for measuring progress in statistical language modeling. This has almost one billion words in the training data.","paperswithcode_id":"billion-word-benchmark","key":""},{"id":"lst20","tags":["annotations_creators:expert-generated","language_creators:found","languages:th","licenses:other-aiforthai","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition","task_ids:part-of-speech-tagging","task_ids:structure-prediction-other-clause-segmentation","task_ids:structure-prediction-other-sentence-segmentation","task_ids:structure-prediction-other-word-segmentation"],"citation":"@article{boonkwan2020annotation,\n  title={The Annotation Guideline of LST20 Corpus},\n  author={Boonkwan, Prachya and Luantangsrisuk, Vorapon and Phaholphinyo, Sitthaa and Kriengket, Kanyanat and Leenoi, Dhanon and Phrombut, Charun and Boriboon, Monthika and Kosawat, Krit and Supnithi, Thepchai},\n  journal={arXiv preprint arXiv:2008.05055},\n  year={2020}\n}","description":"LST20 Corpus is a dataset for Thai language processing developed by National Electronics and Computer Technology Center (NECTEC), Thailand.\nIt offers five layers of linguistic annotation: word boundaries, POS tagging, named entities, clause boundaries, and sentence boundaries.\nAt a large scale, it consists of 3,164,002 words, 288,020 named entities, 248,181 clauses, and 74,180 sentences, while it is annotated with\n16 distinct POS tags. All 3,745 documents are also annotated with one of 15 news genres. Regarding its sheer size, this dataset is\nconsidered large enough for developing joint neural models for NLP.\nManually download at https://aiforthai.in.th/corpus.php","key":""},{"id":"m_lama","tags":["annotations_creators:crowdsourced","annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:crowdsourced","language_creators:expert-generated","language_creators:machine-generated","languages:af","languages:ar","languages:az","languages:be","languages:bg","languages:bn","languages:ca","languages:ceb","languages:cs","languages:cy","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:eu","languages:fa","languages:fi","languages:fr","languages:ga","languages:gl","languages:he","languages:hi","languages:hr","languages:hu","languages:hy","languages:id","languages:it","languages:ja","languages:ka","languages:ko","languages:la","languages:lt","languages:lv","languages:ms","languages:nl","languages:pl","languages:pt","languages:ro","languages:ru","languages:sk","languages:sl","languages:sq","languages:sr","languages:sv","languages:ta","languages:th","languages:tr","languages:uk","languages:ur","languages:vi","languages:zh","licenses:cc-by-nc-sa-4.0","multilinguality:translation","size_categories:100K<n<1M","source_datasets:extended|lama","task_categories:question-answering","task_categories:text-scoring","task_ids:open-domain-qa","task_ids:text-scoring-other-probing"],"citation":"@article{kassner2021multilingual,\n  author    = {Nora Kassner and\n               Philipp Dufter and\n               Hinrich Sch{\\\"{u}}tze},\n  title     = {Multilingual {LAMA:} Investigating Knowledge in Multilingual Pretrained\n               Language Models},\n  journal   = {CoRR},\n  volume    = {abs/2102.00894},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2102.00894},\n  archivePrefix = {arXiv},\n  eprint    = {2102.00894},\n  timestamp = {Tue, 09 Feb 2021 13:35:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-00894.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  note      = {to appear in EACL2021}\n}","description":"mLAMA: a multilingual version of the LAMA benchmark (T-REx and GoogleRE) covering 53 languages.","key":""},{"id":"mac_morpho","tags":["annotations_creators:expert-generated","language_creators:found","languages:pt","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:part-of-speech-tagging"],"citation":"@article{fonseca2015evaluating,\n  title={Evaluating word embeddings and a revised corpus for part-of-speech tagging in Portuguese},\n  author={Fonseca, Erick R and Rosa, Joao Luis G and Aluisio, Sandra Maria},\n  journal={Journal of the Brazilian Computer Society},\n  volume={21},\n  number={1},\n  pages={2},\n  year={2015},\n  publisher={Springer}\n}","description":"Mac-Morpho is a corpus of Brazilian Portuguese texts annotated with part-of-speech tags.\nIts first version was released in 2003 [1], and since then, two revisions have been made in order\nto improve the quality of the resource [2, 3].\nThe corpus is available for download split into train, development and test sections.\nThese are 76%, 4% and 20% of the corpus total, respectively (the reason for the unusual numbers\nis that the corpus was first split into 80%/20% train/test, and then 5% of the train section was\nset aside for development). This split was used in [3], and new POS tagging research with Mac-Morpho\nis encouraged to follow it in order to make consistent comparisons possible.\n\n\n[1] Aluísio, S., Pelizzoni, J., Marchi, A.R., de Oliveira, L., Manenti, R., Marquiafável, V. 2003.\nAn account of the challenge of tagging a reference corpus for brazilian portuguese.\nIn: Proceedings of the 6th International Conference on Computational Processing of the Portuguese Language. PROPOR 2003\n\n[2] Fonseca, E.R., Rosa, J.L.G. 2013. Mac-morpho revisited: Towards robust part-of-speech.\nIn: Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology – STIL\n\n[3] Fonseca, E.R., Aluísio, Sandra Maria, Rosa, J.L.G. 2015.\nEvaluating word embeddings and a revised corpus for part-of-speech tagging in Portuguese.\nJournal of the Brazilian Computer Society.","key":""},{"id":"makhzan","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:ur","licenses:other-my-license","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@InProceedings{huggingface:dataset,\ntitle = {A great new dataset},\nauthors={huggingface, Inc.\n},\nyear={2020}\n}","description":"An Urdu text corpus for machine learning, natural language processing and linguistic analysis.","key":""},{"id":"masakhaner","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:am","languages:ha","languages:ig","languages:rw","languages:lg","languages:luo","languages:pcm","languages:sw","languages:wo","languages:yo","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@article{Adelani2021MasakhaNERNE,\n  title={MasakhaNER: Named Entity Recognition for African Languages},\n  author={D. Adelani and Jade Abbott and Graham Neubig and Daniel D'Souza and Julia Kreutzer and Constantine Lignos\n  and Chester Palen-Michel and Happy Buzaaba and Shruti Rijhwani and Sebastian Ruder and Stephen Mayhew and\n  Israel Abebe Azime and S. Muhammad and Chris C. Emezue and Joyce Nakatumba-Nabende and Perez Ogayo and\n  Anuoluwapo Aremu and Catherine Gitau and Derguene Mbaye and J. Alabi and Seid Muhie Yimam and Tajuddeen R. Gwadabe and\n  Ignatius Ezeani and Rubungo Andre Niyongabo and Jonathan Mukiibi and V. Otiende and Iroro Orife and Davis David and\n  Samba Ngom and Tosin P. Adewumi and Paul Rayson and Mofetoluwa Adeyemi and Gerald Muriuki and Emmanuel Anebi and\n  C. Chukwuneke and N. Odu and Eric Peter Wairagala and S. Oyerinde and Clemencia Siro and Tobius Saul Bateesa and\n  Temilola Oloyede and Yvonne Wambui and Victor Akinode and Deborah Nabagereka and Maurice Katusiime and\n  Ayodele Awokoya and Mouhamadane Mboup and D. Gebreyohannes and Henok Tilaye and Kelechi Nwaike and Degaga Wolde and\n   Abdoulaye Faye and Blessing Sibanda and Orevaoghene Ahia and Bonaventure F. P. Dossou and Kelechi Ogueji and\n   Thierno Ibrahima Diop and A. Diallo and Adewale Akinfaderin and T. Marengereke and Salomey Osei},\n  journal={ArXiv},\n  year={2021},\n  volume={abs/2103.11811}\n}","description":"MasakhaNER is the first large publicly available high-quality dataset for named entity recognition (NER) in ten African languages.\n\nNamed entities are phrases that contain the names of persons, organizations, locations, times and quantities.\n\nExample:\n[PER Wolff] , currently a journalist in [LOC Argentina] , played with [PER Del Bosque] in the final years of the seventies in [ORG Real Madrid] .\nMasakhaNER is a named entity dataset consisting of PER, ORG, LOC, and DATE entities annotated by Masakhane for ten African languages:\n- Amharic\n- Hausa\n- Igbo\n- Kinyarwanda\n- Luganda\n- Luo\n- Nigerian-Pidgin\n- Swahili\n- Wolof\n- Yoruba\n\nThe train/validation/test sets are available for all the ten languages.\n\nFor more details see https://arxiv.org/abs/2103.11811","key":""},{"id":"math_dataset","tags":["languages:en"],"citation":"@article{2019arXiv,\n  author = {Saxton, Grefenstette, Hill, Kohli},\n  title = {Analysing Mathematical Reasoning Abilities of Neural Models},\n  year = {2019},\n  journal = {arXiv:1904.01557}\n}","description":"Mathematics database.\n\nThis dataset code generates mathematical question and answer pairs,\nfrom a range of question types at roughly school-level difficulty.\nThis is designed to test the mathematical learning and algebraic\nreasoning skills of learning models.\n\nOriginal paper: Analysing Mathematical Reasoning Abilities of Neural Models\n(Saxton, Grefenstette, Hill, Kohli).\n\nExample usage:\ntrain_examples, val_examples = datasets.load_dataset(\n    'math_dataset/arithmetic__mul',\n    split=['train', 'test'],\n    as_supervised=True)","paperswithcode_id":"mathematics","key":""},{"id":"math_qa","tags":["languages:en"],"citation":"","description":"Our dataset is gathered by using a new representation language to annotate over the AQuA-RAT dataset. AQuA-RAT has provided the questions, options, rationale, and the correct options.","paperswithcode_id":"mathqa","key":""},{"id":"matinf","tags":[],"citation":"@inproceedings{xu-etal-2020-matinf,\n    title = \"{MATINF}: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization\",\n    author = \"Xu, Canwen  and\n      Pei, Jiaxin  and\n      Wu, Hongtao  and\n      Liu, Yiyu  and\n      Li, Chenliang\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.330\",\n    pages = \"3586--3596\",\n}","description":"MATINF is the first jointly labeled large-scale dataset for classification, question answering and summarization.\n MATINF contains 1.07 million question-answer pairs with human-labeled categories and user-generated question\n descriptions. Based on such rich information, MATINF is applicable for three major NLP tasks, including classification,\n question answering, and summarization. We benchmark existing methods and a novel multi-task baseline over MATINF to\n inspire further research. Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the\n merits held by MATINF.","paperswithcode_id":"matinf","key":""},{"id":"mc4","tags":["pretty_name:mC4","annotations_creators:no-annotation","language_creators:found","languages:af","languages:am","languages:ar","languages:az","languages:be","languages:bg","languages:bg-Latn","languages:bn","languages:ca","languages:ceb","languages:co","languages:cs","languages:cy","languages:da","languages:de","languages:el","languages:el-Latn","languages:en","languages:eo","languages:es","languages:et","languages:eu","languages:fa","languages:fi","languages:fil","languages:fr","languages:fy","languages:ga","languages:gd","languages:gl","languages:gu","languages:ha","languages:haw","languages:hi","languages:hi-Latn","languages:hmn","languages:ht","languages:hu","languages:hy","languages:id","languages:ig","languages:is","languages:it","languages:iw","languages:ja","languages:ja-Latn","languages:jv","languages:ka","languages:kk","languages:km","languages:kn","languages:ko","languages:ku","languages:ky","languages:la","languages:lb","languages:lo","languages:lt","languages:lv","languages:mg","languages:mi","languages:mk","languages:ml","languages:mn","languages:mr","languages:ms","languages:mt","languages:my","languages:ne","languages:nl","languages:no","languages:ny","languages:pa","languages:pl","languages:ps","languages:pt","languages:ro","languages:ru","languages:ru-Latn","languages:sd","languages:si","languages:sk","languages:sl","languages:sm","languages:sn","languages:so","languages:sq","languages:sr","languages:st","languages:su","languages:sv","languages:sw","languages:ta","languages:te","languages:tg","languages:th","languages:tr","languages:uk","languages:und","languages:ur","languages:uz","languages:vi","languages:xh","languages:yi","languages:yo","languages:zh","languages:zh-Latn","languages:zu","licenses:odc-by-1.0","multilinguality:multilingual","size_categories:n<1K","size_categories:1K<n<10K","size_categories:10K<n<100K","size_categories:100K<n<1M","size_categories:1M<n<10M","size_categories:10M<n<100M","size_categories:100M<n<1B","size_categories:1B<n<10B","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@article{2019t5,\n    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n    journal = {arXiv e-prints},\n    year = {2019},\n    archivePrefix = {arXiv},\n    eprint = {1910.10683},\n}","description":"A colossal, cleaned version of Common Crawl's web crawl corpus.\n\nBased on Common Crawl dataset: \"https://commoncrawl.org\".\n\nThis is the processed version of Google's mC4 dataset by AllenAI.","paperswithcode_id":"mc4","key":""},{"id":"mc_taco","tags":["annotations_creators:crowdsourced","annotations_creators:machine-generated","language_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@inproceedings{ZKNR19,\n    author = {Ben Zhou, Daniel Khashabi, Qiang Ning and Dan Roth},\n    title = {“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding },\n    booktitle = {EMNLP},\n    year = {2019},\n}","description":"MC-TACO (Multiple Choice TemporAl COmmonsense) is a dataset of 13k question-answer\npairs that require temporal commonsense comprehension. A system receives a sentence\nproviding context information, a question designed to require temporal commonsense\nknowledge, and multiple candidate answers. More than one candidate answer can be plausible.\n\nThe task is framed as binary classification: givent he context, the question,\nand the candidate answer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\").","paperswithcode_id":"mc-taco","key":""},{"id":"md_gender_bias","tags":["annotations_creators:machine-generated","annotations_creators:found","annotations_creators:crowdsourced","language_creators:found","language_creators:crowdsourced","languages:en","licenses:mit","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:10K<n<100K","size_categories:n<1K","size_categories:1M<n<10M","size_categories:1K<n<10K","source_datasets:extended|other-convai2","source_datasets:original","source_datasets:extended|other-light","source_datasets:extended|other-opensubtitles","source_datasets:extended|other-yelp","task_categories:text-classification","task_ids:text-classification-other-gender-bias"],"citation":"@inproceedings{md_gender_bias,\n  author    = {Emily Dinan and\n               Angela Fan and\n               Ledell Wu and\n               Jason Weston and\n               Douwe Kiela and\n               Adina Williams},\n  editor    = {Bonnie Webber and\n               Trevor Cohn and\n               Yulan He and\n               Yang Liu},\n  title     = {Multi-Dimensional Gender Bias Classification},\n  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural\n               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},\n  pages     = {314--331},\n  publisher = {Association for Computational Linguistics},\n  year      = {2020},\n  url       = {https://www.aclweb.org/anthology/2020.emnlp-main.23/}\n}","description":"Machine learning models are trained to find patterns in data.\nNLP models can inadvertently learn socially undesirable patterns when training on gender biased text.\nIn this work, we propose a general framework that decomposes gender bias in text along several pragmatic and semantic dimensions:\nbias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker.\nUsing this fine-grained framework, we automatically annotate eight large scale datasets with gender information.\nIn addition, we collect a novel, crowdsourced evaluation benchmark of utterance-level gender rewrites.\nDistinguishing between gender bias along multiple dimensions is important, as it enables us to train finer-grained gender bias classifiers.\nWe show our classifiers prove valuable for a variety of important applications, such as controlling for gender bias in generative models,\ndetecting gender bias in arbitrary text, and shed light on offensive language in terms of genderedness.","paperswithcode_id":"md-gender","key":""},{"id":"mdd","tags":["annotations_creators:no-annotation","language_creators:found","languages:en","licenses:cc-by-3.0","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:1M<n<10M","source_datasets:original","task_categories:sequence-modeling","task_ids:dialogue-modeling","pretty_name:Movie Dialog dataset (MDD)"],"citation":"@misc{dodge2016evaluating,\n      title={Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems},\n      author={Jesse Dodge and Andreea Gane and Xiang Zhang and Antoine Bordes and Sumit Chopra and Alexander Miller and Arthur Szlam and Jason Weston},\n      year={2016},\n      eprint={1511.06931},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"The Movie Dialog dataset (MDD) is designed to measure how well\nmodels can perform at goal and non-goal orientated dialog\ncentered around the topic of movies (question answering,\nrecommendation and discussion).","paperswithcode_id":"mdd","key":""},{"id":"med_hop","tags":["annotations_creators:crowdsourced","language_creators:expert-generated","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa","task_ids:question-answering-other-multi-hop"],"citation":"@misc{welbl2018constructing,\n      title={Constructing Datasets for Multi-hop Reading Comprehension Across Documents},\n      author={Johannes Welbl and Pontus Stenetorp and Sebastian Riedel},\n      year={2018},\n      eprint={1710.06481},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"MedHop is based on research paper abstracts from PubMed, and the queries are about interactions between pairs of drugs. The correct answer has to be inferred by combining information from a chain of reactions of drugs and proteins.","paperswithcode_id":"medhop","key":""},{"id":"medal","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10M<n<100M","source_datasets:original","task_categories:other","task_ids:other-other-disambiguation"],"citation":"@inproceedings{wen-etal-2020-medal,\n    title = \"{M}e{DAL}: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining\",\n    author = \"Wen, Zhi  and\n      Lu, Xing Han  and\n      Reddy, Siva\",\n    booktitle = \"Proceedings of the 3rd Clinical Natural Language Processing Workshop\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.clinicalnlp-1.15\",\n    pages = \"130--135\",\n    abstract = \"One of the biggest challenges that prohibit the use of many current NLP methods in clinical settings is the availability of public datasets. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. We pre-trained several models of common architectures on this dataset and empirically showed that such pre-training leads to improved performance and convergence speed when fine-tuning on downstream medical tasks.\",\n}","description":"A large medical text dataset (14Go) curated to 4Go for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. For example, DHF can be disambiguated to dihydrofolate, diastolic heart failure, dengue hemorragic fever or dihydroxyfumarate","paperswithcode_id":"medal","key":""},{"id":"medical_dialog","tags":["annotations_creators:found","language_creators:expert-generated","language_creators:found","languages:en","languages:zh","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:question-answering","task_ids:closed-domain-qa"],"citation":"@article{chen2020meddiag,\n  title={MedDialog: a large-scale medical dialogue dataset},\n  author={Chen, Shu and Ju, Zeqian and Dong, Xiangyu and Fang, Hongchao and Wang, Sicheng and Yang, Yue and Zeng, Jiaqi and Zhang, Ruisi and Zhang, Ruoyu and Zhou, Meng and Zhu, Penghui and Xie, Pengtao},\n  journal={arXiv preprint arXiv:2004.03329},\n  year={2020}\n}","description":"The MedDialog dataset (English) contains conversations (in English) between doctors and patients.It has 0.26 million dialogues. The data is continuously growing and more dialogues will be added. The raw dialogues are from healthcaremagic.com and icliniq.com.\nAll copyrights of the data belong to healthcaremagic.com and icliniq.com.","key":""},{"id":"medical_questions_pairs","tags":["annotations_creators:expert-generated","language_creators:other","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:semantic-similarity-classification"],"citation":"@misc{mccreery2020effective,\n      title={Effective Transfer Learning for Identifying Similar Questions: Matching User Questions to COVID-19 FAQs},\n      author={Clara H. McCreery and Namit Katariya and Anitha Kannan and Manish Chablani and Xavier Amatriain},\n      year={2020},\n      eprint={2008.13546},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}","description":"This dataset consists of 3048 similar and dissimilar medical question pairs hand-generated and labeled by Curai's doctors.","key":""},{"id":"menyo20k_mt","tags":["annotations_creators:expert-generated","annotations_creators:found","language_creators:found","languages:en","languages:yo","licenses:cc-by-4.0","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@dataset{david_ifeoluwa_adelani_2020_4297448,\n  author       = {David Ifeoluwa Adelani and\n                  Jesujoba O. Alabi and\n                  Damilola Adebonojo and\n                  Adesina Ayeni and\n                  Mofe Adeyemi and\n                  Ayodele Awokoya},\n  title        = {MENYO-20k: A Multi-domain English - Yorùbá Corpus\n                  for Machine Translation},\n  month        = nov,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {1.0},\n  doi          = {10.5281/zenodo.4297448},\n  url          = {https://doi.org/10.5281/zenodo.4297448}\n}","description":"MENYO-20k is a multi-domain parallel dataset with texts obtained from news articles, ted talks, movie transcripts, radio transcripts, science and technology texts, and other short articles curated from the web and professional translators. The dataset has 20,100 parallel sentences split into 10,070 training sentences, 3,397 development sentences, and 6,633 test sentences (3,419 multi-domain, 1,714 news domain, and 1,500 ted talks speech transcript domain). The development and test sets are available upon request.","key":""},{"id":"meta_woz","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:Microsoft Research Data License Agreement","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:dialogue-modeling"],"citation":"@InProceedings{shalyminov2020fast,\nauthor = {Shalyminov, Igor and Sordoni, Alessandro and Atkinson, Adam and Schulz, Hannes},\ntitle = {Fast Domain Adaptation For Goal-Oriented Dialogue Using A Hybrid Generative-Retrieval Transformer},\nbooktitle = {2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\nyear = {2020},\nmonth = {April},\nurl = {https://www.microsoft.com/en-us/research/publication/fast-domain-adaptation-for-goal-oriented-dialogue-using-a\n-hybrid-generative-retrieval-transformer/},\n}","description":"MetaLWOz: A Dataset of Multi-Domain Dialogues for the Fast Adaptation of Conversation Models. We introduce the Meta-Learning Wizard of Oz (MetaLWOz) dialogue dataset for developing fast adaptation methods for conversation models. This data can be used to train task-oriented dialogue models, specifically to develop methods to quickly simulate user responses with a small amount of data. Such fast-adaptation models fall into the research areas of transfer learning and meta learning. The dataset consists of 37,884 crowdsourced dialogues recorded between two human users in a Wizard of Oz setup, in which one was instructed to behave like a bot, and the other a true human user. The users are assigned a task belonging to a particular domain, for example booking a reservation at a particular restaurant, and work together to complete the task. Our dataset spans 47 domains having 227 tasks total. Dialogues are a minimum of 10 turns long.","paperswithcode_id":"metalwoz","key":""},{"id":"metooma","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_categories:text-retrieval","task_ids:multi-class-classification","task_ids:multi-label-classification"],"citation":"@inproceedings{gautam2020metooma,\n    title={# MeTooMA: Multi-Aspect Annotations of Tweets Related to the MeToo Movement},\n    author={Gautam, Akash and Mathur, Puneet and Gosangi, Rakesh and Mahata, Debanjan and Sawhney, Ramit and Shah, Rajiv Ratn},\n    booktitle={Proceedings of the International AAAI Conference on Web and Social Media},\n    volume={14},\n    pages={209--216},\n    year={2020} }","description":"The dataset consists of tweets belonging to #MeToo movement on Twitter, labelled into different categories.\nDue to Twitter's development policies, we only provide the tweet ID's and corresponding labels,\nother data can be fetched via Twitter API.\nThe data has been labelled by experts, with the majority taken into the account for deciding the final label.\nWe provide these labels for each of the tweets. The labels provided for each data point\nincludes -- Relevance, Directed Hate, Generalized Hate,\nSarcasm, Allegation, Justification, Refutation, Support, Oppose","paperswithcode_id":"metooma","key":""},{"id":"metrec","tags":["annotations_creators:no-annotation","language_creators:found","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-poetry-classification"],"citation":"@article{metrec2020,\n  title={MetRec: A dataset for meter classification of arabic poetry},\n  author={Al-shaibani, Maged S and Alyafeai, Zaid and Ahmad, Irfan},\n  journal={Data in Brief},\n  year={2020},\n  publisher={Elsevier}\n}","description":"Arabic Poetry Metric Classification.\nThe dataset contains the verses and their corresponding meter classes.Meter classes are represented as numbers from 0 to 13. The dataset can be highly useful for further research in order to improve the field of Arabic poems’ meter classification.The train dataset contains 47,124 records and the test dataset contains 8316 records.","paperswithcode_id":"metrec","key":""},{"id":"miam","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:es","languages:it","languages:fr","languages:en","languages:de","licenses:cc-by-sa-4.0","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_categories:text-classification","task_ids:dialogue-modeling","task_ids:language-modeling","task_ids:text-classification-other-dialogue-act-classification"],"citation":"@unpublished{\nanonymous2021cross-lingual,\ntitle={Cross-Lingual Pretraining Methods for Spoken Dialog},\nauthor={Anonymous},\njournal={OpenReview Preprint},\nyear={2021},\nurl{https://openreview.net/forum?id=c1oDhu_hagR},\nnote={anonymous preprint under review}\n}","description":"Multilingual dIalogAct benchMark is a collection of resources for training, evaluating, and\nanalyzing natural language understanding systems specifically designed for spoken language. Datasets\nare in English, French, German, Italian and Spanish. They cover a variety of domains including\nspontaneous speech, scripted scenarios, and joint task completion. Some datasets additionally include\nemotion and/or sentimant labels.","key":""},{"id":"mkb","tags":["task_categories:sequence-modeling","multilinguality:translation","task_ids:language-modeling","languages:hi","languages:te","languages:ta","languages:ml","languages:gu","languages:ur","languages:bn","languages:or","languages:mr","languages:pa","languages:en","annotations_creators:no-annotation","source_datasets:original","size_categories:1K<n<10K","size_categories:n<1K","licenses:cc-by-4.0"],"citation":"@misc{siripragada2020multilingual,\n      title={A Multilingual Parallel Corpora Collection Effort for Indian Languages},\n      author={Shashank Siripragada and Jerin Philip and Vinay P. Namboodiri and C V Jawahar},\n      year={2020},\n      eprint={2007.07691},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"The Prime Minister's speeches - Mann Ki Baat, on All India Radio, translated into many languages.","key":""},{"id":"mkqa","tags":["annotations_creators:crowdsourced","language_creators:found","languages:ar","languages:da","languages:de","languages:en","languages:es","languages:fi","languages:fr","languages:he","languages:hu","languages:it","languages:ja","languages:ko","languages:km","languages:ms","languages:nl","languages:no","languages:pl","languages:pt","languages:ru","languages:sv","languages:th","languages:tr","languages:vi","languages:zh","licenses:cc-by-3.0","multilinguality:multilingual","multilinguality:translation","size_categories:10K<n<100K","source_datasets:extended|natural_questions","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@misc{mkqa,\n    title = {MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering},\n    author = {Shayne Longpre and Yi Lu and Joachim Daiber},\n    year = {2020},\n    URL = {https://arxiv.org/pdf/2007.15207.pdf}\n}","description":"We introduce MKQA, an open-domain question answering evaluation set comprising 10k question-answer pairs sampled from the Google Natural Questions dataset, aligned across 26 typologically diverse languages (260k question-answer pairs in total). For each query we collected new passage-independent answers. These queries and answers were then human translated into 25 Non-English languages.","paperswithcode_id":"mkqa","key":""},{"id":"mlqa","tags":["languages:en"],"citation":"@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}","description":"    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between\n    4 different languages on average.","paperswithcode_id":"mlqa","key":""},{"id":"mlsum","tags":["annotations_creators:found","language_creators:found","languages:de","languages:es","languages:fr","languages:ru","languages:tr","licenses:other-research-only","multilinguality:multilingual","size_categories:100K<n<1M","size_categories:10K<n<100K","source_datasets:extended|cnn_dailymail","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-classification","task_ids:machine-translation","task_ids:multi-class-classification","task_ids:multi-label-classification","task_ids:summarization","task_ids:topic-classification"],"citation":"@article{scialom2020mlsum,\n  title={MLSUM: The Multilingual Summarization Corpus},\n  author={Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},\n  journal={arXiv preprint arXiv:2004.14900},\n  year={2020}\n}","description":"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset.\nObtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish.\nTogether with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community.\nWe report cross-lingual comparative analyses based on state-of-the-art systems.\nThese highlight existing biases which motivate the use of a multi-lingual dataset.","paperswithcode_id":"mlsum","key":""},{"id":"mnist","tags":["annotations_creators:experts","language_creators:found","licenses:MIT","size_categories:10K<n<100K","source_datasets:extended|other-nist","task_categories:other","task_ids:other-other-image-classification"],"citation":"@article{lecun2010mnist,\n  title={MNIST handwritten digit database},\n  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n  volume={2},\n  year={2010}\n}","description":"The MNIST dataset consists of 70,000 28x28 black-and-white images in 10 classes (one for each digits), with 7,000\nimages per class. There are 60,000 training images and 10,000 test images.","paperswithcode_id":"mnist","key":""},{"id":"mocha","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:question-answering-other-generative-reading-comprehension-metric"],"citation":"@inproceedings{Chen2020MOCHAAD,\n    author={Anthony Chen and Gabriel Stanovsky and Sameer Singh and Matt Gardner},\n    title={MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics},\n    booktitle={EMNLP},\n    year={2020}\n}","description":"Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train an evaluation metric: LERC, a Learned Evaluation metric for Reading Comprehension, to mimic human judgement scores.","paperswithcode_id":"mocha","key":""},{"id":"moroco","tags":["annotations_creators:found","language_creators:found","languages:ro","languages:ro-md","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:topic-classification"],"citation":"@inproceedings{ Butnaru-ACL-2019,\n    author = {Andrei M. Butnaru and Radu Tudor Ionescu},\n    title = \"{MOROCO: The Moldavian and Romanian Dialectal Corpus}\",\n    booktitle = {Proceedings of ACL},\n    year = {2019},\n    pages={688--698},\n}","description":"The MOROCO (Moldavian and Romanian Dialectal Corpus) dataset contains 33564 samples of text collected from the news domain.\nThe samples belong to one of the following six topics:\n    - culture\n    - finance\n    - politics\n    - science\n    - sports\n    - tech","paperswithcode_id":"moroco","key":""},{"id":"movie_rationales","tags":["languages:en"],"citation":"@unpublished{eraser2019,\n    title = {ERASER: A Benchmark to Evaluate Rationalized NLP Models},\n    author = {Jay DeYoung and Sarthak Jain and Nazneen Fatema Rajani and Eric Lehman and Caiming Xiong and Richard Socher and Byron C. Wallace}\n}\n@InProceedings{zaidan-eisner-piatko-2008:nips,\n  author    =  {Omar F. Zaidan  and  Jason Eisner  and  Christine Piatko},\n  title     =  {Machine Learning with Annotator Rationales to Reduce Annotation Cost},\n  booktitle =  {Proceedings of the NIPS*2008 Workshop on Cost Sensitive Learning},\n  month     =  {December},\n  year      =  {2008}\n}","description":"The movie rationale dataset contains human annotated rationales for movie\nreviews.","key":""},{"id":"mrqa","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:extended|drop","source_datasets:extended|hotpot_qa","source_datasets:extended|natural_questions","source_datasets:extended|race","source_datasets:extended|search_qa","source_datasets:extended|squad","source_datasets:extended|trivia_qa","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@inproceedings{fisch2019mrqa,\n    title={{MRQA} 2019 Shared Task: Evaluating Generalization in Reading Comprehension},\n    author={Adam Fisch and Alon Talmor and Robin Jia and Minjoon Seo and Eunsol Choi and Danqi Chen},\n    booktitle={Proceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Workshop at EMNLP},\n    year={2019},\n}","description":"The MRQA 2019 Shared Task focuses on generalization in question answering.\nAn effective question answering system should do more than merely\ninterpolate from the training set to answer test examples drawn\nfrom the same distribution: it should also be able to extrapolate\nto out-of-distribution examples — a significantly harder challenge.\n\nThe dataset is a collection of 18 existing QA dataset (carefully selected\nsubset of them) and converted to the same format (SQuAD format). Among\nthese 18 datasets, six datasets were made available for training,\nsix datasets were made available for development, and the final six\nfor testing. The dataset is released as part of the MRQA 2019 Shared Task.","paperswithcode_id":"mrqa-2019","key":""},{"id":"ms_marco","tags":["languages:en"],"citation":"@article{DBLP:journals/corr/NguyenRSGTMD16,\n  author    = {Tri Nguyen and\n               Mir Rosenberg and\n               Xia Song and\n               Jianfeng Gao and\n               Saurabh Tiwary and\n               Rangan Majumder and\n               Li Deng},\n  title     = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  journal   = {CoRR},\n  volume    = {abs/1611.09268},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1611.09268},\n  archivePrefix = {arXiv},\n  eprint    = {1611.09268},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n}","description":"Starting with a paper released at NIPS 2016, MS MARCO is a collection of datasets focused on deep learning in search.\n\nThe first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer.\nSince then we released a 1,000,000 question dataset, a natural langauge generation dataset, a passage ranking dataset,\nkeyphrase extraction dataset, crawling dataset, and a conversational search.\n\nThere have been 277 submissions. 20 KeyPhrase Extraction submissions, 87 passage ranking submissions, 0 document ranking\nsubmissions, 73 QnA V2 submissions, 82 NLGEN submisions, and 15 QnA V1 submissions\n\nThis data comes in three tasks/forms: Original QnA dataset(v1.1), Question Answering(v2.1), Natural Language Generation(v2.1).\n\nThe original question answering datset featured 100,000 examples and was released in 2016. Leaderboard is now closed but data is availible below.\n\nThe current competitive tasks are Question Answering and Natural Language Generation. Question Answering features over 1,000,000 queries and\nis much like the original QnA dataset but bigger and with higher quality. The Natural Language Generation dataset features 180,000 examples and\nbuilds upon the QnA dataset to deliver answers that could be spoken by a smart speaker.","paperswithcode_id":"ms-marco","key":""},{"id":"ms_terms","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:af","languages:sq","languages:am","languages:ar","languages:hy","languages:as","languages:az","languages:bn","languages:other-bn-india","languages:eu","languages:be","languages:bs","languages:other-bs-latin","languages:bg","languages:ca","languages:ku","languages:chr","languages:zh","languages:other-zh-Hant_HK","languages:other-zh-Hant_TW","languages:hr","languages:cs","languages:da","languages:prs","languages:nl","languages:en","languages:et","languages:fil","languages:fi","languages:fr","languages:other-fr_CA","languages:gl","languages:ka","languages:de","languages:el","languages:gu","languages:ha","languages:he","languages:hi","languages:hu","languages:is","languages:ig","languages:id","languages:iu","languages:ga","languages:xh","languages:zu","languages:it","languages:ja","languages:quc","languages:kn","languages:kk","languages:km","languages:rw","languages:swh","languages:knn","languages:ko","languages:ky","languages:lo","languages:lv","languages:lt","languages:lb","languages:mk","languages:other-ms-brunei (Brunei Darus.)","languages:ms","languages:ml","languages:mt","languages:mi","languages:mr","languages:mn","languages:ne","languages:nb","languages:nn","languages:ory","languages:pst","languages:fa","languages:pl","languages:other-pt-br","languages:pt","languages:pa","languages:qu","languages:ro","languages:ru","languages:gd","languages:other-sr-bih","languages:sr","languages:other-sr-latin","languages:st","languages:tn","languages:sd","languages:si","languages:sk","languages:sl","languages:es","languages:other-es-MX","languages:sv","languages:tg","languages:ta","languages:tt","languages:te","languages:th","languages:ti","languages:tr","languages:tk","languages:uk","languages:ur","languages:ug","languages:uz","languages:other-valencian","languages:vi","languages:guc","languages:cy","languages:wo","languages:yo","licenses:ms-pl","multilinguality:multilingual","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"description":"The Microsoft Terminology Collection can be used to develop localized versions of applications that integrate with Microsoft products.\nIt can also be used to integrate Microsoft terminology into other terminology collections or serve as a base IT glossary\nfor language development in the nearly 100 languages available. Terminology is provided in .tbx format, an industry standard for terminology exchange.","key":""},{"id":"msr_genomics_kbcomp","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:other-my-license","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:other","task_ids:other-other-NCI-PID-PubMed Genomics Knowledge Base Completion Dataset"],"citation":"@inproceedings{toutanova-etal-2016-compositional,\n    title = \"Compositional Learning of Embeddings for Relation Paths in Knowledge Base and Text\",\n    author = \"Toutanova, Kristina  and\n      Lin, Victoria  and\n      Yih, Wen-tau  and\n      Poon, Hoifung  and\n      Quirk, Chris\",\n    booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2016\",\n    address = \"Berlin, Germany\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P16-1136\",\n    doi = \"10.18653/v1/P16-1136\",\n    pages = \"1434--1444\",\n}","description":"The database is derived from the NCI PID Pathway Interaction Database, and the textual mentions are extracted from cooccurring pairs of genes in PubMed abstracts, processed and annotated by Literome (Poon et al. 2014). This dataset was used in the paper “Compositional Learning of Embeddings for Relation Paths in Knowledge Bases and Text” (Toutanova, Lin, Yih, Poon, and Quirk, 2016).","key":""},{"id":"msr_sqa","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:ms-pl","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@inproceedings{iyyer2017search,\n  title={Search-based neural structured learning for sequential question answering},\n  author={Iyyer, Mohit and Yih, Wen-tau and Chang, Ming-Wei},\n  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={1821--1831},\n  year={2017}\n}","description":"Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We created SQA by asking crowdsourced workers to decompose 2,022 questions from WikiTableQuestions (WTQ), which contains highly-compositional questions about tables from Wikipedia. We had three workers decompose each WTQ question, resulting in a dataset of 6,066 sequences that contain 17,553 questions in total. Each question is also associated with answers in the form of cell locations in the tables.","key":""},{"id":"msr_text_compression","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:other-Microsoft Research Data License","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-Open-American-National-Corpus-(OANC1)","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@inproceedings{Toutanova2016ADA,\n  title={A Dataset and Evaluation Metrics for Abstractive Compression of Sentences and Short Paragraphs},\n  author={Kristina Toutanova and Chris Brockett and Ke M. Tran and Saleema Amershi},\n  booktitle={EMNLP},\n  year={2016}\n}","description":"This dataset contains sentences and short paragraphs with corresponding shorter (compressed) versions. There are up to five compressions for each input text, together with quality judgements of their meaning preservation and grammaticality. The dataset is derived using source texts from the Open American National Corpus (ww.anc.org) and crowd-sourcing.","key":""},{"id":"msr_zhen_translation_parity","tags":["annotations_creators:no-annotation","language_creators:expert-generated","language_creators:machine-generated","languages:en","licenses:ms-pl","multilinguality:monolingual","multilinguality:translation","size_categories:1K<n<10K","source_datasets:extended|other-newstest2017","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@misc{hassan2018achieving,\n      title={Achieving Human Parity on Automatic Chinese to English News Translation},\n      author={ Hany Hassan and Anthony Aue and Chang Chen and Vishal Chowdhary and Jonathan Clark\n               and Christian Federmann and Xuedong Huang and Marcin Junczys-Dowmunt and William Lewis\n               and Mu Li and Shujie Liu and Tie-Yan Liu and Renqian Luo and Arul Menezes and Tao Qin\n               and Frank Seide and Xu Tan and Fei Tian and Lijun Wu and Shuangzhi Wu and Yingce Xia\n               and Dongdong Zhang and Zhirui Zhang and Ming Zhou},\n      year={2018},\n      eprint={1803.05567},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Translator Human Parity Data\n\nHuman evaluation results and translation output for the Translator Human Parity Data release,\nas described in https://blogs.microsoft.com/ai/machine-translation-news-test-set-human-parity/.\nThe Translator Human Parity Data release contains all human evaluation results and translations\nrelated to our paper \"Achieving Human Parity on Automatic Chinese to English News Translation\",\npublished on March 14, 2018.","key":""},{"id":"msra_ner","tags":["annotations_creators:crowdsourced","language_creators:found","languages:zh","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{levow2006third,\n  author    = {Gina{-}Anne Levow},\n  title     = {The Third International Chinese Language Processing Bakeoff: Word\n               Segmentation and Named Entity Recognition},\n  booktitle = {SIGHAN@COLING/ACL},\n  pages     = {108--117},\n  publisher = {Association for Computational Linguistics},\n  year      = {2006}\n}","description":"The Third International Chinese Language\nProcessing Bakeoff was held in Spring\n2006 to assess the state of the art in two\nimportant tasks: word segmentation and\nnamed entity recognition. Twenty-nine\ngroups submitted result sets in the two\ntasks across two tracks and a total of five\ncorpora. We found strong results in both\ntasks as well as continuing challenges.\n\nMSRA NER is one of the provided dataset.\nThere are three types of NE, PER (person),\nORG (organization) and LOC (location).\nThe dataset is in the BIO scheme.\n\nFor more details see https://faculty.washington.edu/levow/papers/sighan06.pdf","key":""},{"id":"mt_eng_vietnamese","tags":["annotations_creators:found","language_creators:found","multilinguality:multilingual","languages:en","languages:vi","licenses:unknown","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{Luong-Manning:iwslt15,\n        Address = {Da Nang, Vietnam}\n        Author = {Luong, Minh-Thang  and Manning, Christopher D.},\n        Booktitle = {International Workshop on Spoken Language Translation},\n        Title = {Stanford Neural Machine Translation Systems for Spoken Language Domain},\n        Year = {2015}}","description":"Preprocessed Dataset from IWSLT'15 English-Vietnamese machine translation: English-Vietnamese.","key":""},{"id":"muchocine","tags":["annotations_creators:no-annotation","language_creators:found","languages:es","licenses:cc-by-2.1","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"description":"The Muchocine reviews dataset contains 3,872 longform movie reviews in Spanish language,\neach with a shorter summary review, and a rating on a 1-5 scale.","key":""},{"id":"multi_booked","tags":["annotations_creators:expert-generated","language_creators:found","languages:ca","languages:eu","licenses:cc-by-3.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{Barnes2018multibooked,\n    author={Barnes, Jeremy and Lambert, Patrik and Badia, Toni},\n    title={MultiBooked: A corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification},\n    booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC'18)},\n    year = {2018},\n    month = {May},\n    date = {7-12},\n    address = {Miyazaki, Japan},\n    publisher = {European Language Resources Association (ELRA)},\n    language = {english}\n}","description":"MultiBooked is a corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification.\n\nThe corpora are compiled from hotel reviews taken mainly from booking.com. The corpora are in Kaf/Naf format, which is\nan xml-style stand-off format that allows for multiple layers of annotation. Each review was sentence- and\nword-tokenized and lemmatized using Freeling for Catalan and ixa-pipes for Basque. Finally, for each language two\nannotators annotated opinion holders, opinion targets, and opinion expressions for each review, following the\nguidelines set out in the OpeNER project.","paperswithcode_id":"multibooked","key":""},{"id":"multi_news","tags":["languages:en"],"citation":"@misc{alex2019multinews,\n    title={Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},\n    author={Alexander R. Fabbri and Irene Li and Tianwei She and Suyi Li and Dragomir R. Radev},\n    year={2019},\n    eprint={1906.01749},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"Multi-News, consists of news articles and human-written summaries\nof these articles from the site newser.com.\nEach summary is professionally written by editors and\nincludes links to the original articles cited.\n\nThere are two features:\n  - document: text of news articles seperated by special token \"|||||\".\n  - summary: news summary.","paperswithcode_id":"multi-news","key":""},{"id":"multi_nli","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-3.0","licenses:cc-by-sa-3.0-at","licenses:mit","licenses:other-Open Portion of the American National Corpus","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-scoring","task_ids:semantic-similarity-scoring"],"citation":"@InProceedings{N18-1101,\n  author = {Williams, Adina\n            and Nangia, Nikita\n            and Bowman, Samuel},\n  title = {A Broad-Coverage Challenge Corpus for\n           Sentence Understanding through Inference},\n  booktitle = {Proceedings of the 2018 Conference of\n               the North American Chapter of the\n               Association for Computational Linguistics:\n               Human Language Technologies, Volume 1 (Long\n               Papers)},\n  year = {2018},\n  publisher = {Association for Computational Linguistics},\n  pages = {1112--1122},\n  location = {New Orleans, Louisiana},\n  url = {http://aclweb.org/anthology/N18-1101}\n}","description":"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a\ncrowd-sourced collection of 433k sentence pairs annotated with textual\nentailment information. The corpus is modeled on the SNLI corpus, but differs in\nthat covers a range of genres of spoken and written text, and supports a\ndistinctive cross-genre generalization evaluation. The corpus served as the\nbasis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.","paperswithcode_id":"multinli","key":""},{"id":"multi_nli_mismatch","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-3.0","licenses:cc-by-sa-3.0-at","licenses:mit","licenses:other-Open Portion of the American National Corpus","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-scoring","task_ids:semantic-similarity-scoring"],"citation":"@InProceedings{N18-1101,\n  author = {Williams, Adina\n            and Nangia, Nikita\n            and Bowman, Samuel},\n  title = {A Broad-Coverage Challenge Corpus for\n           Sentence Understanding through Inference},\n  booktitle = {Proceedings of the 2018 Conference of\n               the North American Chapter of the\n               Association for Computational Linguistics:\n               Human Language Technologies, Volume 1 (Long\n               Papers)},\n  year = {2018},\n  publisher = {Association for Computational Linguistics},\n  pages = {1112--1122},\n  location = {New Orleans, Louisiana},\n  url = {http://aclweb.org/anthology/N18-1101}\n}","description":"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a\ncrowd-sourced collection of 433k sentence pairs annotated with textual\nentailment information. The corpus is modeled on the SNLI corpus, but differs in\nthat covers a range of genres of spoken and written text, and supports a\ndistinctive cross-genre generalization evaluation. The corpus served as the\nbasis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.","paperswithcode_id":"multinli","key":""},{"id":"multi_para_crawl","tags":["annotations_creators:found","language_creators:found","languages:bg","languages:ca","languages:cs","languages:da","languages:de","languages:el","languages:es","languages:et","languages:eu","languages:fi","languages:fr","languages:ga","languages:gl","languages:ha","languages:hr","languages:hu","languages:ig","languages:is","languages:it","languages:km","languages:lt","languages:lv","languages:mt","languages:my","languages:nb","languages:ne","languages:nl","languages:nn","languages:pl","languages:ps","languages:pt","languages:ro","languages:ru","languages:si","languages:sk","languages:sl","languages:so","languages:sv","languages:sw","languages:tl","licenses:cc0-1.0","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J�rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"Parallel corpora from Web Crawls collected in the ParaCrawl project and further processed for making it a multi-parallel corpus by pivoting via English. Here we only provide the additional language pairs that came out of pivoting. The bitexts for English are available from the ParaCrawl release.\n40 languages, 669 bitexts\ntotal number of files: 40\ntotal number of tokens: 10.14G\ntotal number of sentence fragments: 505.48M\n\nPlease, acknowledge the ParaCrawl project at http://paracrawl.eu. This version is derived from the original release at their website adjusted for redistribution via the OPUS corpus collection. Please, acknowledge OPUS as well for this service.","key":""},{"id":"multi_re_qa","tags":["annotations_creators:expert-generated","annotations_creators:found","language_creators:expert-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:1K<n<10K","size_categories:100K<n<1M","size_categories:1M<n<10M","source_datasets:extended|other-BioASQ","source_datasets:extended|other-DuoRC","source_datasets:extended|other-HotpotQA","source_datasets:extended|other-Natural-Questions","source_datasets:extended|other-Relation-Extraction","source_datasets:extended|other-SQuAD","source_datasets:extended|other-SearchQA","source_datasets:extended|other-TextbookQA","source_datasets:extended|other-TriviaQA","task_categories:question-answering","task_ids:extractive-qa","task_ids:open-domain-qa"],"citation":"@misc{m2020multireqa,\n    title={MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering Models},\n    author={Mandy Guo and Yinfei Yang and Daniel Cer and Qinlan Shen and Noah Constant},\n    year={2020},\n    eprint={2005.02507},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"MultiReQA contains the sentence boundary annotation from eight publicly available QA datasets including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, BioASQ, RelationExtraction, and TextbookQA. Five of these datasets, including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, contain both training and test data, and three, including BioASQ, RelationExtraction, TextbookQA, contain only the test data","paperswithcode_id":"multireqa","key":""},{"id":"multi_woz_v22","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","language_creators:machine-generated","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_categories:structure-prediction","task_categories:text-classification","task_ids:dialogue-modeling","task_ids:multi-class-classification","task_ids:parsing"],"citation":"@article{corr/abs-2007-12720,\n  author    = {Xiaoxue Zang and\n               Abhinav Rastogi and\n               Srinivas Sunkara and\n               Raghav Gupta and\n               Jianguo Zhang and\n               Jindong Chen},\n  title     = {MultiWOZ 2.2 : {A} Dialogue Dataset with Additional Annotation Corrections\n               and State Tracking Baselines},\n  journal   = {CoRR},\n  volume    = {abs/2007.12720},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2007.12720},\n  archivePrefix = {arXiv},\n  eprint    = {2007.12720}\n}","description":"Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.\nMultiWOZ 2.1 (Eric et al., 2019) identified and fixed many erroneous annotations and user utterances in the original version, resulting in an\nimproved version of the dataset. MultiWOZ 2.2 is a yet another improved version of this dataset, which identifies and fizes dialogue state annotation errors\nacross 17.3% of the utterances on top of MultiWOZ 2.1 and redefines the ontology by disallowing vocabularies of slots with a large number of possible values\n(e.g., restaurant name, time of booking) and introducing standardized slot span annotations for these slots.","paperswithcode_id":"multiwoz","key":""},{"id":"multi_x_science_sum","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@article{lu2020multi,\n  title={Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles},\n  author={Lu, Yao and Dong, Yue and Charlin, Laurent},\n  journal={arXiv preprint arXiv:2010.14235},\n  year={2020}\n}","description":"Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references.","paperswithcode_id":"multi-xscience","key":""},{"id":"mutual_friends","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:dialogue-modeling"],"citation":"@inproceedings{he-etal-2017-learning,\n    title = \"Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings\",\n    author = \"He, He  and\n      Balakrishnan, Anusha  and\n      Eric, Mihail  and\n      Liang, Percy\",\n    booktitle = \"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2017\",\n    address = \"Vancouver, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P17-1162\",\n    doi = \"10.18653/v1/P17-1162\",\n    pages = \"1766--1776\",\n    abstract = \"We study a \\textit{symmetric collaborative dialogue} setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.\",\n}","description":"Our goal is to build systems that collaborate with people by exchanging\ninformation through natural language and reasoning over structured knowledge\nbase. In the MutualFriend task, two agents, A and B, each have a private\nknowledge base, which contains a list of friends with multiple attributes\n(e.g., name, school, major, etc.). The agents must chat with each other\nto find their unique mutual friend.","paperswithcode_id":"mutualfriends","key":""},{"id":"mwsc","tags":["languages:en"],"citation":"@article{McCann2018decaNLP,\n  title={The Natural Language Decathlon: Multitask Learning as Question Answering},\n  author={Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},\n  journal={arXiv preprint arXiv:1806.08730},\n  year={2018}\n}","description":"Examples taken from the Winograd Schema Challenge modified to ensure that answers are a single word from the context.\nThis modified Winograd Schema Challenge (MWSC) ensures that scores are neither inflated nor deflated by oddities in phrasing.","key":""},{"id":"myanmar_news","tags":["languages:my","licenses:gpl-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:topic-classification"],"description":"The Myanmar news dataset contains article snippets in four categories:\nBusiness, Entertainment, Politics, and Sport.\n\nThese were collected in October 2017 by Aye Hninn Khine","key":""},{"id":"narrativeqa","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:abstractive-qa"],"citation":"@article{narrativeqa,\nauthor = {Tom\\\\'a\\\\v s Ko\\\\v cisk\\\\'y and Jonathan Schwarz and Phil Blunsom and\n          Chris Dyer and Karl Moritz Hermann and G\\\\'abor Melis and\n          Edward Grefenstette},\ntitle = {The {NarrativeQA} Reading Comprehension Challenge},\njournal = {Transactions of the Association for Computational Linguistics},\nurl = {https://TBD},\nvolume = {TBD},\nyear = {2018},\npages = {TBD},\n}","description":"The NarrativeQA dataset for question answering on long documents (movie scripts, books). It includes the list of documents with Wikipedia summaries, links to full stories, and questions and answers.","paperswithcode_id":"narrativeqa","key":""},{"id":"narrativeqa_manual","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:abstractive-qa"],"citation":"@article{kovcisky2018narrativeqa,\n  title={The narrativeqa reading comprehension challenge},\n  author={Ko{\\v{c}}isk{\\'y}, Tom{\\'a}{\\v{s}} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\\'a}bor and Grefenstette, Edward},\n  journal={Transactions of the Association for Computational Linguistics},\n  volume={6},\n  pages={317--328},\n  year={2018},\n  publisher={MIT Press}\n}","description":"The Narrative QA Manual dataset is a reading comprehension dataset, in which the reader must answer questions about stories by reading entire books or movie scripts. The QA tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience.\\THIS DATASET REQUIRES A MANUALLY DOWNLOADED FILE! Because of a script in the original repository which downloads the stories from original URLs everytime, The links are sometimes broken or invalid.  Therefore, you need to manually download the stories for this dataset using the script provided by the authors (https://github.com/deepmind/narrativeqa/blob/master/download_stories.sh). Running the shell script creates a folder named \"tmp\" in the root directory and downloads the stories there. This folder containing the storiescan be used to load the dataset via `datasets.load_dataset(\"narrativeqa_manual\", data_dir=\"<path/to/folder>\")`.","paperswithcode_id":"narrativeqa","key":""},{"id":"natural_questions","tags":["languages:en"],"citation":"@article{47761,\ntitle\t= {Natural Questions: a Benchmark for Question Answering Research},\nauthor\t= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},\nyear\t= {2019},\njournal\t= {Transactions of the Association of Computational Linguistics}\n}","description":"The NQ corpus contains questions from real users, and it requires QA systems to\nread and comprehend an entire Wikipedia article that may or may not contain the\nanswer to the question. The inclusion of real user questions, and the\nrequirement that solutions should read an entire page to find the answer, cause\nNQ to be a more realistic and challenging task than prior QA datasets.","paperswithcode_id":"natural-questions","key":""},{"id":"ncbi_disease","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@article{dougan2014ncbi,\n         title={NCBI disease corpus: a resource for disease name recognition and concept normalization},\n         author={Dogan, Rezarta Islamaj and Leaman, Robert and Lu, Zhiyong},\n         journal={Journal of biomedical informatics},\n         volume={47},\n         pages={1--10},\n         year={2014},\n         publisher={Elsevier}\n}","description":"This paper presents the disease name and concept annotations of the NCBI disease corpus, a collection of 793 PubMed\nabstracts fully annotated at the mention and concept level to serve as a research resource for the biomedical natural\nlanguage processing community. Each PubMed abstract was manually annotated by two annotators with disease mentions\nand their corresponding concepts in Medical Subject Headings (MeSH®) or Online Mendelian Inheritance in Man (OMIM®).\nManual curation was performed using PubTator, which allowed the use of pre-annotations as a pre-step to manual annotations.\nFourteen annotators were randomly paired and differing annotations were discussed for reaching a consensus in two\nannotation phases. In this setting, a high inter-annotator agreement was observed. Finally, all results were checked\nagainst annotations of the rest of the corpus to assure corpus-wide consistency.\n\nFor more details, see: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3951655/\n\nThe original dataset can be downloaded from: https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/NCBI_corpus.zip\nThis dataset has been converted to CoNLL format for NER using the following tool: https://github.com/spyysalo/standoff2conll\nNote: there is a duplicate document (PMID 8528200) in the original data, and the duplicate is recreated in the converted data.","paperswithcode_id":"ncbi-disease-1","key":""},{"id":"nchlt","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:af","languages:nr","languages:nso","languages:ss","languages:tn","languages:ts","languages:ve","languages:xh","languages:zu","licenses:cc-by-2.5","multilinguality:multilingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{eiselen2014developing,\n  title={Developing Text Resources for Ten South African Languages.},\n  author={Eiselen, Roald and Puttkammer, Martin J},\n  booktitle={LREC},\n  pages={3698--3703},\n  year={2014}\n}","description":"The development of linguistic resources for use in natural language processingis of utmost importance for the continued growth of research anddevelopment in the field, especially for resource-scarce languages. In this paper we describe the process and challenges of simultaneouslydevelopingmultiple linguistic resources for ten of the official languages of South Africa. The project focussed on establishing a set of foundational resources that can foster further development of both resources and technologies for the NLP industry in South Africa. The development efforts during the project included creating monolingual unannotated corpora, of which a subset of the corpora for each language was annotated on token, orthographic, morphological and morphosyntactic layers. The annotated subsetsincludes both development and test setsand were used in the creation of five core-technologies, viz. atokeniser, sentenciser,lemmatiser, part of speech tagger and morphological decomposer for each language. We report on the quality of these tools for each language and provide some more context of the importance of the resources within the South African context.","key":""},{"id":"ncslgr","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:ase","languages:en","licenses:mit","multilinguality:translation","size_categories:n<1K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@misc{dataset:databases2007volumes,\n    title={Volumes 2--7},\n    author={Databases, NCSLGR},\n    year={2007},\n    publisher={American Sign Language Linguistic Research Project (Distributed on CD-ROM~…}\n}","description":"A small corpus of American Sign Language (ASL) video data from native signers, annotated with non-manual features.","key":""},{"id":"nell","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1M<n<10M","size_categories:10M<n<100M","size_categories:100M<n<1B","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-retrieval","task_ids:entity-linking-retrieval","task_ids:fact-checking-retrieval","task_ids:other-stuctured-to-text"],"citation":"@inproceedings{mitchell2015,\n  added-at = {2015-01-27T15:35:24.000+0100},\n  author = {Mitchell, T. and Cohen, W. and Hruscha, E. and Talukdar, P. and Betteridge, J. and Carlson, A. and Dalvi, B. and Gardner, M. and Kisiel, B. and Krishnamurthy, J. and Lao, N. and Mazaitis, K. and Mohammad, T. and Nakashole, N. and Platanios, E. and Ritter, A. and Samadi, M. and Settles, B. and Wang, R. and Wijaya, D. and Gupta, A. and Chen, X. and Saparov, A. and Greaves, M. and Welling, J.},\n  biburl = {https://www.bibsonomy.org/bibtex/263070703e6bb812852cca56574aed093/hotho},\n  booktitle = {AAAI},\n  description = {Papers by William W. Cohen},\n  interhash = {52d0d71f6f5b332dabc1412f18e3a93d},\n  intrahash = {63070703e6bb812852cca56574aed093},\n  keywords = {learning nell ontology semantic toread},\n  note = {: Never-Ending Learning in AAAI-2015},\n  timestamp = {2015-01-27T15:35:24.000+0100},\n  title = {Never-Ending Learning},\n  url = {http://www.cs.cmu.edu/~wcohen/pubs.html},\n  year = 2015\n}","description":"This dataset provides version 1115 of the belief\nextracted by CMU's Never Ending Language Learner (NELL) and version\n1110 of the candidate belief extracted by NELL. See\nhttp://rtw.ml.cmu.edu/rtw/overview.  NELL is an open information\nextraction system that attempts to read the Clueweb09 of 500 million\nweb pages (http://boston.lti.cs.cmu.edu/Data/clueweb09/) and general\nweb searches.\n\nThe dataset has 4 configurations: nell_belief, nell_candidate,\nnell_belief_sentences, and nell_candidate_sentences. nell_belief is\ncertainties of belief are lower. The two sentences config extracts the\nCPL sentence patterns filled with the applicable 'best' literal string\nfor the entities filled into the sentence patterns. And also provides\nsentences found using web searches containing the entities and\nrelationships.\n\nThere are roughly 21M entries for nell_belief_sentences, and 100M\nsentences for nell_candidate_sentences.","paperswithcode_id":"nell","key":""},{"id":"neural_code_search","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:n<1K","size_categories:1M<n<10M","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@InProceedings{huggingface:dataset,\ntitle         = {Neural Code Search Evaluation Dataset},\nauthors       = {Hongyu Li, Seohyun Kim and Satish Chandra},\njournal       = {arXiv e-prints},\nyear          = 2018,\neid           = {arXiv:1908.09804 [cs.SE]},\npages         = {arXiv:1908.09804 [cs.SE]},\narchivePrefix = {arXiv},\neprint        = {1908.09804},\n}","description":"Neural-Code-Search-Evaluation-Dataset presents an evaluation dataset consisting of natural language query and code snippet pairs and a search corpus consisting of code snippets collected from the most popular Android repositories on GitHub.","paperswithcode_id":"neural-code-search-evaluation-dataset","key":""},{"id":"news_commentary","tags":["annotations_creators:found","language_creators:found","languages:ar","languages:cs","languages:de","languages:en","languages:es","languages:fr","languages:it","languages:ja","languages:nl","languages:pt","languages:ru","languages:zh","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J�rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"A parallel corpus of News Commentaries provided by WMT for training SMT. The source is taken from CASMACAT: http://www.casmacat.eu/corpus/news-commentary.html\n\n12 languages, 63 bitexts\ntotal number of files: 61,928\ntotal number of tokens: 49.66M\ntotal number of sentence fragments: 1.93M","key":""},{"id":"newsgroup","tags":["languages:en"],"citation":"@inproceedings{Lang95,\n    author = {Ken Lang},\n    title = {Newsweeder: Learning to filter netnews}\n    year = {1995}\n    booktitle = {Proceedings of the Twelfth International Conference on Machine Learning}\n    pages = {331-339}\n    }","description":"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across\n20 different newsgroups. The 20 newsgroups collection has become a popular data set for experiments in text applications of\nmachine learning techniques, such as text classification and text clustering.","key":""},{"id":"newsph","tags":["annotations_creators:no-annotation","language_creators:found","languages:fil","languages:tl","licenses:gpl-3.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@article{cruz2020investigating,\n  title={Investigating the True Performance of Transformers in Low-Resource Languages: A Case Study in Automatic Corpus Creation},\n  author={Jan Christian Blaise Cruz and Jose Kristian Resabal and James Lin and Dan John Velasco and Charibeth Cheng},\n  journal={arXiv preprint arXiv:2010.11574},\n  year={2020}\n}","description":"Large-scale dataset of Filipino news articles. Sourced for the NewsPH-NLI Project (Cruz et al., 2020).","paperswithcode_id":"newsph-nli","key":""},{"id":"newsph_nli","tags":["annotations_creators:machine-generated","language_creators:found","languages:tl","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:natural-language-inference"],"citation":"@article{cruz2020investigating,\n    title={Investigating the True Performance of Transformers in Low-Resource Languages: A Case Study in Automatic Corpus Creation},\n    author={Jan Christian Blaise Cruz and Jose Kristian Resabal and James Lin and Dan John Velasco and Charibeth Cheng},\n    journal={arXiv preprint arXiv:2010.11574},\n    year={2020}\n}","description":"First benchmark dataset for sentence entailment in the low-resource Filipino language.\nConstructed through exploting the structure of news articles. Contains 600,000 premise-hypothesis pairs,\nin 70-15-15 split for training, validation, and testing.","paperswithcode_id":"newsph-nli","key":""},{"id":"newspop","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-4","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-scoring","task_ids:other-social-media-shares-prediction"],"citation":"@article{Moniz2018MultiSourceSF,\n  title={Multi-Source Social Feedback of Online News Feeds},\n  author={N. Moniz and L. Torgo},\n  journal={ArXiv},\n  year={2018},\n  volume={abs/1801.07055}\n}","description":"This is a large data set of news items and their respective social feedback on multiple platforms: Facebook, Google+ and LinkedIn.\nThe collected data relates to a period of 8 months, between November 2015 and July 2016, accounting for about 100,000 news items on four different topics: economy, microsoft, obama and palestine.\nThis data set is tailored for evaluative comparisons in predictive analytics tasks, although allowing for tasks in other research areas such as topic detection and tracking, sentiment analysis in short text, first story detection or news recommendation.","key":""},{"id":"newsqa","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:mit","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@inproceedings{trischler2017newsqa,\n  title={NewsQA: A Machine Comprehension Dataset},\n  author={Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},\n  booktitle={Proceedings of the 2nd Workshop on Representation Learning for NLP},\n  pages={191--200},\n  year={2017}\n}","description":"NewsQA is a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles.","paperswithcode_id":"newsqa","key":""},{"id":"newsroom","tags":["languages:en"],"citation":"@inproceedings{N18-1065,\n  author    = {Grusky, Max and Naaman, Mor and Artzi, Yoav},\n  title     = {NEWSROOM: A Dataset of 1.3 Million Summaries\n               with Diverse Extractive Strategies},\n  booktitle = {Proceedings of the 2018 Conference of the\n               North American Chapter of the Association for\n               Computational Linguistics: Human Language Technologies},\n  year      = {2018},\n}","description":"NEWSROOM is a large dataset for training and evaluating summarization systems.\nIt contains 1.3 million articles and summaries written by authors and\neditors in the newsrooms of 38 major publications.\n\nDataset features includes:\n  - text: Input news text.\n  - summary: Summary for the news.\nAnd additional features:\n  - title: news title.\n  - url: url of the news.\n  - date: date of the article.\n  - density: extractive density.\n  - coverage: extractive coverage.\n  - compression: compression ratio.\n  - density_bin: low, medium, high.\n  - coverage_bin: extractive, abstractive.\n  - compression_bin: low, medium, high.\n\nThis dataset can be downloaded upon requests. Unzip all the contents\n\"train.jsonl, dev.josnl, test.jsonl\" to the tfds folder.","paperswithcode_id":"newsroom","key":""},{"id":"nkjp-ner","tags":["annotations_creators:expert-generated","language_creators:other","languages:pl","licenses:gpl-3.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@book{przepiorkowski2012narodowy,\ntitle={Narodowy korpus jezyka polskiego},\nauthor={Przepi{\\'o}rkowski, Adam},\nyear={2012},\npublisher={Naukowe PWN}\n}","description":"The NKJP-NER is based on a human-annotated part of National Corpus of Polish (NKJP). We extracted sentences with named entities of exactly one type. The task is to predict the type of the named entity.","key":""},{"id":"nli_tr","tags":[],"citation":"\\\r\n@inproceedings{budur-etal-2020-data,\r\n    title = \"Data and Representation for Turkish Natural Language Inference\",\r\n    author = \"Budur, Emrah and\r\n      \\\"{O}zçelik, Rıza and\r\n      G\\\"{u}ng\\\"{o}r, Tunga\",\r\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\r\n    month = nov,\r\n    year = \"2020\",\r\n    address = \"Online\",\r\n    publisher = \"Association for Computational Linguistics\",\r\n    abstract = \"Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now robust. Can we leverage these systems to translate English-language datasets automatically? In this paper, we offer a positive response for natural language inference (NLI) in Turkish. We translated two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. Using these datasets, we address core issues of representation for Turkish NLI. We find that in-language embeddings are essential and that morphological parsing can be avoided where the training set is large. Finally, we show that models trained on our machine-translated datasets are successful on human-translated evaluation sets. We share all code, models, and data publicly.\",\r\n}","description":"\\\r\nThe Natural Language Inference in Turkish (NLI-TR) is a set of two large scale datasets that were obtained by translating the foundational NLI corpora (SNLI and MNLI) using Amazon Translate.","paperswithcode_id":"nli-tr","key":""},{"id":"nlu_evaluation_data","tags":["annotations_creators:expert-generated","extended:original","language_creators:expert-generated","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification","task_ids:multi-class-classification"],"citation":"@InProceedings{XLiu.etal:IWSDS2019,\n  author    = {Xingkun Liu, Arash Eshghi, Pawel Swietojanski and Verena Rieser},\n  title     = {Benchmarking Natural Language Understanding Services for building Conversational Agents},\n  booktitle = {Proceedings of the Tenth International Workshop on Spoken Dialogue Systems Technology (IWSDS)},\n  month     = {April},\n  year      = {2019},\n  address   = {Ortigia, Siracusa (SR), Italy},\n  publisher = {Springer},\n  pages     = {xxx--xxx},\n  url       = {http://www.xx.xx/xx/}\n}","description":"Raw part of NLU Evaluation Data. It contains 25 715 non-empty examples (original dataset has 25716 examples) from 68 unique intents belonging to 18 scenarios.","key":""},{"id":"norec","tags":["annotations_creators:expert-generated","language_creators:found","languages:nb","languages:nn","languages:no","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@InProceedings{VelOvrBer18,\n  author = {Erik Velldal and Lilja Ovrelid and\n            Eivind Alexander Bergem and Cathrine Stadsnes and\n            Samia Touileb and Fredrik Jorgensen},\n  title = {{NoReC}: The {N}orwegian {R}eview {C}orpus},\n  booktitle = {Proceedings of the 11th edition of the\n               Language Resources and Evaluation Conference},\n  year = {2018},\n  address = {Miyazaki, Japan},\n  pages = {4186--4191}\n}","description":"NoReC was created as part of the SANT project (Sentiment Analysis for Norwegian Text), a collaboration between the Language Technology Group (LTG) at the Department of Informatics at the University of Oslo, the Norwegian Broadcasting Corporation (NRK), Schibsted Media Group and Aller Media. This first release of the corpus comprises 35,194 reviews extracted from eight different news sources: Dagbladet, VG, Aftenposten, Bergens Tidende, Fædrelandsvennen, Stavanger Aftenblad, DinSide.no and P3.no. In terms of publishing date the reviews mainly cover the time span 2003–2017, although it also includes a handful of reviews dating back as far as 1998.","paperswithcode_id":"norec","key":""},{"id":"norne","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","licenses:other-national-library-of-norway","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{johansen2019ner,\n  title={NorNE: Annotating Named Entities for Norwegian},\n  author={Fredrik Jørgensen, Tobias Aasmoe, Anne-Stine Ruud Husevåg,\n          Lilja Øvrelid, and Erik Velldal},\n  booktitle={LREC 2020},\n  year={2020},\n  url={https://arxiv.org/abs/1911.12146}\n}","description":"NorNE is a manually annotated\ncorpus of named entities which extends the annotation of the existing\nNorwegian Dependency Treebank. Comprising both of the official standards of\nwritten Norwegian (Bokmål and Nynorsk), the corpus contains around 600,000\ntokens and annotates a rich set of entity types including persons,\norganizations, locations, geo-political entities, products, and events,\nin addition to a class corresponding to nominals derived from names.","key":""},{"id":"norwegian_ner","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:no","licenses:unknown-","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{johansen2019ner,\n  title={Named-Entity Recognition for Norwegian},\n  author={Johansen, Bjarte},\n  booktitle={Proceedings of the 22nd Nordic Conference on Computational Linguistics, NoDaLiDa},\n  year={2019}\n}","description":"Named entities Recognition dataset for Norwegian. It is\na version of the Universal Dependency (UD) Treebank for both Bokmål and Nynorsk (UDN) where\nall proper nouns have been tagged with their type according to the NER tagging scheme. UDN is a converted\nversion of the Norwegian Dependency Treebank into the UD scheme.","key":""},{"id":"nq_open","tags":["annotations_creators:expert-generated","language_creators:other","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|natural_questions","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@article{doi:10.1162/tacl_a_00276,\n    author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew                         M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},\n    title = {Natural Questions: A Benchmark for Question Answering Research},\n    journal = {Transactions of the Association for Computational Linguistics},\n    volume = {7},\n    number = {},\n    pages = {453-466},\n    year = {2019},\n    doi = {10.1162/tacl_a_00276},\n    URL = {\n            https://doi.org/10.1162/tacl_a_00276\n        },\n    eprint = {\n            https://doi.org/10.1162/tacl_a_00276\n        },\n    abstract = { We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature. }\n}\n\n@inproceedings{lee-etal-2019-latent,\n    title = \"Latent Retrieval for Weakly Supervised Open Domain Question Answering\",\n    author = \"Lee, Kenton  and\n      Chang, Ming-Wei  and\n      Toutanova, Kristina\",\n    booktitle = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P19-1612\",\n    doi = \"10.18653/v1/P19-1612\",\n    pages = \"6086--6096\",\n    abstract = \"Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.\",\n}","description":"The NQ-Open task, introduced by Lee et.al. 2019,\nis an open domain question answering benchmark that is derived from Natural Questions.\nThe goal is to predict an English answer string for an input English question.\nAll questions can be answered using the contents of English Wikipedia.","key":""},{"id":"nsmc","tags":["annotations_creators:crowdsourced","language_creators:found","languages:ko","licenses:cc-by-1.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@InProceedings{Park:2016,\n  title        = \"Naver Sentiment Movie Corpus\",\n  author       = \"Lucy Park\",\n  year         = \"2016\",\n  howpublished = {\\\\url{https://github.com/e9t/nsmc}}\n}","description":"This is a movie review dataset in the Korean language. Reviews were scraped from Naver movies. The dataset construction is based on the method noted in Large movie review dataset from Maas et al., 2011.","paperswithcode_id":"nsmc","key":""},{"id":"numer_sense","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other","task_categories:sequence-modeling","task_ids:slot-filling"],"citation":"@inproceedings{lin2020numersense,\n    title={Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models},\n    author={Bill Yuchen Lin and Seyeon Lee and Rahul Khanna and Xiang Ren},\n    booktitle={Proceedings of EMNLP},\n    year={2020},\n    note={to appear}\n}","description":"NumerSense is a new numerical commonsense reasoning probing task, with a diagnostic dataset consisting of 3,145 masked-word-prediction probes.\n\nWe propose to study whether numerical commonsense knowledge can be induced from pre-trained language models like BERT, and to what extent this access to knowledge robust against adversarial examples is. We hope this will be beneficial for tasks such as knowledge base completion and open-domain question answering.","paperswithcode_id":"numersense","key":""},{"id":"numeric_fused_head","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:mit","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:structure-prediction-other-fused-head-identification"],"citation":"@article{elazar_head,\n    author = {Elazar, Yanai and Goldberg, Yoav},\n    title = {Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution},\n    journal = {Transactions of the Association for Computational Linguistics},\n    volume = {7},\n    number = {},\n    pages = {519-535},\n    year = {2019},\n    doi = {10.1162/tacl\\\\_a\\\\_00280},\n    URL = {https://doi.org/10.1162/tacl_a_00280},\n}","description":"Fused Head constructions are noun phrases in which the head noun is missing and is said to be \"fused\" with its dependent modifier. This missing information is implicit and is important for sentence understanding.The missing heads are easily filled in by humans,  but pose a challenge for computational models.\n\nFor example, in the sentence: \"I bought 5 apples but got only 4.\", 4 is a Fused-Head, and the missing head is apples, which appear earlier in the sentence.\n\nThis is a crowd-sourced dataset of 10k numerical fused head examples (1M tokens).","paperswithcode_id":"numeric-fused-head","key":""},{"id":"oclar","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_categories:text-scoring","task_ids:sentiment-classification","task_ids:sentiment-scoring"],"citation":"@misc{Dua:2019 ,\nauthor = \"Dua, Dheeru and Graff, Casey\",\nyear = \"2017\",\ntitle = \"{UCI} Machine Learning Repository\",\nurl = \"http://archive.ics.uci.edu/ml\",\ninstitution = \"University of California, Irvine, School of Information and Computer Sciences\" }\n\n@InProceedings{AlOmari2019oclar,\ntitle = {Sentiment Classifier: Logistic Regression for Arabic Services Reviews in Lebanon},\nauthors={Al Omari, M., Al-Hajj, M., Hammami, N., & Sabra, A.},\nyear={2019}\n}","description":"The researchers of OCLAR Marwan et al. (2019), they gathered Arabic costumer reviews from Google reviewsa and Zomato\nwebsite (https://www.zomato.com/lebanon) on wide scope of domain, including restaurants, hotels, hospitals, local shops,\netc.The corpus finally contains 3916 reviews in 5-rating scale. For this research purpose, the positive class considers\nrating stars from 5 to 3 of 3465 reviews, and the negative class is represented from values of 1 and 2 of about\n451 texts.","key":""},{"id":"offcombr","tags":["annotations_creators:expert-generated","language_creators:found","languages:pt","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-hate-speech-detection"],"citation":"@article{Pelle2017,\ntitle={Offensive Comments in the Brazilian Web: a dataset and baseline results},\nauthor={Rogers P. de Pelle and Viviane P. Moreira},\nbooktitle={6th Brazilian Workshop on Social Network Analysis and Mining (BraSNAM)},\nyear={2017},\n}","description":"OffComBR: an annotated dataset containing for hate speech detection in Portuguese composed of news comments on the Brazilian Web.","paperswithcode_id":"offcombr","key":""},{"id":"offenseval2020_tr","tags":["annotations_creators:found","language_creators:found","languages:tr","licenses:found","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-offensive-language-classification"],"citation":"@InProceedings{coltekin2020lrec,\n author  = {Cagri Coltekin},\n year  = {2020},\n title  = {A Corpus of Turkish Offensive Language on Social Media},\n booktitle  = {Proceedings of The 12th Language Resources and Evaluation Conference},\n pages  = {6174--6184},\n address  = {Marseille, France},\n url  = {https://www.aclweb.org/anthology/2020.lrec-1.758},\n}","description":"OffensEval-TR 2020 is a Turkish offensive language corpus. The corpus consist of randomly sampled tweets and annotated in a similar way to OffensEval and GermEval.","key":""},{"id":"offenseval_dravidian","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","languages:kn","languages:ml","languages:ta","licenses:cc-by-4.0","multilinguality:multilingual","size_categories:1K<n<10K","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-offensive-language"],"citation":"@inproceedings{dravidianoffensive-eacl,\ntitle={Findings of the Shared Task on {O}ffensive {L}anguage {I}dentification in {T}amil, {M}alayalam, and {K}annada},\nauthor={Chakravarthi, Bharathi Raja and\nPriyadharshini, Ruba and\nJose, Navya and\nM, Anand Kumar and\nMandl, Thomas and\nKumaresan, Prasanna Kumar and\nPonnsamy, Rahul and\nV,Hariharan and\nSherly, Elizabeth and\nMcCrae, John Philip },\nbooktitle = \"Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages\",\nmonth = April,\nyear = \"2021\",\npublisher = \"Association for Computational Linguistics\",\nyear={2021}\n}","description":"Offensive language identification in dravidian lanaguages dataset. The goal of this task is to identify offensive language content of the code-mixed dataset of comments/posts in Dravidian Languages ( (Tamil-English, Malayalam-English, and Kannada-English)) collected from social media.","key":""},{"id":"ofis_publik","tags":["annotations_creators:found","language_creators:found","languages:br","languages:fr","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }\n @inproceedings{tyers-2009-rule,\n    title = \"Rule-Based Augmentation of Training Data in {B}reton-{F}rench Statistical Machine Translation\",\n    author = \"Tyers, Francis M.\",\n    booktitle = \"Proceedings of the 13th Annual conference of the European Association for Machine Translation\",\n    month = may # \" 14{--}15\",\n    year = \"2009\",\n    address = \"Barcelona, Spain\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"https://www.aclweb.org/anthology/2009.eamt-1.29\",\n}","description":"Texts from the Ofis Publik ar Brezhoneg (Breton Language Board) provided by Francis Tyers\n2 languages, total number of files: 278\ntotal number of tokens: 2.12M\ntotal number of sentence fragments: 0.13M","key":""},{"id":"ohsumed","tags":["annotations_creators:human-annotated","language_creators:crowdsourced","languages:en","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:multi-label-classification"],"citation":"@InProceedings{10.1007/978-1-4471-2099-5_20,\nauthor=\"Hersh, William\nand Buckley, Chris\nand Leone, T. J.\nand Hickam, David\",\neditor=\"Croft, Bruce W.\nand van Rijsbergen, C. J.\",\ntitle=\"OHSUMED: An Interactive Retrieval Evaluation and New Large Test Collection for Research\",\nbooktitle=\"SIGIR '94\",\nyear=\"1994\",\npublisher=\"Springer London\",\naddress=\"London\",\npages=\"192--201\",\nabstract=\"A series of information retrieval experiments was carried out with a computer installed in a medical practice setting for relatively inexperienced physician end-users. Using a commercial MEDLINE product based on the vector space model, these physicians searched just as effectively as more experienced searchers using Boolean searching. The results of this experiment were subsequently used to create a new large medical test collection, which was used in experiments with the SMART retrieval system to obtain baseline performance data as well as compare SMART with the other searchers.\",\nisbn=\"978-1-4471-2099-5\"\n}","description":"The OHSUMED test collection is a set of 348,566 references from\nMEDLINE, the on-line medical information database, consisting of\ntitles and/or abstracts from 270 medical journals over a five-year\nperiod (1987-1991). The available fields are title, abstract, MeSH\nindexing terms, author, source, and publication type.","key":""},{"id":"ollie","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:en","licenses:other-university-of-washington-academic","multilinguality:monolingual","size_categories:10M<n<100M","size_categories:1M<n<10M","source_datasets:original","task_categories:other","task_ids:other-stuctured-to-text","task_ids:other-other-relation-extraction"],"citation":"@inproceedings{ollie-emnlp12,\n  author = {Mausam and Michael Schmitz and Robert Bart and Stephen Soderland and Oren Etzioni},\n  title = {Open Language Learning for Information Extraction},\n  booktitle = {Proceedings of Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CONLL)},\n  year = {2012}\n}","description":"The Ollie dataset includes two configs for the data\nused to train the Ollie informatation extraction algorithm, for 18M\nsentences and 3M sentences respectively.\n\nThis data is for academic use only. From the authors:\n\nOllie is a program that automatically identifies and extracts binary\nrelationships from English sentences. Ollie is designed for Web-scale\ninformation extraction, where target relations are not specified in\nadvance.\n\nOllie is our second-generation information extraction system . Whereas\nReVerb operates on flat sequences of tokens, Ollie works with the\ntree-like (graph with only small cycles) representation using\nStanford's compression of the dependencies. This allows Ollie to\ncapture expression that ReVerb misses, such as long-range relations.\n\nOllie also captures context that modifies a binary relation. Presently\nOllie handles attribution (He said/she believes) and enabling\nconditions (if X then).\n\nMore information is available at the Ollie homepage:\nhttps://knowitall.github.io/ollie/","key":""},{"id":"omp","tags":["pretty_name:One Million Posts","annotations_creators:expert-generated","language_creators:crowdsourced","languages:de","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@InProceedings{Schabus2017,\n  Author    = {Dietmar Schabus and Marcin Skowron and Martin Trapp},\n  Title     = {One Million Posts: A Data Set of German Online Discussions},\n  Booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},\n  Pages     = {1241--1244},\n  Year      = {2017},\n  Address   = {Tokyo, Japan},\n  Doi       = {10.1145/3077136.3080711},\n  Month     = aug\n}","description":"The “One Million Posts” corpus is an annotated data set consisting of\nuser comments posted to an Austrian newspaper website (in German language).\n\nDER STANDARD is an Austrian daily broadsheet newspaper. On the newspaper’s website,\nthere is a discussion section below each news article where readers engage in\nonline discussions. The data set contains a selection of user posts from the\n12 month time span from 2015-06-01 to 2016-05-31. There are 11,773 labeled and\n1,000,000 unlabeled posts in the data set. The labeled posts were annotated by\nprofessional forum moderators employed by the newspaper.\n\nThe data set contains the following data for each post:\n\n* Post ID\n* Article ID\n* Headline (max. 250 characters)\n* Main Body (max. 750 characters)\n* User ID (the user names used by the website have been re-mapped to new numeric IDs)\n* Time stamp\n* Parent post (replies give rise to tree-like discussion thread structures)\n* Status (online or deleted by a moderator)\n* Number of positive votes by other community members\n* Number of negative votes by other community members\n\nFor each article, the data set contains the following data:\n\n* Article ID\n* Publishing date\n* Topic Path (e.g.: Newsroom / Sports / Motorsports / Formula 1)\n* Title\n* Body\n\nDetailed descriptions of the post selection and annotation procedures are given in the paper.\n\n## Annotated Categories\n\nPotentially undesirable content:\n\n* Sentiment (negative/neutral/positive)\n    An important goal is to detect changes in the prevalent sentiment in a discussion, e.g.,\n    the location within the fora and the point in time where a turn from positive/neutral\n    sentiment to negative sentiment takes place.\n* Off-Topic (yes/no)\n    Posts which digress too far from the topic of the corresponding article.\n* Inappropriate (yes/no)\n    Swearwords, suggestive and obscene language, insults, threats etc.\n* Discriminating (yes/no)\n    Racist, sexist, misogynistic, homophobic, antisemitic and other misanthropic content.\n\nNeutral content that requires a reaction:\n\n* Feedback (yes/no)\n    Sometimes users ask questions or give feedback to the author of the article or the\n    newspaper in general, which may require a reply/reaction.\n\nPotentially desirable content:\n\n* Personal Stories (yes/no)\n    In certain fora, users are encouraged to share their personal stories, experiences,\n    anecdotes etc. regarding the respective topic.\n* Arguments Used (yes/no)\n    It is desirable for users to back their statements with rational argumentation,\n    reasoning and sources.","paperswithcode_id":"one-million-posts-corpus","key":""},{"id":"onestop_english","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-classification","task_ids:multi-class-classification","task_ids:text-simplification"],"citation":"@inproceedings{vajjala-lucic-2018-onestopenglish,\n    title = {OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification},\n    author = {Sowmya Vajjala and Ivana Lučić},\n    booktitle = {Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications},\n    year = {2018}\n}","description":"This dataset is a compilation of the OneStopEnglish corpus of texts written at three reading levels into one file.\nText documents are classified into three reading levels - ele, int, adv (Elementary, Intermediate and Advance).\nThis dataset demonstrates its usefulness for through two applica-tions - automatic  readability  assessment  and automatic text simplification.\nThe corpus consists of 189 texts, each in three versions/reading levels (567 in total).","paperswithcode_id":"onestopenglish","key":""},{"id":"open_subtitles","tags":["annotations_creators:found","language_creators:found","languages:af","languages:ar","languages:bg","languages:bn","languages:br","languages:bs","languages:ca","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:eo","languages:es","languages:et","languages:eu","languages:fa","languages:fi","languages:fr","languages:gl","languages:he","languages:hi","languages:hr","languages:hu","languages:hy","languages:id","languages:is","languages:it","languages:ja","languages:ka","languages:kk","languages:ko","languages:lt","languages:lv","languages:mk","languages:ml","languages:ms","languages:nl","languages:no","languages:pl","languages:pt","languages:pt_br","languages:ro","languages:ru","languages:si","languages:sk","languages:sl","languages:sq","languages:sr","languages:sv","languages:ta","languages:te","languages:th","languages:tl","languages:tr","languages:uk","languages:ur","languages:vi","languages:ze_en","languages:ze_zh","languages:zh_cn","languages:zh_tw","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","size_categories:1M<n<10M","size_categories:n<1K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"P. Lison and J. Tiedemann, 2016, OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles. In Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016)","description":"This is a new collection of translated movie subtitles from http://www.opensubtitles.org/.\n\nIMPORTANT: If you use the OpenSubtitle corpus: Please, add a link to http://www.opensubtitles.org/ to your website and to your reports and publications produced with the data!\n\nThis is a slightly cleaner version of the subtitle collection using improved sentence alignment and better language checking.\n\n62 languages, 1,782 bitexts\ntotal number of files: 3,735,070\ntotal number of tokens: 22.10G\ntotal number of sentence fragments: 3.35G","paperswithcode_id":"opensubtitles","key":""},{"id":"openbookqa","tags":["languages:en"],"citation":"@inproceedings{OpenBookQA2018,\n title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},\n author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},\n booktitle={EMNLP},\n year={2018}\n}","paperswithcode_id":"openbookqa","key":""},{"id":"openslr","tags":["pretty_name:OpenSLR","annotations_creators:found","language_creators:found","languages:af","languages:st","languages:tn","languages:xh","languages:jv","languages:su","languages:km","languages:ne","languages:si","languages:bn","languages:ml","languages:mr","languages:ta","languages:te","languages:ca","languages:en-NG","languages:es-CL","languages:es-CO","languages:es-PE","languages:es-PR","languages:ve","languages:eu","languages:gl","languages:gu","languages:kn","languages:my","languages:yo","licenses:cc-by-sa-4.0","multilinguality:multilingual","size_categories:1K<n<10K","source_datasets:original","task_categories:speech-processing","task_ids:automatic-speech-recognition"],"citation":"SLR32:\n@inproceedings{van-niekerk-etal-2017,\n    title = {{Rapid development of TTS corpora for four South African languages}},\n    author = {Daniel van Niekerk and Charl van Heerden and Marelie Davel and Neil Kleynhans and Oddur Kjartansson\n    and Martin Jansche and Linne Ha},\n    booktitle = {Proc. Interspeech 2017},\n    pages = {2178--2182},\n    address = {Stockholm, Sweden},\n    month = aug,\n    year  = {2017},\n    URL   = {http://dx.doi.org/10.21437/Interspeech.2017-1139}\n}\n\nSLR35, SLR36, SLR52, SLR53, SLR54:\n@inproceedings{kjartansson-etal-sltu2018,\n    title = {{Crowd-Sourced Speech Corpora for Javanese, Sundanese,  Sinhala, Nepali, and Bangladeshi Bengali}},\n    author = {Oddur Kjartansson and Supheakmungkol Sarin and Knot Pipatsrisawat and Martin Jansche and Linne Ha},\n    booktitle = {Proc. The 6th Intl. Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU)},\n    year  = {2018},\n    address = {Gurugram, India},\n    month = aug,\n    pages = {52--55},\n    URL   = {https://dx.doi.org/10.21437/SLTU.2018-11},\n}\n\nSLR41, SLR42, SLR43, SLR44:\n@inproceedings{kjartansson-etal-tts-sltu2018,\n    title = {{A Step-by-Step Process for Building TTS Voices Using Open Source Data and Framework for Bangla, Javanese,\n    Khmer, Nepali, Sinhala, and Sundanese}},\n    author = {Keshan Sodimana and Knot Pipatsrisawat and Linne Ha and Martin Jansche and Oddur Kjartansson and Pasindu\n    De Silva and Supheakmungkol Sarin},\n    booktitle = {Proc. The 6th Intl. Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU)},\n    year  = {2018},\n    address = {Gurugram, India},\n    month = aug,\n    pages = {66--70},\n    URL   = {https://dx.doi.org/10.21437/SLTU.2018-14}\n}\n\nSLR63, SLR64, SLR65, SLR66, SLR78, SLR79:\n@inproceedings{he-etal-2020-open,\n  title = {{Open-source Multi-speaker Speech Corpora for Building Gujarati, Kannada, Malayalam, Marathi, Tamil and\n  Telugu Speech Synthesis Systems}},\n  author = {He, Fei and Chu, Shan-Hui Cathy and Kjartansson, Oddur and Rivera, Clara and Katanova, Anna and Gutkin,\n  Alexander and Demirsahin, Isin and Johny, Cibu and Jansche, Martin and Sarin, Supheakmungkol and Pipatsrisawat, Knot},\n  booktitle = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC)},\n  month = may,\n  year = {2020},\n  address = {Marseille, France},\n  publisher = {European Language Resources Association (ELRA)},\n  pages = {6494--6503},\n  url = {https://www.aclweb.org/anthology/2020.lrec-1.800},\n  ISBN = \"{979-10-95546-34-4},\n}\n\nSLR69, SLR76, SLR77:\n@inproceedings{kjartansson-etal-2020-open,\n    title = {{Open-Source High Quality Speech Datasets for Basque, Catalan and Galician}},\n    author = {Kjartansson, Oddur and Gutkin, Alexander and Butryna, Alena and Demirsahin, Isin and Rivera, Clara},\n    booktitle = {Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages\n    (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)},\n    year = {2020},\n    pages = {21--27},\n    month = may,\n    address = {Marseille, France},\n    publisher = {European Language Resources association (ELRA)},\n    url = {https://www.aclweb.org/anthology/2020.sltu-1.3},\n    ISBN = {979-10-95546-35-1},\n}\n\nSLR71, SLR71, SLR72, SLR73, SLR74, SLR75:\n@inproceedings{guevara-rukoz-etal-2020-crowdsourcing,\n    title = {{Crowdsourcing Latin American Spanish for Low-Resource Text-to-Speech}},\n    author = {Guevara-Rukoz, Adriana and Demirsahin, Isin and He, Fei and Chu, Shan-Hui Cathy and Sarin,\n    Supheakmungkol and Pipatsrisawat, Knot and Gutkin, Alexander and Butryna, Alena and Kjartansson, Oddur},\n    booktitle = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC)},\n    year = {2020},\n    month = may,\n    address = {Marseille, France},\n    publisher = {European Language Resources Association (ELRA)},\n    url = {https://www.aclweb.org/anthology/2020.lrec-1.801},\n    pages = {6504--6513},\n    ISBN = {979-10-95546-34-4},\n}\n\nSLR80\n@inproceedings{oo-etal-2020-burmese,\n    title = {{Burmese Speech Corpus, Finite-State Text Normalization and Pronunciation Grammars with an Application\n    to Text-to-Speech}},\n    author = {Oo, Yin May and Wattanavekin, Theeraphol and Li, Chenfang and De Silva, Pasindu and Sarin,\n    Supheakmungkol and Pipatsrisawat, Knot and Jansche, Martin and Kjartansson, Oddur and Gutkin, Alexander},\n    booktitle = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC)},\n    month = may,\n    year = {2020},\n    pages = \"6328--6339\",\n    address = {Marseille, France},\n    publisher = {European Language Resources Association (ELRA)},\n    url = {https://www.aclweb.org/anthology/2020.lrec-1.777},\n    ISBN = {979-10-95546-34-4},\n}\n\nSLR86\n@inproceedings{gutkin-et-al-yoruba2020,\n    title = {{Developing an Open-Source Corpus of Yoruba Speech}},\n    author = {Alexander Gutkin and Işın Demirşahin and Oddur Kjartansson and Clara Rivera and Kọ́lá Túbọ̀sún},\n    booktitle = {Proceedings of Interspeech 2020},\n    pages = {404--408},\n    month = {October},\n    year = {2020},\n    address = {Shanghai, China},\n    publisher = {International Speech and Communication Association (ISCA)},\n    doi = {10.21437/Interspeech.2020-1096},\n    url = {https://dx.doi.org/10.21437/Interspeech.2020-1096},\n}","description":"OpenSLR is a site devoted to hosting speech and language resources, such as training corpora for speech recognition,\nand software related to speech recognition. We intend to be a convenient place for anyone to put resources that\nthey have created, so that they can be downloaded publicly.","key":""},{"id":"openwebtext","tags":["languages:en"],"citation":"@misc{Gokaslan2019OpenWeb,\n  title={OpenWebText Corpus},\n  author={Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, Stefanie Tellex},\n  howpublished{\\\\url{http://Skylion007.github.io/OpenWebTextCorpus}},\n  year={2019}\n}","description":"An open-source replication of the WebText dataset from OpenAI.","paperswithcode_id":"openwebtext","key":""},{"id":"opinosis","tags":["languages:en"],"citation":"@inproceedings{ganesan2010opinosis,\n  title={Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions},\n  author={Ganesan, Kavita and Zhai, ChengXiang and Han, Jiawei},\n  booktitle={Proceedings of the 23rd International Conference on Computational Linguistics},\n  pages={340--348},\n  year={2010},\n  organization={Association for Computational Linguistics}\n}","description":"The Opinosis Opinion Dataset consists of sentences extracted from reviews for 51 topics.\nTopics and opinions are obtained from Tripadvisor, Edmunds.com and Amazon.com.","paperswithcode_id":"opinosis","key":""},{"id":"opus100","tags":["task_categories:sequence-modeling","multilinguality:translation","task_ids:language-modeling","languages:af","languages:en","languages:am","languages:an","languages:ar","languages:as","languages:az","languages:be","languages:bg","languages:bn","languages:br","languages:bs","languages:ca","languages:cs","languages:cy","languages:da","languages:de","languages:dz","languages:el","languages:eo","languages:es","languages:et","languages:eu","languages:fa","languages:fi","languages:fr","languages:fy","languages:ga","languages:gd","languages:gl","languages:gu","languages:ha","languages:he","languages:hi","languages:hr","languages:hu","languages:hy","languages:id","languages:ig","languages:is","languages:it","languages:ja","languages:ka","languages:kk","languages:km","languages:ko","languages:kn","languages:ku","languages:ky","languages:li","languages:lt","languages:lv","languages:mg","languages:mk","languages:ml","languages:mn","languages:mr","languages:ms","languages:mt","languages:my","languages:nb","languages:ne","languages:nl","languages:nn","languages:no","languages:oc","languages:or","languages:pa","languages:pl","languages:ps","languages:pt","languages:ro","languages:ru","languages:rw","languages:se","languages:sh","languages:si","languages:sk","languages:sl","languages:sq","languages:sr","languages:sv","languages:ta","languages:te","languages:tg","languages:th","languages:tk","languages:tr","languages:tt","languages:ug","languages:uk","languages:ur","languages:uz","languages:vi","languages:wa","languages:xh","languages:yi","languages:yo","languages:zh","languages:zu","annotations_creators:no-annotation","source_datasets:extended","size_categories:100K<n<1M","size_categories:10K<n<100K","size_categories:1K<n<10K","size_categories:1M<n<10M","size_categories:n<1K","licenses:unknown"],"citation":"@misc{zhang2020improving,\n      title={Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation},\n      author={Biao Zhang and Philip Williams and Ivan Titov and Rico Sennrich},\n      year={2020},\n      eprint={2004.11867},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"OPUS-100 is English-centric, meaning that all training pairs include English on either the source or target side.\nThe corpus covers 100 languages (including English).OPUS-100 contains approximately 55M sentence pairs.\nOf the 99 language pairs, 44 have 1M sentence pairs of training data, 73 have at least 100k, and 95 have at least 10k.","paperswithcode_id":"opus-100","key":""},{"id":"opus_books","tags":["annotations_creators:found","language_creators:found","languages:ca","languages:de","languages:el","languages:en","languages:eo","languages:es","languages:fi","languages:fr","languages:hu","languages:it","languages:nl","languages:no","languages:pl","languages:pt","languages:ru","languages:sv","licenses:unknown","multilinguality:multilingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J�rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"This is a collection of copyright free books aligned by Andras Farkas, which are available from http://www.farkastranslations.com/bilingual_books.php\nNote that the texts are rather dated due to copyright issues and that some of them are manually reviewed (check the meta-data at the top of the corpus files in XML). The source is multilingually aligned, which is available from http://www.farkastranslations.com/bilingual_books.php. In OPUS, the alignment is formally bilingual but the multilingual alignment can be recovered from the XCES sentence alignment files. Note also that the alignment units from the original source may include multi-sentence paragraphs, which are split and sentence-aligned in OPUS.\nAll texts are freely available for personal, educational and research use. Commercial use (e.g. reselling as parallel books) and mass redistribution without explicit permission are not granted. Please acknowledge the source when using the data!\n\n16 languages, 64 bitexts\ntotal number of files: 158\ntotal number of tokens: 19.50M\ntotal number of sentence fragments: 0.91M","key":""},{"id":"opus_dgt","tags":["annotations_creators:found","language_creators:found","languages:bg","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:fi","languages:fr","languages:ga","languages:hr","languages:hu","languages:it","languages:lt","languages:lv","languages:mt","languages:nl","languages:pl","languages:pt","languages:ro","languages:sh","languages:sk","languages:sl","languages:sv","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","size_categories:1M<n<10M","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"A collection of translation memories provided by the JRC. Source: https://ec.europa.eu/jrc/en/language-technologies/dgt-translation-memory\n25 languages, 299 bitexts\ntotal number of files: 817,410\ntotal number of tokens: 2.13G\ntotal number of sentence fragments: 113.52M","key":""},{"id":"opus_dogc","tags":["annotations_creators:no-annotation","language_creators:expert-generated","languages:ca","languages:es","licenses:cc0-1.0","multilinguality:translation","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{tiedemann-2012-parallel,\n    title = \"Parallel Data, Tools and Interfaces in {OPUS}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)\",\n    month = may,\n    year = \"2012\",\n    address = \"Istanbul, Turkey\",\n    publisher = \"European Language Resources Association (ELRA)\",\n    url = \"http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf\",\n    pages = \"2214--2218\",\n    abstract = \"This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.\",\n}","description":"This is a collection of documents from the Official Journal of the Government of Catalonia, in Catalan and Spanish languages, provided by Antoni Oliver Gonzalez from the Universitat Oberta de Catalunya.","key":""},{"id":"opus_elhuyar","tags":["annotations_creators:found","language_creators:found","languages:es","languages:eu","licenses:unknown","multilinguality:translation","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{opus:Elhuyar,\ntitle = {Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)},\nauthors={J. Tiedemann},\nyear={2012}\n}","description":"Dataset provided by the foundation Elhuyar, which is having data in languages Spanish to Basque.","key":""},{"id":"opus_euconst","tags":["annotations_creators:found","language_creators:found","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:fi","languages:fr","languages:ga","languages:hu","languages:it","languages:lt","languages:lv","languages:mt","languages:nl","languages:pl","languages:pt","languages:sk","languages:sl","languages:sv","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)","description":"A parallel corpus collected from the European Constitution for 21 language.","key":""},{"id":"opus_finlex","tags":["annotations_creators:found","language_creators:found","languages:fi","languages:sv","licenses:unknown","multilinguality:translation","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)","description":"The Finlex Data Base is a comprehensive collection of legislative and other judicial information of Finland, which is available in Finnish, Swedish and partially in English. This corpus is taken from the Semantic Finlex serice that provides the Finnish and Swedish data as linked open data and also raw XML files.","key":""},{"id":"opus_fiskmo","tags":["annotations_creators:found","language_creators:found","languages:fi","languages:sv","licenses:unknown","multilinguality:translation","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)","description":"fiskmo, a massive parallel corpus for Finnish and Swedish.","key":""},{"id":"opus_gnome","tags":["annotations_creators:found","language_creators:found","languages:af","languages:am","languages:an","languages:ang","languages:ar","languages:ar_TN","languages:ara","languages:as","languages:ast","languages:az","languages:az_IR","languages:bal","languages:be","languages:bem","languages:bg","languages:bg_BG","languages:bn","languages:bn_IN","languages:bo","languages:br","languages:brx","languages:bs","languages:ca","languages:cat","languages:crh","languages:cs","languages:csb","languages:cy","languages:da","languages:da_DK","languages:de","languages:de_CH","languages:dv","languages:dz","languages:el","languages:en","languages:en_AU","languages:en_CA","languages:en_GB","languages:en_NZ","languages:en_US","languages:en_ZA","languages:eo","languages:es","languages:es_AR","languages:es_CL","languages:es_CO","languages:es_CR","languages:es_DO","languages:es_EC","languages:es_ES","languages:es_GT","languages:es_HN","languages:es_MX","languages:es_NI","languages:es_PA","languages:es_PE","languages:es_PR","languages:es_SV","languages:es_UY","languages:es_VE","languages:et","languages:eu","languages:fa","languages:fa_IR","languages:fi","languages:fo","languages:foo","languages:fr","languages:fur","languages:fy","languages:ga","languages:gd","languages:gl","languages:gn","languages:gr","languages:gu","languages:gv","languages:ha","languages:he","languages:hi","languages:hi_IN","languages:hr","languages:hu","languages:hy","languages:ia","languages:id","languages:ig","languages:io","languages:is","languages:it","languages:it_IT","languages:ja","languages:jbo","languages:ka","languages:kg","languages:kk","languages:km","languages:kn","languages:ko","languages:kr","languages:ks","languages:ku","languages:ky","languages:la","languages:lg","languages:li","languages:lo","languages:lt","languages:lv","languages:mai","languages:mg","languages:mi","languages:mk","languages:ml","languages:mn","languages:mr","languages:ms","languages:ms_MY","languages:mt","languages:mus","languages:my","languages:nb","languages:nb_NO","languages:nds","languages:ne","languages:nhn","languages:nl","languages:nn","languages:nn_NO","languages:n/o","languages:no_nb","languages:nqo","languages:nr","languages:nso","languages:oc","languages:or","languages:os","languages:pa","languages:pl","languages:ps","languages:pt","languages:pt_BR","languages:pt_PT","languages:quz","languages:ro","languages:ru","languages:rw","languages:si","languages:sk","languages:sl","languages:so","languages:sq","languages:sr","languages:sr_ME","languages:st","languages:sv","languages:sw","languages:szl","languages:ta","languages:te","languages:tg","languages:tg_TJ","languages:th","languages:tk","languages:tl","languages:tl_PH","languages:tmp","languages:tr","languages:tr_TR","languages:ts","languages:tt","languages:ug","languages:uk","languages:ur","languages:ur_PK","languages:uz","languages:vi","languages:vi_VN","languages:wa","languages:xh","languages:yi","languages:yo","languages:zh_CN","languages:zh_HK","languages:zh_TW","languages:zu","licenses:unknown","multilinguality:multilingual","size_categories:n<1K","size_categories:1K<n<10K","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"A parallel corpus of GNOME localization files. Source: https://l10n.gnome.org\n\n187 languages, 12,822 bitexts\ntotal number of files: 113,344\ntotal number of tokens: 267.27M\ntotal number of sentence fragments: 58.12M","key":""},{"id":"opus_infopankki","tags":["annotations_creators:found","language_creators:found","languages:ar","languages:en","languages:es","languages:et","languages:fa","languages:fi","languages:fr","languages:ru","languages:so","languages:sv","languages:tr","languages:zh","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J�rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n}","description":"A parallel corpus of 12 languages, 66 bitexts.","key":""},{"id":"opus_memat","tags":["annotations_creators:found","language_creators:found","languages:en","languages:xh","licenses:unknown","multilinguality:translation","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)","description":"Xhosa-English parallel corpora, funded by EPSRC, the Medical Machine Translation project worked on machine translation between ixiXhosa and English, with a focus on the medical domain.","key":""},{"id":"opus_montenegrinsubs","tags":["annotations_creators:found","language_creators:found","languages:en","languages:cnr","licenses:unknown","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)","description":"Opus MontenegrinSubs dataset for machine translation task, for language pair en-me: english and montenegrin","key":""},{"id":"opus_openoffice","tags":["annotations_creators:found","language_creators:found","languages:de","languages:en_GB","languages:es","languages:fr","languages:ja","languages:ru","languages:sv","languages:zh_CN","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J�rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"A collection of documents from http://www.openoffice.org/.","key":""},{"id":"opus_paracrawl","tags":["annotations_creators:found","language_creators:found","languages:bg","languages:ca","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:eu","languages:fi","languages:fr","languages:ga","languages:gl","languages:ha","languages:hr","languages:hu","languages:ig","languages:is","languages:it","languages:km","languages:lt","languages:lv","languages:mt","languages:my","languages:nb","languages:ne","languages:nl","languages:nn","languages:pl","languages:pt","languages:ro","languages:ru","languages:si","languages:sk","languages:sl","languages:so","languages:sv","languages:sw","languages:tl","licenses:cc0-1.0","multilinguality:multilingual","size_categories:100K<n<1M","size_categories:1M<n<10M","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"Parallel corpora from Web Crawls collected in the ParaCrawl project\n40 languages, 41 bitexts\ntotal number of files: 20,995\ntotal number of tokens: 21.40G\ntotal number of sentence fragments: 1.12G","key":""},{"id":"opus_rf","tags":["annotations_creators:found","language_creators:expert-generated","languages:de","languages:en","languages:es","languages:fr","languages:sv","licenses:unknown","multilinguality:multilingual","size_categories:n<1K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"RF is a tiny parallel corpus of the Declarations of the Swedish Government and its translations.","key":""},{"id":"opus_tedtalks","tags":["annotations_creators:found","language_creators:found","languages:en","languages:hr","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"This is a Croatian-English parallel corpus of transcribed and translated TED talks, originally extracted from https://wit3.fbk.eu. The corpus is compiled by Željko Agić and is taken from http://lt.ffzg.hr/zagic provided under the CC-BY-NC-SA license.\n2 languages, total number of files: 2\ntotal number of tokens: 2.81M\ntotal number of sentence fragments: 0.17M","key":""},{"id":"opus_ubuntu","tags":["annotations_creators:found","language_creators:found","languages:ace","languages:af","languages:ak","languages:am","languages:an","languages:ang","languages:ar","languages:ar_SY","languages:ary","languages:as","languages:ast","languages:az","languages:ba","languages:bal","languages:be","languages:bem","languages:ber","languages:bg","languages:bho","languages:bn","languages:bn_IN","languages:bo","languages:br","languages:brx","languages:bs","languages:bua","languages:byn","languages:ca","languages:ce","languages:ceb","languages:chr","languages:ckb","languages:co","languages:crh","languages:cs","languages:csb","languages:cv","languages:cy","languages:da","languages:de","languages:de_AT","languages:de_DE","languages:dsb","languages:dv","languages:dz","languages:el","languages:en","languages:en_AU","languages:en_CA","languages:en_GB","languages:en_NZ","languages:en_US","languages:eo","languages:es","languages:es_AR","languages:es_CL","languages:es_CO","languages:es_CR","languages:es_DO","languages:es_EC","languages:es_ES","languages:es_GT","languages:es_HN","languages:es_MX","languages:es_NI","languages:es_PA","languages:es_PE","languages:es_PR","languages:es_SV","languages:es_UY","languages:es_VE","languages:et","languages:eu","languages:fa","languages:fa_AF","languages:ff","languages:fi","languages:fil","languages:fo","languages:fr","languages:fr_CA","languages:fr_FR","languages:frm","languages:frp","languages:fur","languages:fy","languages:ga","languages:gd","languages:gl","languages:gn","languages:grc","languages:gu","languages:guc","languages:gv","languages:ha","languages:haw","languages:he","languages:hi","languages:hil","languages:hne","languages:hr","languages:hsb","languages:ht","languages:hu","languages:hy","languages:ia","languages:id","languages:ig","languages:io","languages:is","languages:it","languages:iu","languages:ja","languages:jbo","languages:jv","languages:ka","languages:kab","languages:kg","languages:kk","languages:kl","languages:km","languages:kn","languages:ko","languages:kok","languages:ks","languages:ksh","languages:ku","languages:kw","languages:ky","languages:la","languages:lb","languages:lg","languages:li","languages:lij","languages:lld","languages:ln","languages:lo","languages:lt","languages:ltg","languages:lv","languages:mai","languages:mg","languages:mh","languages:mhr","languages:mi","languages:miq","languages:mk","languages:ml","languages:mn","languages:mo","languages:mr","languages:ms","languages:mt","languages:mus","languages:my","languages:nan","languages:nap","languages:nb","languages:nds","languages:ne","languages:nhn","languages:nl","languages:nl_NL","languages:nn","languages:n/o","languages:nso","languages:ny","languages:oc","languages:oj","languages:om","languages:or","languages:os","languages:pa","languages:pam","languages:pap","languages:pl","languages:pms","languages:pmy","languages:ps","languages:pt","languages:pt_BR","languages:pt_PT","languages:qu","languages:rm","languages:ro","languages:rom","languages:ru","languages:rw","languages:sa","languages:sc","languages:sco","languages:sd","languages:se","languages:shn","languages:shs","languages:si","languages:sk","languages:sl","languages:sm","languages:sml","languages:sn","languages:so","languages:son","languages:sq","languages:sr","languages:st","languages:sv","languages:sw","languages:syr","languages:szl","languages:ta","languages:ta_LK","languages:te","languages:tet","languages:tg","languages:th","languages:ti","languages:tk","languages:tl","languages:tlh","languages:tr","languages:trv","languages:ts","languages:tt","languages:ug","languages:uk","languages:ur","languages:uz","languages:ve","languages:vec","languages:vi","languages:wa","languages:wae","languages:wo","languages:xal","languages:xh","languages:yi","languages:yo","languages:zh","languages:zh_CN","languages:zh_HK","languages:zh_TW","languages:zu","languages:zza","licenses:unknown","multilinguality:multilingual","size_categories:1K<n<10K","size_categories:n<1K","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"A parallel corpus of Ubuntu localization files. Source: https://translations.launchpad.net\n244 languages, 23,988 bitexts\ntotal number of files: 30,959\ntotal number of tokens: 29.84M\ntotal number of sentence fragments: 7.73M","key":""},{"id":"opus_wikipedia","tags":["annotations_creators:found","language_creators:found","languages:ar","languages:bg","languages:cs","languages:de","languages:el","languages:en","languages:es","languages:fa","languages:fr","languages:he","languages:hu","languages:it","languages:nl","languages:pl","languages:pt","languages:ro","languages:ru","languages:sl","languages:tr","languages:vi","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"This is a corpus of parallel sentences extracted from Wikipedia by Krzysztof Wołk and Krzysztof Marasek. Please cite the following publication if you use the data: Krzysztof Wołk and Krzysztof Marasek: Building Subject-aligned Comparable Corpora and Mining it for Truly Parallel Sentence Pairs., Procedia Technology, 18, Elsevier, p.126-132, 2014\n20 languages, 36 bitexts\ntotal number of files: 114\ntotal number of tokens: 610.13M\ntotal number of sentence fragments: 25.90M","key":""},{"id":"opus_xhosanavy","tags":["annotations_creators:found","language_creators:found","languages:en","languages:xh","licenses:unknown","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)","description":"This dataset is designed for machine translation from English to Xhosa.","key":""},{"id":"orange_sum","tags":["annotations_creators:found","language_creators:found","languages:fr","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@article{eddine2020barthez,\n  title={BARThez: a Skilled Pretrained French Sequence-to-Sequence Model},\n  author={Eddine, Moussa Kamal and Tixier, Antoine J-P and Vazirgiannis, Michalis},\n  journal={arXiv preprint arXiv:2010.12321},\n  year={2020}\n}","description":"The OrangeSum dataset was inspired by the XSum dataset. It was created by scraping the \"Orange Actu\" website: https://actu.orange.fr/. Orange S.A. is a large French multinational telecommunications corporation, with 266M customers worldwide. Scraped pages cover almost a decade from Feb 2011 to Sep 2020. They belong to five main categories: France, world, politics, automotive, and society. The society category is itself divided into 8 subcategories: health, environment, people, culture, media, high-tech, unsual (\"insolite\" in French), and miscellaneous.\n\nEach article featured a single-sentence title as well as a very brief abstract, both professionally written by the author of the article. These two fields were extracted from each page, thus creating two summarization tasks: OrangeSum Title and OrangeSum Abstract.","paperswithcode_id":"orangesum","key":""},{"id":"oscar","tags":["pretty_name:OSCAR","annotations_creators:no-annotation","language_creators:found","languages:af","languages:als","languages:am","languages:an","languages:ar","languages:arz","languages:as","languages:ast","languages:av","languages:az","languages:azb","languages:ba","languages:bar","languages:bcl","languages:be","languages:bg","languages:bh","languages:bn","languages:bo","languages:bpy","languages:br","languages:bs","languages:bxr","languages:ca","languages:cbk","languages:ce","languages:ceb","languages:ckb","languages:cs","languages:cv","languages:cy","languages:da","languages:de","languages:diq","languages:dsb","languages:dv","languages:el","languages:eml","languages:en","languages:eo","languages:es","languages:et","languages:eu","languages:fa","languages:fi","languages:fr","languages:frr","languages:fy","languages:ga","languages:gd","languages:gl","languages:gn","languages:gom","languages:gu","languages:he","languages:hi","languages:hr","languages:hsb","languages:ht","languages:hu","languages:hy","languages:ia","languages:id","languages:ie","languages:ilo","languages:io","languages:is","languages:it","languages:ja","languages:jbo","languages:jv","languages:ka","languages:kk","languages:km","languages:kn","languages:ko","languages:krc","languages:ku","languages:kv","languages:kw","languages:ky","languages:la","languages:lb","languages:lez","languages:li","languages:lmo","languages:lo","languages:lrc","languages:lt","languages:lv","languages:mai","languages:mg","languages:mhr","languages:min","languages:mk","languages:ml","languages:mn","languages:mr","languages:mrj","languages:ms","languages:mt","languages:mwl","languages:my","languages:myv","languages:mzn","languages:nah","languages:nap","languages:nds","languages:ne","languages:new","languages:nl","languages:nn","languages:no","languages:oc","languages:or","languages:os","languages:pa","languages:pam","languages:pl","languages:pms","languages:pnb","languages:ps","languages:pt","languages:qu","languages:rm","languages:ro","languages:ru","languages:sa","languages:sah","languages:scn","languages:sd","languages:sh","languages:si","languages:sk","languages:sl","languages:so","languages:sq","languages:sr","languages:su","languages:sv","languages:sw","languages:ta","languages:te","languages:tg","languages:th","languages:tk","languages:tl","languages:tr","languages:tt","languages:tyv","languages:ug","languages:uk","languages:ur","languages:uz","languages:vec","languages:vi","languages:vo","languages:wa","languages:war","languages:wuu","languages:xal","languages:xmf","languages:yi","languages:yo","languages:yue","languages:zh","licenses:cc0-1.0","multilinguality:multilingual","size_categories:100K<n<1M","size_categories:1K<n<10K","size_categories:10K<n<100K","size_categories:1M<n<10M","size_categories:n<1K","size_categories:10M<n<100M","size_categories:100M<n<1B","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@inproceedings{ortiz-suarez-etal-2020-monolingual,\n    title = \"A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages\",\n    author = \"Ortiz Su{\\'a}rez, Pedro Javier  and\n      Romary, Laurent  and\n      Sagot, Benoit\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.156\",\n    pages = \"1703--1714\",\n    abstract = \"We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.\",\n}\n\n@inproceedings{OrtizSuarezSagotRomary2019,\n  author    = {Pedro Javier {Ortiz Su{\\'a}rez} and Benoit Sagot and Laurent Romary},\n  title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},\n  series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},\n  editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{\\\"u}ngen and Caroline Iliadi},\n  publisher = {Leibniz-Institut f{\\\"u}r Deutsche Sprache},\n  address   = {Mannheim},\n  doi       = {10.14618/ids-pub-9021},\n  url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},\n  pages     = {9 -- 16},\n  year      = {2019},\n  abstract  = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},\n  language  = {en}\n}","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","paperswithcode_id":"oscar","key":""},{"id":"para_crawl","tags":[],"citation":"@misc {paracrawl,\n    title  = {ParaCrawl},\n    year   = {2018},\n    url    = {http://paracrawl.eu/download.html.}\n}","paperswithcode_id":"paracrawl","key":""},{"id":"para_pat","tags":["annotations_creators:machine-generated","language_creators:expert-generated","languages:cs","languages:de","languages:el","languages:en","languages:es","languages:fr","languages:hu","languages:ja","languages:ko","languages:pt","languages:ro","languages:ru","languages:sk","languages:uk","languages:zh","licenses:cc-by-4.0","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@inproceedings{soares-etal-2020-parapat,\n    title = \"{P}ara{P}at: The Multi-Million Sentences Parallel Corpus of Patents Abstracts\",\n    author = \"Soares, Felipe  and\n      Stevenson, Mark  and\n      Bartolome, Diego  and\n      Zaretskaya, Anna\",\n    booktitle = \"Proceedings of The 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.465\",\n    pages = \"3769--3774\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}","description":"ParaPat: The Multi-Million Sentences Parallel Corpus of Patents Abstracts\n\nThis dataset contains the developed parallel corpus from the open access Google\nPatents dataset in 74 language pairs, comprising more than 68 million sentences\nand 800 million tokens. Sentences were automatically aligned using the Hunalign algorithm\nfor the largest 22 language pairs, while the others were abstract (i.e. paragraph) aligned.","paperswithcode_id":"parapat","key":""},{"id":"parsinlu_reading_comprehension","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:fa","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|wikipedia|google","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@article{huggingface:dataset,\n    title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n    authors = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and others},\n    year={2020}\n    journal = {arXiv e-prints},\n    eprint = {2012.06154},\n}","description":"A Persian reading comprehenion task (generating an answer, given a question and a context paragraph).\nThe questions are mined using Google auto-complete, their answers and the corresponding evidence documents are manually annotated by native speakers.","key":""},{"id":"paws","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:machine-generated","languages:en","licenses:other","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_categories:text-scoring","task_ids:semantic-similarity-classification","task_ids:semantic-similarity-scoring","task_ids:text-scoring-other-paraphrase-identification"],"citation":"@InProceedings{paws2019naacl,\n  title = {{PAWS: Paraphrase Adversaries from Word Scrambling}},\n  author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},\n  booktitle = {Proc. of NAACL},\n  year = {2019}\n}","description":"PAWS: Paraphrase Adversaries from Word Scrambling\n\nThis dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature\nthe importance of modeling structure, context, and word order information for the problem\nof paraphrase identification. The dataset has two subsets, one based on Wikipedia and the\nother one based on the Quora Question Pairs (QQP) dataset.\n\nFor further details, see the accompanying paper: PAWS: Paraphrase Adversaries from Word Scrambling\n(https://arxiv.org/abs/1904.01130)\n\nPAWS-QQP is not available due to license of QQP. It must be reconstructed by downloading the original\ndata and then running our scripts to produce the data and attach the labels.\n\nNOTE: There might be some missing or wrong labels in the dataset and we have replaced them with -1.","paperswithcode_id":"paws","key":""},{"id":"paws-x","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:expert-generated","language_creators:machine-generated","languages:de","languages:en","languages:es","languages:fr","languages:ja","languages:ko","languages:zh","licenses:other","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:extended|other-paws","task_categories:text-classification","task_categories:text-scoring","task_ids:semantic-similarity-classification","task_ids:semantic-similarity-scoring","task_ids:text-scoring-other-paraphrase-identification"],"citation":"@InProceedings{pawsx2019emnlp,\n  title = {{PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification}},\n  author = {Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},\n  booktitle = {Proc. of EMNLP},\n  year = {2019}\n}","description":"PAWS-X, a multilingual version of PAWS (Paraphrase Adversaries from Word Scrambling) for six languages.\n\nThis dataset contains 23,659 human translated PAWS evaluation pairs and 296,406 machine\ntranslated training pairs in six typologically distinct languages: French, Spanish, German,\nChinese, Japanese, and Korean. English language is available by default. All translated\npairs are sourced from examples in PAWS-Wiki.\n\nFor further details, see the accompanying paper: PAWS-X: A Cross-lingual Adversarial Dataset\nfor Paraphrase Identification (https://arxiv.org/abs/1908.11828)\n\nNOTE: There might be some missing or wrong labels in the dataset and we have replaced them with -1.","paperswithcode_id":"paws-x","key":""},{"id":"pec","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:gpl-3.0+","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_categories:text-retrieval","task_ids:dialogue-modeling","task_ids:utterance-retrieval"],"citation":"\\\r\n@inproceedings{zhong2020towards,\r\n    title = \"Towards Persona-Based Empathetic Conversational Models\",\r\n    author = \"Zhong, Peixiang  and\r\n      Zhang, Chen  and\r\n      Wang, Hao  and\r\n      Liu, Yong  and\r\n      Miao, Chunyan\",\r\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\r\n    year = \"2020\",\r\n    publisher = \"Association for Computational Linguistics\",\r\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.531\",\r\n    pages = \"6556--6566\"}","description":"\\\r\nA dataset of around 350K persona-based empathetic conversations. Each speaker is associated with a persona, which comprises multiple persona sentences. The response of each conversation is empathetic.","paperswithcode_id":"pec","key":""},{"id":"peer_read","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-acceptability-classification"],"citation":"@inproceedings{kang18naacl,\n  title = {A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications},\n  author = {Dongyeop Kang and Waleed Ammar and Bhavana Dalvi and Madeleine van Zuylen and Sebastian Kohlmeier and Eduard Hovy and Roy Schwartz},\n  booktitle = {Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)},\n  address = {New Orleans, USA},\n  month = {June},\n  url = {https://arxiv.org/abs/1804.09635},\n  year = {2018}\n}","description":"PearRead is a dataset of scientific peer reviews available to help researchers study this important artifact. The dataset consists of over 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR, as well as over 10K textual peer reviews written by experts for a subset of the papers.","paperswithcode_id":"peerread","key":""},{"id":"peoples_daily_ner","tags":["annotations_creators:expert-generated","language_creators:found","languages:zh","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"description":"People's Daily NER Dataset is a commonly used dataset for Chinese NER, with\ntext from People's Daily (人民日报), the largest official newspaper.\n\nThe dataset is in BIO scheme. Entity types are: PER (person), ORG (organization)\nand LOC (location).","key":""},{"id":"per_sent","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-MPQA-KBP Challenge-MediaRank","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{bastan2020authors,\n      title={Author's Sentiment Prediction},\n      author={Mohaddeseh Bastan and Mahnaz Koupaee and Youngseo Son and Richard Sicoli and Niranjan Balasubramanian},\n      year={2020},\n      eprint={2011.06128},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Person SenTiment (PerSenT) is a crowd-sourced dataset that captures the sentiment of an author towards the main entity in a news article. This dataset contains annotation for 5.3k documents and 38k paragraphs covering 3.2k unique entities.\n\nThe dataset consists of sentiment annotations on news articles about people. For each article, annotators judge what the author’s sentiment is towards the main (target) entity of the article. The annotations also include similar judgments on paragraphs within the article.\n\nTo split the dataset, entities into 4 mutually exclusive sets. Due to the nature of news collections, some entities tend to dominate the collection. In the collection, there were four entities which were the main entity in nearly 800 articles. To avoid these entities from dominating the train or test splits, we moved them to a separate test collection. We split the remaining into a training, dev, and test sets at random. Thus our collection includes one standard test set consisting of articles drawn at random (Test Standard -- `test_random`), while the other is a test set which contains multiple articles about a small number of popular entities (Test Frequent -- `test_fixed`).","paperswithcode_id":"persent","key":""},{"id":"persian_ner","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:fa","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{poostchi-etal-2016-personer,\n    title = \"{P}erso{NER}: {P}ersian Named-Entity Recognition\",\n    author = \"Poostchi, Hanieh  and\n      Zare Borzeshi, Ehsan  and\n      Abdous, Mohammad  and\n      Piccardi, Massimo\",\n    booktitle = \"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers\",\n    month = dec,\n    year = \"2016\",\n    address = \"Osaka, Japan\",\n    publisher = \"The COLING 2016 Organizing Committee\",\n    url = \"https://www.aclweb.org/anthology/C16-1319\",\n    pages = \"3381--3389\",\n    abstract = \"Named-Entity Recognition (NER) is still a challenging task for languages with low digital resources. The main difficulties arise from the scarcity of annotated corpora and the consequent problematic training of an effective NER pipeline. To abridge this gap, in this paper we target the Persian language that is spoken by a population of over a hundred million people world-wide. We first present and provide ArmanPerosNERCorpus, the first manually-annotated Persian NER corpus. Then, we introduce PersoNER, an NER pipeline for Persian that leverages a word embedding and a sequential max-margin classifier. The experimental results show that the proposed approach is capable of achieving interesting MUC7 and CoNNL scores while outperforming two alternatives based on a CRF and a recurrent neural network.\",\n}","description":"The dataset includes 250,015 tokens and 7,682 Persian sentences in total. It is available in 3 folds to be used in turn as training and test sets. The NER tags are in IOB format.","key":""},{"id":"pg19","tags":["languages:en"],"citation":"@article{raecompressive2019,\n  author = {Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and\n            Hillier, Chloe and Lillicrap, Timothy P},\n  title = {Compressive Transformers for Long-Range Sequence Modelling},\n  journal = {arXiv preprint},\n  url = {https://arxiv.org/abs/1911.05507},\n  year = {2019},\n}","description":"This repository contains the PG-19 language modeling benchmark.\nIt includes a set of books extracted from the Project Gutenberg books library, that were published before 1919.\nIt also contains metadata of book titles and publication dates.\n\nPG-19 is over double the size of the Billion Word benchmark and contains documents that are 20X longer, on average, than the WikiText long-range language modelling benchmark.\nBooks are partitioned into a train, validation, and test set. Book metadata is stored in metadata.csv which contains (book_id, short_book_title, publication_date).\n\nUnlike prior benchmarks, we do not constrain the vocabulary size --- i.e. mapping rare words to an UNK token --- but instead release the data as an open-vocabulary benchmark. The only processing of the text that has been applied is the removal of boilerplate license text, and the mapping of offensive discriminatory words as specified by Ofcom to placeholder tokens. Users are free to model the data at the character-level, subword-level, or via any mechanism that can model an arbitrary string of text.\nTo compare models we propose to continue measuring the word-level perplexity, by calculating the total likelihood of the dataset (via any chosen subword vocabulary or character-based scheme) divided by the number of tokens --- specified below in the dataset statistics table.\nOne could use this dataset for benchmarking long-range language models, or use it to pre-train for other natural language processing tasks which require long-range reasoning, such as LAMBADA or NarrativeQA. We would not recommend using this dataset to train a general-purpose language model, e.g. for applications to a production-system dialogue agent, due to the dated linguistic style of old texts and the inherent biases present in historical writing.","paperswithcode_id":"pg-19","key":""},{"id":"php","tags":["annotations_creators:found","language_creators:found","languages:cs","languages:de","languages:en","languages:es","languages:fi","languages:fr","languages:he","languages:hu","languages:it","languages:ja","languages:ko","languages:nl","languages:pl","languages:pt_BR","languages:ro","languages:ru","languages:sk","languages:sl","languages:sv","languages:tr","languages:tw","languages:zh","languages:zh_TW","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"A parallel corpus originally extracted from http://se.php.net/download-docs.php. The original documents are written in English and have been partly translated into 21 languages. The original manuals contain about 500,000 words. The amount of actually translated texts varies for different languages between 50,000 and 380,000 words. The corpus is rather noisy and may include parts from the English original in some of the translations. The corpus is tokenized and each language pair has been sentence aligned.\n\n23 languages, 252 bitexts\ntotal number of files: 71,414\ntotal number of tokens: 3.28M\ntotal number of sentence fragments: 1.38M","key":""},{"id":"piaf","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:fr-FR","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa","task_ids:open-domain-qa"],"citation":"@InProceedings{keraron-EtAl:2020:LREC,\n  author    = {Keraron, Rachel  and  Lancrenon, Guillaume  and  Bras, Mathilde  and  Allary, Frédéric  and  Moyse, Gilles  and  Scialom, Thomas  and  Soriano-Morales, Edmundo-Pavel  and  Staiano, Jacopo},\n  title     = {Project PIAF: Building a Native French Question-Answering Dataset},\n  booktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},\n  month          = {May},\n  year           = {2020},\n  address        = {Marseille, France},\n  publisher      = {European Language Resources Association},\n  pages     = {5483--5492},\n  abstract  = {Motivated by the lack of data for non-English languages, in particular for the evaluation of downstream tasks such as Question Answering, we present a participatory effort to collect a native French Question Answering Dataset. Furthermore, we describe and publicly release the annotation tool developed for our collection effort, along with the data obtained and preliminary baselines.},\n  url       = {https://www.aclweb.org/anthology/2020.lrec-1.673}\n}","description":"Piaf is a reading comprehension dataset. This version, published in February 2020, contains 3835 questions on French Wikipedia.","key":""},{"id":"pib","tags":["task_categories:sequence-modeling","task_ids:language-modeling","multilinguality:translation","languages:en","languages:hi","languages:ta","languages:te","languages:ml","languages:ur","languages:bn","languages:mr","languages:gu","languages:or","languages:pa","language_creators:other","annotations_creators:no-annotation","source_datasets:original","size_categories:10K<n<100K","size_categories:1K<n<10K","size_categories:n<1K","size_categories:100K<n<1M","licenses:cc-by-4.0"],"citation":"@InProceedings{cvit-pib:multilingual-corpus,\ntitle = {Revisiting Low Resource Status of Indian Languages in Machine Translation},\nauthors={Jerin Philip, Shashank Siripragada, Vinay P. Namboodiri, C.V. Jawahar\n},\nyear={2020}\n}","description":"This new dataset is the large scale sentence aligned corpus in 11 Indian languages,\nviz. CVIT-PIB corpus that is the largest multilingual corpus available for Indian languages.","key":""},{"id":"piqa","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@inproceedings{Bisk2020,\n  author = {Yonatan Bisk and Rowan Zellers and\n            Ronan Le Bras and Jianfeng Gao\n            and Yejin Choi},\n  title = {PIQA: Reasoning about Physical Commonsense in\n           Natural Language},\n  booktitle = {Thirty-Fourth AAAI Conference on\n               Artificial Intelligence},\n  year = {2020},\n}","description":"To apply eyeshadow without a brush, should I use a cotton swab or a toothpick?\nQuestions requiring this kind of physical commonsense pose a challenge to state-of-the-art\nnatural language understanding systems. The PIQA dataset introduces the task of physical commonsense reasoning\nand a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA.\n\nPhysical commonsense knowledge is a major challenge on the road to true AI-completeness,\nincluding robots that interact with the world and understand natural language.\n\nPIQA focuses on everyday situations with a preference for atypical solutions.\nThe dataset is inspired by instructables.com, which provides users with instructions on how to build, craft,\nbake, or manipulate objects using everyday materials.\n\nThe underlying task is formualted as multiple choice question answering:\ngiven a question `q` and two possible solutions `s1`, `s2`, a model or\na human must choose the most appropriate solution, of which exactly one is correct.\nThe dataset is further cleaned of basic artifacts using the AFLite algorithm which is an improvement of\nadversarial filtering. The dataset contains 16,000 examples for training, 2,000 for development and 3,000 for testing.","paperswithcode_id":"piqa","key":""},{"id":"pn_summary","tags":["annotations_creators:found","language_creators:found","languages:fa","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-classification","task_ids:summarization","task_ids:text-simplification","task_ids:topic-classification"],"citation":"@article{pnSummary, title={Leveraging ParsBERT and Pretrained mT5 for Persian Abstractive Text Summarization},\nauthor={Mehrdad Farahani, Mohammad Gharachorloo, Mohammad Manthouri},\nyear={2020},\neprint={2012.11204},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}","description":"A well-structured summarization dataset for the Persian language consists of 93,207 records. It is prepared for Abstractive/Extractive tasks (like cnn_dailymail for English). It can also be used in other scopes like Text Generation, Title Generation, and News Category Classification.\nIt is imperative to consider that the newlines were replaced with the `[n]` symbol. Please interpret them into normal newlines (for ex. `t.replace(\"[n]\", \"\\n\")`) and then use them for your purposes.","paperswithcode_id":"pn-summary","key":""},{"id":"poem_sentiment","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@misc{sheng2020investigating,\n      title={Investigating Societal Biases in a Poetry Composition System},\n      author={Emily Sheng and David Uthus},\n      year={2020},\n      eprint={2011.02686},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Poem Sentiment is a sentiment dataset of poem verses from Project Gutenberg. This dataset can be used for tasks such as sentiment classification or style transfer for poems.","paperswithcode_id":"gutenberg-poem-dataset","key":""},{"id":"polemo2","tags":["annotations_creators:expert-generated","language_creators:other","languages:pl","licenses:bsd-3-clause","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{kocon-etal-2019-multi,\ntitle = \"Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: Extended Corpus of Multi-Domain Consumer Reviews\",\nauthor = \"Koco{\\'n}, Jan and\nMilkowski, Piotr and\nZa{\\'s}ko-Zieli{\\'n}ska, Monika\",\nbooktitle = \"Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)\",\nmonth = nov,\nyear = \"2019\",\naddress = \"Hong Kong, China\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/K19-1092\",\ndoi = \"10.18653/v1/K19-1092\",\npages = \"980--991\",\n}","description":"The PolEmo2.0 is a set of online reviews from medicine and hotels domains. The task is to predict the sentiment of a review. There are two separate test sets, to allow for in-domain (medicine and hotels) as well as out-of-domain (products and university) validation.","key":""},{"id":"poleval2019_cyberbullying","tags":["annotations_creators:found","language_creators:found","languages:pl","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification"],"citation":"@proceedings{ogr:kob:19:poleval,\n  editor    = {Maciej Ogrodniczuk and Łukasz Kobyliński},\n  title     = {{Proceedings of the PolEval 2019 Workshop}},\n  year      = {2019},\n  address   = {Warsaw, Poland},\n  publisher = {Institute of Computer Science, Polish Academy of Sciences},\n  url       = {http://2019.poleval.pl/files/poleval2019.pdf},\n  isbn      = \"978-83-63159-28-3\"}\n}","description":"    In Task 6-1, the participants are to distinguish between normal/non-harmful tweets (class: 0) and tweets\n    that contain any kind of harmful information (class: 1). This includes cyberbullying, hate speech and\n    related phenomena.\n\n    In Task 6-2, the participants shall distinguish between three classes of tweets: 0 (non-harmful),\n    1 (cyberbullying), 2 (hate-speech). There are various definitions of both cyberbullying and hate-speech,\n    some of them even putting those two phenomena in the same group. The specific conditions on which we based\n    our annotations for both cyberbullying and hate-speech, which have been worked out during ten years of research\n    will be summarized in an introductory paper for the task, however, the main and definitive condition to 1\n    distinguish the two is whether the harmful action is addressed towards a private person(s) (cyberbullying),\n    or a public person/entity/large group (hate-speech).","key":""},{"id":"poleval2019_mt","tags":["annotations_creators:no-annotation","language_creators:expert-generated","language_creators:found","languages:en","languages:pl","languages:ru","licenses:unknown","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"description":"PolEval is a SemEval-inspired evaluation campaign for natural language processing tools for Polish.Submitted solutions compete against one another within certain tasks selected by organizers, using available data and are evaluated according topre-established procedures. One of the tasks in PolEval-2019 was Machine Translation (Task-4).\nThe task is to train as good as possible machine translation system, using any technology,with limited textual resources.The competition will be done for 2 language pairs, more popular English-Polish (into Polish direction) and pair that can be called low resourcedRussian-Polish (in both directions).\n\nHere, Polish-English is also made available to allow for training in both directions. However, the test data is ONLY available for English-Polish.","key":""},{"id":"polsum","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:pl","licenses:cc-by-3.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@inproceedings{\n    ogro:kop:14:lrec,\n    author = \"Ogrodniczuk, Maciej and Kopeć, Mateusz\",\n    pdf = \"http://nlp.ipipan.waw.pl/Bib/ogro:kop:14:lrec.pdf\",\n    title = \"The {P}olish {S}ummaries {C}orpus\",\n    pages = \"3712--3715\",\n    crossref = \"lrec:14\"\n}\n@proceedings{\n    lrec:14,\n    editor = \"Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios\",\n    isbn = \"978-2-9517408-8-4\",\n    title = \"Proceedings of the Ninth International {C}onference on {L}anguage {R}esources and {E}valuation, {LREC}~2014\",\n    url = \"http://www.lrec-conf.org/proceedings/lrec2014/index.html\",\n    booktitle = \"Proceedings of the Ninth International {C}onference on {L}anguage {R}esources and {E}valuation, {LREC}~2014\",\n    address = \"Reykjavík, Iceland\",\n    key = \"LREC\",\n    year = \"2014\",\n    organization = \"European Language Resources Association (ELRA)\"\n}","description":"Polish Summaries Corpus: the corpus of Polish news summaries.","key":""},{"id":"polyglot_ner","tags":[],"citation":"@article{polyglotner,\n         author = {Al-Rfou, Rami and Kulkarni, Vivek and Perozzi, Bryan and Skiena, Steven},\n         title = {{Polyglot-NER}: Massive Multilingual Named Entity Recognition},\n         journal = {{Proceedings of the 2015 {SIAM} International Conference on Data Mining, Vancouver, British Columbia, Canada, April 30- May 2, 2015}},\n         month     = {April},\n         year      = {2015},\n         publisher = {SIAM},\n}","description":"Polyglot-NER\nA training dataset automatically generated from Wikipedia and Freebase the task\nof named entity recognition. The dataset contains the basic Wikipedia based\ntraining data for 40 languages we have (with coreference resolution) for the task of\nnamed entity recognition. The details of the procedure of generating them is outlined in\nSection 3 of the paper (https://arxiv.org/abs/1410.3791). Each config contains the data\ncorresponding to a different language. For example, \"es\" includes only spanish examples.","paperswithcode_id":"polyglot-ner","key":""},{"id":"prachathai67k","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:topic-classification"],"citation":"@misc{prachathai67k,\n  author = {cstorm125, lukkiddd },\n  title = {prachathai67k},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished={\\\\url{https://github.com/PyThaiNLP/prachathai-67k}},\n}","description":"`prachathai-67k`: News Article Corpus and Multi-label Text Classificdation from Prachathai.com\nThe prachathai-67k dataset was scraped from the news site Prachathai.\nWe filtered out those articles with less than 500 characters of body text, mostly images and cartoons.\nIt contains 67,889 articles wtih 12 curated tags from August 24, 2004 to November 15, 2018.\nThe dataset was originally scraped by @lukkiddd and cleaned by @cstorm125.\nYou can also see preliminary exploration at https://github.com/PyThaiNLP/prachathai-67k/blob/master/exploration.ipynb","paperswithcode_id":"prachathai-67k","key":""},{"id":"pragmeval","tags":["languages:en","multilinguality:monolingual","size_categories:1K<n<10K","size_categories:10K<n<100K","size_categories:n<1K","task_categories:text-classification","task_ids:multi-class-classification"],"citation":"@misc{sileo2019discoursebased,\n      title={Discourse-Based Evaluation of Language Understanding},\n      author={Damien Sileo and Tim Van-de-Cruys and Camille Pradel and Philippe Muller},\n      year={2019},\n      eprint={1907.08672},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Evaluation of language understanding with a 11 datasets benchmark focusing on discourse and pragmatics","key":""},{"id":"proto_qa","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:other","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa","task_ids:open-domain-qa"],"citation":"@InProceedings{huggingface:dataset,\ntitle = {ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning},\nauthors={Michael Boratko, Xiang Lorraine Li, Tim O’Gorman, Rajarshi Das, Dan Le, Andrew McCallum},\nyear={2020},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished={\\\\url{https://github.com/iesl/protoqa-data}},\n}","description":"This dataset is for studying computational models trained to reason about prototypical situations. Using deterministic filtering a sampling from a larger set of all transcriptions was built. It contains 9789 instances where each instance represents a survey question from Family Feud game. Each instance exactly is a question, a set of answers, and a count associated with each answer.\nEach line is a json dictionary, in which:\n1. question - contains the question (in original and a normalized form)\n2. answerstrings - contains the original answers provided by survey respondents (when available), along with the counts for each string. Because the FamilyFeud data has only cluster names rather than strings, those cluster names are included with 0 weight.\n3. answer-clusters - lists clusters, with the count of each cluster and the strings included in that cluster. Each cluster is given a unique ID that can be linked to in the assessment files.","paperswithcode_id":"protoqa","key":""},{"id":"psc","tags":["annotations_creators:expert-generated","language_creators:other","languages:pl","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@inproceedings{ogro:kop:14:lrec,\ntitle={The {P}olish {S}ummaries {C}orpus},\nauthor={Ogrodniczuk, Maciej and Kope{\\'c}, Mateusz},\nbooktitle = \"Proceedings of the Ninth International {C}onference on {L}anguage {R}esources and {E}valuation, {LREC}~2014\",\nyear = \"2014\",\n}","description":"The Polish Summaries Corpus contains news articles and their summaries. We used summaries of the same article as positive pairs and sampled the most similar summaries of different articles as negatives.","key":""},{"id":"ptb_text_only","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:other-LDC User Agreement for Non-Members","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@article{marcus-etal-1993-building,\n    title = \"Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank\",\n    author = \"Marcus, Mitchell P.  and\n      Santorini, Beatrice  and\n      Marcinkiewicz, Mary Ann\",\n    journal = \"Computational Linguistics\",\n    volume = \"19\",\n    number = \"2\",\n    year = \"1993\",\n    url = \"https://www.aclweb.org/anthology/J93-2004\",\n    pages = \"313--330\",\n}","description":"This is the Penn Treebank Project: Release 2 CDROM, featuring a million words of 1989 Wall Street Journal material. This corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure.","key":""},{"id":"pubmed","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:other-nlm-license","multilinguality:monolingual","size_categories:10M<n<100M","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_categories:text-classification","task_categories:text-scoring","task_ids:language-modeling","task_ids:other-stuctured-to-text","task_ids:text-scoring-other-citation-estimation","task_ids:topic-classification"],"citation":"\\","description":"NLM produces a baseline set of MEDLINE/PubMed citation records in XML format for download on an annual basis. The annual baseline is released in December of each year. Each day, NLM produces update files that include new, revised and deleted citations. See our documentation page for more information.","paperswithcode_id":"pubmed","key":""},{"id":"pubmed_qa","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:expert-generated","languages:en","licenses:mit","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:1K<n<10K","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@inproceedings{jin2019pubmedqa,\n  title={PubMedQA: A Dataset for Biomedical Research Question Answering},\n  author={Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William and Lu, Xinghua},\n  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},\n  pages={2567--2577},\n  year={2019}\n}","description":"PubMedQA is a novel biomedical question answering (QA) dataset collected from PubMed abstracts.\nThe task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative\nstatins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts.\nPubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances.\nEach PubMedQA instance is composed of (1) a question which is either an existing research article\ntitle or derived from one, (2) a context which is the corresponding abstract without its conclusion,\n(3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question,\nand (4) a yes/no/maybe answer which summarizes the conclusion.\nPubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their\nquantitative contents, is required to answer the questions.","paperswithcode_id":"pubmedqa","key":""},{"id":"py_ast","tags":["annotations_creators:machine-generated","language_creators:found","languages:code","licenses:0bsd","licenses:mit","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_ids:sequence-modeling-code-modeling"],"citation":"@InProceedings{OOPSLA ’16, ACM,\ntitle = {Probabilistic Model for Code with Decision Trees.},\nauthors={Raychev, V., Bielik, P., and Vechev, M.},\nyear={2016}\n}","description":"dataset consisting of parsed Parsed ASTs that were used to train and\nevaluate the DeepSyn tool.\nThe Python programs are collected from GitHub repositories\nby removing duplicate files, removing project forks (copy of another existing repository)\n,keeping only programs that parse and have at most 30'000 nodes in the AST and\nwe aim to remove obfuscated files","key":""},{"id":"qa4mre","tags":[],"description":"QA4MRE dataset was created for the CLEF 2011/2012/2013 shared tasks to promote research in\nquestion answering and reading comprehension. The dataset contains a supporting\npassage and a set of questions corresponding to the passage. Multiple options\nfor answers are provided for each question, of which only one is correct. The\ntraining and test datasets are available for the main track.\nAdditional gold standard documents are available for two pilot studies: one on\nalzheimers data, and the other on entrance exams data.","key":""},{"id":"qa_srl","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa","task_ids:open-domain-qa"],"citation":"@InProceedings{huggingface:dataset,\ntitle = {QA-SRL: Question-Answer Driven Semantic Role Labeling},\nauthors={Luheng He, Mike Lewis, Luke Zettlemoyer},\nyear={2015}\npublisher = {cs.washington.edu},\nhowpublished={\\\\url{https://dada.cs.washington.edu/qasrl/#page-top}},\n}","description":"The dataset contains question-answer pairs to model verbal predicate-argument structure. The questions start with wh-words (Who, What, Where, What, etc.) and contain a verb predicate in the sentence; the answers are phrases in the sentence.\nThere were 2 datsets used in the paper, newswire and wikipedia. Unfortunately the newswiredataset is built from CoNLL-2009 English training set that is covered under license\nThus, we are providing only Wikipedia training set here. Please check README.md for more details on newswire dataset.\nFor the Wikipedia domain, randomly sampled sentences from the English Wikipedia (excluding questions and sentences with fewer than 10 or more than 60 words) were taken.\nThis new dataset is designed to solve this great NLP task and is crafted with a lot of care.","paperswithcode_id":"qa-srl","key":""},{"id":"qa_zre","tags":["languages:en"],"citation":"@inproceedings{levy-etal-2017-zero,\n    title = \"Zero-Shot Relation Extraction via Reading Comprehension\",\n    author = \"Levy, Omer  and\n      Seo, Minjoon  and\n      Choi, Eunsol  and\n      Zettlemoyer, Luke\",\n    booktitle = \"Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)\",\n    month = aug,\n    year = \"2017\",\n    address = \"Vancouver, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/K17-1034\",\n    doi = \"10.18653/v1/K17-1034\",\n    pages = \"333--342\",\n}","description":"A dataset reducing relation extraction to simple reading comprehension questions","key":""},{"id":"qangaroo","tags":["languages:en"],"citation":"","description":"  We have created two new Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference.\n\nSeveral pieces of information often jointly imply another fact. In multi-hop inference, a new fact is derived by combining facts via a chain of multiple steps.\n\nOur aim is to build Reading Comprehension methods that perform multi-hop inference on text, where individual facts are spread out across different documents.\n\nThe two QAngaroo datasets provide a training and evaluation resource for such methods.","key":""},{"id":"qanta","tags":["languages:en"],"citation":"@article{Rodriguez2019QuizbowlTC,\n  title={Quizbowl: The Case for Incremental Question Answering},\n  author={Pedro Rodriguez and Shi Feng and Mohit Iyyer and He He and Jordan L. Boyd-Graber},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1904.04792}\n}","description":"The Qanta dataset is a question answering dataset based on the academic trivia game Quizbowl.","paperswithcode_id":"quizbowl","key":""},{"id":"qasc","tags":["languages:en"],"citation":"@article{allenai:qasc,\n      author    = {Tushar Khot and Peter Clark and Michal Guerquin and Peter Jansen and Ashish Sabharwal},\n      title     = {QASC: A Dataset for Question Answering via Sentence Composition},\n      journal   = {arXiv:1910.11473v2},\n      year      = {2020},\n}","description":"QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice\nquestions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.","paperswithcode_id":"qasc","key":""},{"id":"qasper","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en-US","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|s2orc","task_categories:question-answering","task_ids:closed-domain-qa"],"citation":"@inproceedings{Dasigi2021ADO,\n  title={A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},\n  author={Pradeep Dasigi and Kyle Lo and Iz Beltagy and Arman Cohan and Noah A. Smith and Matt Gardner},\n  year={2021}\n}","description":"A dataset containing 1585 papers with 5049 information-seeking questions asked by regular readers of NLP papers, and answered by a separate set of NLP practitioners.","key":""},{"id":"qed","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|natural_questions","task_categories:question-answering","task_ids:extractive-qa","task_ids:question-answering-other-explanations-in-question-answering"],"citation":"@misc{lamm2020qed,\n    title={QED: A Framework and Dataset for Explanations in Question Answering},\n    author={Matthew Lamm and Jennimaria Palomaki and Chris Alberti and Daniel Andor and Eunsol Choi and Livio Baldini Soares and Michael Collins},\n    year={2020},\n    eprint={2009.06354},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"QED, is a linguistically informed, extensible framework for explanations in question answering. A QED explanation specifies the relationship between a question and answer according to formal semantic notions such as referential equality, sentencehood, and entailment. It is an expertannotated dataset of QED explanations built upon a subset of the Google Natural Questions dataset.","paperswithcode_id":"qed","key":""},{"id":"qed_amara","tags":["annotations_creators:found","language_creators:found","languages:aa","languages:ab","languages:ae","languages:aeb","languages:af","languages:aka","languages:amh","languages:an","languages:ar","languages:arq","languages:arz","languages:as","languages:ase","languages:ast","languages:av","languages:ay","languages:az","languages:ba","languages:bam","languages:be","languages:ber","languages:bg","languages:bh","languages:bi","languages:bn","languages:bnt","languages:bo","languages:br","languages:bs","languages:bug","languages:ca","languages:ce","languages:ceb","languages:ch","languages:cho","languages:cku","languages:cnh","languages:co","languages:cr","languages:cs","languages:cu","languages:cv","languages:cy","languages:da","languages:de","languages:dv","languages:dz","languages:ee","languages:efi","languages:el","languages:en","languages:eo","languages:es","languages:et","languages:eu","languages:fa","languages:ff","languages:fi","languages:fil","languages:fj","languages:fo","languages:fr","languages:ful","languages:ga","languages:gd","languages:gl","languages:gn","languages:gu","languages:hai","languages:hau","languages:haw","languages:haz","languages:hb","languages:hch","languages:he","languages:hi","languages:ho","languages:hr","languages:ht","languages:hu","languages:hup","languages:hus","languages:hy","languages:hz","languages:ia","languages:ibo","languages:id","languages:ie","languages:ik","languages:inh","languages:io","languages:iro","languages:is","languages:it","languages:iu","languages:ja","languages:jv","languages:ka","languages:kar","languages:kau","languages:kik","languages:kin","languages:kj","languages:kk","languages:kl","languages:km","languages:kn","languages:ko","languages:ksh","languages:ku","languages:kv","languages:kw","languages:ky","languages:la","languages:lb","languages:lg","languages:li","languages:lin","languages:lkt","languages:lld","languages:lo","languages:lt","languages:ltg","languages:lu","languages:luo","languages:luy","languages:lv","languages:mad","languages:mfe","languages:mi","languages:mk","languages:ml","languages:mlg","languages:mn","languages:mni","languages:mo","languages:moh","languages:mos","languages:mr","languages:ms","languages:mt","languages:mus","languages:my","languages:nb","languages:nci","languages:nd","languages:ne","languages:nl","languages:nn","languages:nso","languages:nv","languages:nya","languages:oc","languages:or","languages:orm","languages:pam","languages:pan","languages:pap","languages:pi","languages:pl","languages:pnb","languages:prs","languages:ps","languages:pt","languages:que","languages:rm","languages:ro","languages:ru","languages:run","languages:rup","languages:ry","languages:sa","languages:sc","languages:scn","languages:sco","languages:sd","languages:sg","languages:sgn","languages:sh","languages:si","languages:sk","languages:sl","languages:sm","languages:sna","languages:som","languages:sot","languages:sq","languages:sr","languages:srp","languages:sv","languages:swa","languages:szl","languages:ta","languages:te","languages:tet","languages:tg","languages:th","languages:tir","languages:tk","languages:tl","languages:tlh","languages:to","languages:tr","languages:ts","languages:tt","languages:tw","languages:ug","languages:uk","languages:umb","languages:ur","languages:uz","languages:ve","languages:vi","languages:vls","languages:vo","languages:wa","languages:wol","languages:xh","languages:yaq","languages:yi","languages:yor","languages:za","languages:zam","languages:zh","languages:zul","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"A. Abdelali, F. Guzman, H. Sajjad and S. Vogel, \"The AMARA Corpus: Building parallel language resources for the educational domain\", The Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC'14). Reykjavik, Iceland, 2014. Pp. 1856-1862. Isbn. 978-2-9517408-8-4.","description":"The QCRI Educational Domain Corpus (formerly QCRI AMARA Corpus) is an open multilingual collection of subtitles for educational videos and lectures collaboratively transcribed and translated over the AMARA web-based platform.\nDeveloped by: Qatar Computing Research Institute, Arabic Language Technologies Group\nThe QED Corpus is made public for RESEARCH purpose only.\nThe corpus is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Copyright Qatar Computing Research Institute. All rights reserved.\n225 languages, 9,291 bitexts\ntotal number of files: 271,558\ntotal number of tokens: 371.76M\ntotal number of sentence fragments: 30.93M","key":""},{"id":"quac","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:found","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|wikipedia","task_categories:question-answering","task_categories:sequence-modeling","task_ids:dialogue-modeling","task_ids:extractive-qa"],"citation":"@inproceedings{choi-etal-2018-quac,\ntitle = \"QUAC: Question answering in context\",\nabstract = \"We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.\",\nauthor = \"Eunsol Choi and He He and Mohit Iyyer and Mark Yatskar and Yih, {Wen Tau} and Yejin Choi and Percy Liang and Luke Zettlemoyer\",\nyear = \"2018\",\nlanguage = \"English (US)\",\nseries = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018\",\npublisher = \"Association for Computational Linguistics\",\npages = \"2174--2184\",\neditor = \"Ellen Riloff and David Chiang and Julia Hockenmaier and Jun'ichi Tsujii\",\nbooktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018\",\nnote = \"2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018 ; Conference date: 31-10-2018 Through 04-11-2018\",\n}","description":"Question Answering in Context is a dataset for modeling, understanding,\nand participating in information seeking dialog. Data instances consist\nof an interactive dialog between two crowd workers: (1) a student who\nposes a sequence of freeform questions to learn as much as possible\nabout a hidden Wikipedia text, and (2) a teacher who answers the questions\nby providing short excerpts (spans) from the text. QuAC introduces\nchallenges not found in existing machine comprehension datasets: its\nquestions are often more open-ended, unanswerable, or only meaningful\nwithin the dialog context.","paperswithcode_id":"quac","key":""},{"id":"quail","tags":["languages:en"],"citation":"@inproceedings{DBLP:conf/aaai/RogersKDR20,\n  author    = {Anna Rogers and\n               Olga Kovaleva and\n               Matthew Downey and\n               Anna Rumshisky},\n  title     = {Getting Closer to {AI} Complete Question Answering: {A} Set of Prerequisite\n               Real Tasks},\n  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}\n               2020, The Thirty-Second Innovative Applications of Artificial Intelligence\n               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational\n               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,\n               February 7-12, 2020},\n  pages     = {8722--8731},\n  publisher = {{AAAI} Press},\n  year      = {2020},\n  url       = {https://aaai.org/ojs/index.php/AAAI/article/view/6398},\n  timestamp = {Thu, 04 Jun 2020 13:18:48 +0200},\n  biburl    = {https://dblp.org/rec/conf/aaai/RogersKDR20.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"QuAIL is a  reading comprehension dataset. QuAIL contains 15K multi-choice questions in texts 300-350 tokens long 4 domains (news, user stories, fiction, blogs).QuAIL is balanced and annotated for question types.\\","paperswithcode_id":"quail","key":""},{"id":"quarel","tags":["languages:en"],"citation":"@inproceedings{quarel_v1,\n    title={QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships},\n    author={Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, Ashish Sabharwal},\n    year={2018},\n    journal={arXiv:1805.05377v1}\n}","description":"QuaRel is a crowdsourced dataset of 2771 multiple-choice story questions, including their logical forms.","paperswithcode_id":"quarel","key":""},{"id":"quartz","tags":["languages:en"],"citation":"@InProceedings{quartz,\n  author = {Oyvind Tafjord and Matt Gardner and Kevin Lin and Peter Clark},\n  title = {\"QUARTZ: An Open-Domain Dataset of Qualitative Relationship\nQuestions\"},\n  year = {\"2019\"},\n}","description":"QuaRTz is a crowdsourced dataset of 3864 multiple-choice questions about open domain qualitative relationships. Each\nquestion is paired with one of 405 different background sentences (sometimes short paragraphs).\nThe QuaRTz dataset V1 contains 3864 questions about open domain qualitative relationships. Each question is paired with\none of 405 different background sentences (sometimes short paragraphs).\nThe dataset is split into train (2696), dev (384) and test (784). A background sentence will only appear in a single split.","paperswithcode_id":"quartz","key":""},{"id":"quora","tags":["languages:en"],"key":""},{"id":"quoref","tags":["languages:en"],"citation":"@article{allenai:quoref,\n      author    = {Pradeep Dasigi and Nelson F. Liu and Ana Marasovic and Noah A. Smith and  Matt Gardner},\n      title     = {Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning},\n      journal   = {arXiv:1908.05803v2 },\n      year      = {2019},\n}","description":"Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems. In this\nspan-selection benchmark containing 24K questions over 4.7K paragraphs from Wikipedia, a system must resolve hard\ncoreferences before selecting the appropriate span(s) in the paragraphs for answering questions.","paperswithcode_id":"quoref","key":""},{"id":"race","tags":["languages:en"],"citation":"@article{lai2017large,\n    title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},\n    author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},\n    journal={arXiv preprint arXiv:1704.04683},\n    year={2017}\n}","description":"Race is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The\n dataset is collected from English examinations in China, which are designed for middle school and high school students.\nThe dataset can be served as the training and test sets for machine comprehension.","paperswithcode_id":"race","key":""},{"id":"re_dial","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:other","task_categories:text-classification","task_ids:sentiment-classification","task_ids:text-classification-other-dialogue-sentiment-classification"],"citation":"@inproceedings{li2018conversational,\n  title={Towards Deep Conversational Recommendations},\n  author={Li, Raymond and Kahou, Samira Ebrahimi and Schulz, Hannes and Michalski, Vincent and Charlin, Laurent and Pal, Chris},\n  booktitle={Advances in Neural Information Processing Systems 31 (NIPS 2018)},\n  year={2018}\n}","description":"ReDial (Recommendation Dialogues) is an annotated dataset of dialogues, where users\nrecommend movies to each other. The dataset was collected by a team of researchers working at\nPolytechnique Montréal, MILA – Quebec AI Institute, Microsoft Research Montréal, HEC Montreal, and Element AI.\n\nThe dataset allows research at the intersection of goal-directed dialogue systems\n(such as restaurant recommendation) and free-form (also called “chit-chat”) dialogue systems.","paperswithcode_id":"redial","key":""},{"id":"reasoning_bg","tags":["annotations_creators:found","language_creators:found","languages:bg","licenses:apache-2.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa"],"citation":"@article{hardalov2019beyond,\n  title={Beyond english-only reading comprehension: Experiments in zero-shot multilingual transfer for bulgarian},\n  author={Hardalov, Momchil and Koychev, Ivan and Nakov, Preslav},\n  journal={arXiv preprint arXiv:1908.01519},\n  year={2019}\n}","description":"This new dataset is designed to do reading comprehension in Bulgarian language.","key":""},{"id":"recipe_nlg","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_categories:text-retrieval","task_ids:document-retrieval","task_ids:entity-linking-retrieval","task_ids:explanation-generation","task_ids:language-modeling","task_ids:summarization"],"citation":"@inproceedings{bien-etal-2020-recipenlg,\ntitle = \"{R}ecipe{NLG}: A Cooking Recipes Dataset for Semi-Structured Text Generation\",\nauthor = \"Bie{'n}, Micha{l}  and\n  Gilski, Micha{l}  and\n  Maciejewska, Martyna  and\n  Taisner, Wojciech  and\n  Wisniewski, Dawid  and\n  Lawrynowicz, Agnieszka\",\nbooktitle = \"Proceedings of the 13th International Conference on Natural Language Generation\",\nmonth = dec,\nyear = \"2020\",\naddress = \"Dublin, Ireland\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/2020.inlg-1.4\",\npages = \"22--28\"\n}","description":"The dataset contains 2231142 cooking recipes (>2 millions). It's processed in more careful way and provides more samples than any other dataset in the area.","paperswithcode_id":"recipenlg","key":""},{"id":"reclor","tags":[],"citation":"@inproceedings{yu2020reclor,\n        author = {Yu, Weihao and Jiang, Zihang and Dong, Yanfei and Feng, Jiashi},\n        title = {ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning},\n        booktitle = {International Conference on Learning Representations (ICLR)},\n        month = {April},\n        year = {2020}\n    }","description":"Logical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary\nlanguage as the definition from LSAC. ReClor is a dataset extracted from logical reasoning questions of standardized graduate\nadmission examinations. Empirical results show that the state-of-the-art models struggle on ReClor with poor performance\nindicating more research is needed to essentially enhance the logical reasoning ability of current models. We hope this\ndataset could help push Machine Reading Comprehension (MRC) towards more complicated reasonin","paperswithcode_id":"reclor","key":""},{"id":"reddit","tags":["languages:en"],"citation":"@inproceedings{volske-etal-2017-tl,\n    title = {TL;DR: Mining {R}eddit to Learn Automatic Summarization},\n    author = {V{\\\"o}lske, Michael  and Potthast, Martin  and Syed, Shahbaz  and Stein, Benno},\n    booktitle = {Proceedings of the Workshop on New Frontiers in Summarization},\n    month = {sep},\n    year = {2017},\n    address = {Copenhagen, Denmark},\n    publisher = {Association for Computational Linguistics},\n    url = {https://www.aclweb.org/anthology/W17-4508},\n    doi = {10.18653/v1/W17-4508},\n    pages = {59--63},\n    abstract = {Recent advances in automatic text summarization have used deep neural networks to generate high-quality abstractive summaries, but the performance of these models strongly depends on large amounts of suitable training data. We propose a new method for mining social media for author-provided summaries, taking advantage of the common practice of appending a {``}TL;DR{''} to long posts. A case study using a large Reddit crawl yields the Webis-TLDR-17 dataset, complementing existing corpora primarily from the news genre. Our technique is likely applicable to other social media sites and general web crawls.},\n}","description":"This corpus contains preprocessed posts from the Reddit dataset.\nThe dataset consists of 3,848,330 posts with an average length of 270 words for content,\nand 28 words for the summary.\n\nFeatures includes strings: author, body, normalizedBody, content, summary, subreddit, subreddit_id.\nContent is used as document and summary is used as summary.","paperswithcode_id":"reddit","key":""},{"id":"reddit_tifu","tags":["languages:en"],"citation":"@misc{kim2018abstractive,\n    title={Abstractive Summarization of Reddit Posts with Multi-level Memory Networks},\n    author={Byeongchang Kim and Hyunwoo Kim and Gunhee Kim},\n    year={2018},\n    eprint={1811.00783},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"Reddit dataset, where TIFU denotes the name of subbreddit /r/tifu.\nAs defined in the publication, styel \"short\" uses title as summary and\n\"long\" uses tldr as summary.\n\nFeatures includes:\n  - document: post text without tldr.\n  - tldr: tldr line.\n  - title: trimmed title without tldr.\n  - ups: upvotes.\n  - score: score.\n  - num_comments: number of comments.\n  - upvote_ratio: upvote ratio.","paperswithcode_id":"reddit-tifu","key":""},{"id":"refresd","tags":["annotations_creators:crowdsourced","annotations_creators:machine-generated","language_creators:crowdsourced","language_creators:machine-generated","languages:en","languages:fr","licenses:mit","multilinguality:translation","size_categories:1K<n<10K","task_categories:text-classification","task_categories:text-scoring","task_ids:semantic-similarity-classification","task_ids:semantic-similarity-scoring"],"citation":"@inproceedings{briakou-carpuat-2020-detecting,\n    title = \"Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank\",\n    author = \"Briakou, Eleftheria and Carpuat, Marine\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.121\",\n    pages = \"1563--1580\",\n}","description":"The Rationalized English-French Semantic Divergences (REFreSD) dataset consists of 1,039\n English-French sentence-pairs annotated with sentence-level divergence judgments and token-level\n rationales. For any questions, write to ebriakou@cs.umd.edu.","paperswithcode_id":"refresd","key":""},{"id":"reuters21578","tags":["languages:en"],"citation":"@article{APTE94,\nauthor = {Chidanand Apt{\\'{e}} and Fred Damerau and Sholom M. Weiss},\ntitle = {Automated Learning of Decision Rules for Text Categorization},\njournal = {ACM Transactions on Information Systems},\nyear = {1994},\nnote = {To appear.}\n}\n\n@inproceedings{APTE94b,\nauthor = {Chidanand Apt{\\'{e}} and Fred Damerau and Sholom M. Weiss},\ntitle = {Toward Language Independent Automated Learning of Text Categorization Models},\nbooktitle = {sigir94},\nyear = {1994},\nnote = {To appear.}\n}\n\n@inproceedings{HAYES8},\nauthor = {Philip J. Hayes and Peggy M. Anderson and Irene B. Nirenburg and\nLinda M. Schmandt},\ntitle = {{TCS}: A Shell for Content-Based Text Categorization},\nbooktitle = {IEEE Conference on Artificial Intelligence Applications},\nyear = {1990}\n}\n\n@inproceedings{HAYES90b,\nauthor = {Philip J. Hayes and Steven P. Weinstein},\ntitle = {{CONSTRUE/TIS:} A System for Content-Based Indexing of a\nDatabase of News Stories},\nbooktitle = {Second Annual Conference on Innovative Applications of\nArtificial Intelligence},\nyear = {1990}\n}\n\n@incollection{HAYES92 ,\nauthor = {Philip J. Hayes},\ntitle = {Intelligent High-Volume Text Processing using Shallow,\nDomain-Specific Techniques},\nbooktitle = {Text-Based Intelligent Systems},\npublisher = {Lawrence Erlbaum},\naddress =  {Hillsdale, NJ},\nyear = {1992},\neditor = {Paul S. Jacobs}\n}\n\n@inproceedings{LEWIS91c ,\nauthor = {David D. Lewis},\ntitle = {Evaluating Text Categorization},\nbooktitle = {Proceedings of Speech and Natural Language Workshop},\nyear = {1991},\nmonth = {feb},\norganization = {Defense Advanced Research Projects Agency},\npublisher = {Morgan Kaufmann},\npages = {312--318}\n\n}\n\n@phdthesis{LEWIS91d,\nauthor = {David Dolan Lewis},\ntitle = {Representation and Learning in Information Retrieval},\nschool = {Computer Science Dept.; Univ. of Massachusetts; Amherst, MA 01003},\nyear = 1992},\nnote = {Technical Report 91--93.}\n}\n\n@inproceedings{LEWIS91e,\nauthor = {David D. Lewis},\ntitle = {Data Extraction as Text Categorization: An Experiment with\nthe {MUC-3} Corpus},\nbooktitle = {Proceedings of the Third Message Understanding Evaluation\nand Conference},\nyear = {1991},\nmonth = {may},\norganization = {Defense Advanced Research Projects Agency},\npublisher = {Morgan Kaufmann},\naddress = {Los Altos, CA}\n\n}\n\n@inproceedings{LEWIS92b,\nauthor = {David D. Lewis},\ntitle = {An Evaluation of Phrasal and Clustered Representations on a Text\nCategorization Task},\nbooktitle = {Fifteenth Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval},\nyear = {1992},\npages = {37--50}\n}\n\n@inproceedings{LEWIS92d ,\nauthor = {David D. Lewis and Richard M. Tong},\ntitle = {Text Filtering in {MUC-3} and {MUC-4}},\nbooktitle = {Proceedings of the Fourth Message Understanding Conference ({MUC-4})},\nyear = {1992},\nmonth = {jun},\norganization = {Defense Advanced Research Projects Agency},\npublisher = {Morgan Kaufmann},\naddress = {Los Altos, CA}\n}\n\n@inproceedings{LEWIS92e,\nauthor = {David D. Lewis},\ntitle = {Feature Selection and Feature Extraction for Text Categorization},\nbooktitle = {Proceedings of Speech and Natural Language Workshop},\nyear = {1992},\nmonth = {feb} ,\norganization = {Defense Advanced Research Projects Agency},\npublisher = {Morgan Kaufmann},\npages = {212--217}\n}\n\n@inproceedings{LEWIS94b,\nauthor = {David D. Lewis and Marc Ringuette},\ntitle = {A Comparison of Two Learning Algorithms for Text Categorization},\nbooktitle = {Symposium on Document Analysis and Information Retrieval},\nyear = {1994},\norganization = {ISRI; Univ. of Nevada, Las Vegas},\naddress = {Las Vegas, NV},\nmonth = {apr},\npages = {81--93}\n}\n\n@article{LEWIS94d,\nauthor = {David D. Lewis and Philip J. Hayes},\ntitle = {Guest Editorial},\njournal = {ACM Transactions on Information Systems},\nyear = {1994},\nvolume  = {12},\nnumber  = {3},\npages = {231},\nmonth = {jul}\n}\n\n@article{SPARCKJONES76,\nauthor = {K. {Sparck Jones} and  C. J. {van Rijsbergen}},\ntitle =  {Information Retrieval Test Collections},\njournal = {Journal of Documentation},\nyear = {1976},\nvolume = {32},\nnumber = {1},\npages = {59--75}\n}\n\n@book{WEISS91,\nauthor = {Sholom M. Weiss and Casimir A. Kulikowski},\ntitle = {Computer Systems That Learn},\npublisher = {Morgan Kaufmann},\nyear = {1991},\naddress = {San Mateo, CA}\n}","description":"The Reuters-21578 dataset  is one of the most widely used data collections for text\ncategorization research. It is collected from the Reuters financial newswire service in 1987.","paperswithcode_id":"reuters-21578","key":""},{"id":"ro_sent","tags":["annotations_creators:found","language_creators:found","languages:ro","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@article{dumitrescu2020birth,\n  title={The birth of Romanian BERT},\n  author={Dumitrescu, Stefan Daniel and Avram, Andrei-Marius and Pyysalo, Sampo},\n  journal={arXiv preprint arXiv:2009.08712},\n  year={2020}\n}","description":"This dataset is a Romanian Sentiment Analysis dataset.\nIt is present in a processed form, as used by the authors of `Romanian Transformers`\nin their examples and based on the original data present in\n`https://github.com/katakonst/sentiment-analysis-tensorflow`. The original dataset is collected\nfrom product and movie reviews in Romanian.","key":""},{"id":"ro_sts","tags":["language_creators:crowdsourced","languages:ro","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-sts-b","task_categories:text-scoring","task_ids:semantic-similarity-scoring"],"citation":"Article under review","description":"The RO-STS (Romanian Semantic Textual Similarity) dataset contains 8628 pairs of sentences with their similarity score. It is a high-quality translation of the STS benchmark dataset.","key":""},{"id":"ro_sts_parallel","tags":["language_creators:crowdsourced","languages:ro","languages:en","licenses:cc-by-4.0","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:extended|other-sts-b","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"Article under review","description":"The RO-STS-Parallel (a Parallel Romanian English dataset - translation of the Semantic Textual Similarity) contains 17256 sentences in Romanian and English. It is a high-quality translation of the English STS benchmark dataset into Romanian.","key":""},{"id":"roman_urdu","tags":["annotations_creators:crowdsourced","language_creators:found","languages:ur","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@InProceedings{Sharf:2018,\ntitle = \"Performing Natural Language Processing on Roman Urdu Datasets\",\nauthors = \"Zareen Sharf and Saif Ur Rahman\",\nbooktitle = \"International Journal of Computer Science and Network Security\",\nvolume = \"18\",\nnumber = \"1\",\npages = \"141-148\",\nyear = \"2018\"\n}\n\n@misc{Dua:2019,\nauthor = \"Dua, Dheeru and Graff, Casey\",\nyear = \"2017\",\ntitle = \"{UCI} Machine Learning Repository\",\nurl = \"http://archive.ics.uci.edu/ml\",\ninstitution = \"University of California, Irvine, School of Information and Computer Sciences\"\n}","description":"This is an extensive compilation of Roman Urdu Dataset (Urdu written in Latin/Roman script) tagged for sentiment analysis.","paperswithcode_id":"roman-urdu-data-set","key":""},{"id":"ronec","tags":["annotations_creators:expert-generated","language_creators:expert-generated","language_creators:found","languages:ro","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@article{dumitrescu2019introducing,\n  title={Introducing RONEC--the Romanian Named Entity Corpus},\n  author={Dumitrescu, Stefan Daniel and Avram, Andrei-Marius},\n  journal={arXiv preprint arXiv:1909.01247},\n  year={2019}\n}","description":"The RONEC (Named Entity Corpus for the Romanian language) dataset  contains over 26000 entities in ~5000 annotated sentence,\nbelonging to 16 distinct classes. It represents the first initiative in the Romanian language space specifically targeted for named entity recognition","paperswithcode_id":"ronec","key":""},{"id":"ropes","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|wikipedia","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@inproceedings{Lin2019ReasoningOP,\n  title={Reasoning Over Paragraph Effects in Situations},\n  author={Kevin Lin and Oyvind Tafjord and Peter Clark and Matt Gardner},\n  booktitle={MRQA@EMNLP},\n  year={2019}\n}","description":"ROPES (Reasoning Over Paragraph Effects in Situations) is a QA dataset\nwhich tests a system's ability to apply knowledge from a passage\nof text to a new situation. A system is presented a background\npassage containing a causal or qualitative relation(s) (e.g.,\n\"animal pollinators increase efficiency of fertilization in flowers\"),\na novel situation that uses this background, and questions that require\nreasoning about effects of the relationships in the background\npassage in the background of the situation.","paperswithcode_id":"ropes","key":""},{"id":"rotten_tomatoes","tags":["languages:en"],"citation":"@InProceedings{Pang+Lee:05a,\n  author =       {Bo Pang and Lillian Lee},\n  title =        {Seeing stars: Exploiting class relationships for sentiment\n                  categorization with respect to rating scales},\n  booktitle =    {Proceedings of the ACL},\n  year =         2005\n}","description":"Movie Review Dataset.\nThis is a dataset of containing 5,331 positive and 5,331 negative processed\nsentences from Rotten Tomatoes movie reviews. This data was first used in Bo\nPang and Lillian Lee, ``Seeing stars: Exploiting class relationships for\nsentiment categorization with respect to rating scales.'', Proceedings of the\nACL, 2005.","key":""},{"id":"s2orc","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:en","licenses:cc-by-2.0","multilinguality:monolingual","size_categories:100M<n<1B","source_datasets:original","task_categories:other","task_categories:sequence-modeling","task_categories:text-classification","task_ids:language-modeling","task_ids:multi-class-classification","task_ids:multi-label-classification","task_ids:other-other-citation-recommendation"],"citation":"@misc{lo2020s2orc,\n      title={S2ORC: The Semantic Scholar Open Research Corpus},\n      author={Kyle Lo and Lucy Lu Wang and Mark Neumann and Rodney Kinney and Dan S. Weld},\n      year={2020},\n      eprint={1911.02782},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"A large corpus of 81.1M English-language academic papers spanning many academic disciplines.\nRich metadata, paper abstracts, resolved bibliographic references, as well as structured full\ntext for 8.1M open access papers. Full text annotated with automatically-detected inline mentions of\ncitations, figures, and tables, each linked to their corresponding paper objects. Aggregated papers\nfrom hundreds of academic publishers and digital archives into a unified source, and create the largest\npublicly-available collection of machine-readable academic text to date.","paperswithcode_id":"s2orc","key":""},{"id":"samsum","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:cc-by-nc-nd-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@article{gliwa2019samsum,\n  title={SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization},\n  author={Gliwa, Bogdan and Mochol, Iwona and Biesek, Maciej and Wawer, Aleksander},\n  journal={arXiv preprint arXiv:1911.12237},\n  year={2019}\n}","description":"SAMSum Corpus contains over 16k chat dialogues with manually annotated\nsummaries.\nThere are two features:\n  - dialogue: text of dialogue.\n  - summary: human written summary of the dialogue.\n  - id: id of a example.","paperswithcode_id":"samsum-corpus","key":""},{"id":"sanskrit_classic","tags":["annotations_creators:no-annotation","language_creators:found","languages:sa","licenses:other-Public Domain Mark 1.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@Misc{johnsonetal2014,\n author = {Johnson, Kyle P. and Patrick Burns and John Stewart and Todd Cook},\n title = {CLTK: The Classical Language Toolkit},\n url = {https://github.com/cltk/cltk},\n year = {2014--2020},\n}","description":"This dataset combines some of the classical Sanskrit texts.","key":""},{"id":"saudinewsnet","tags":["annotations_creators:no-annotation","language_creators:found","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@misc{hagrima2015,\nauthor = \"M. Alhagri\",\ntitle = \"Saudi Newspapers Arabic Corpus (SaudiNewsNet)\",\nyear = 2015,\nurl = \"http://github.com/ParallelMazen/SaudiNewsNet\"\n}","description":"The dataset contains a set of 31,030 Arabic newspaper articles alongwith metadata, extracted from various online Saudi newspapers and written in MSA.","key":""},{"id":"scan","tags":["languages:en"],"citation":"@inproceedings{Lake2018GeneralizationWS,\n  title={Generalization without Systematicity: On the Compositional Skills of\n         Sequence-to-Sequence Recurrent Networks},\n  author={Brenden M. Lake and Marco Baroni},\n  booktitle={ICML},\n  year={2018},\n  url={https://arxiv.org/pdf/1711.00350.pdf},\n}","description":"SCAN tasks with various splits.\n\nSCAN is a set of simple language-driven navigation tasks for studying\ncompositional learning and zero-shot generalization.\n\nSee https://github.com/brendenlake/SCAN for a description of the splits.\n\nExample usage:\ndata = datasets.load_dataset('scan/length')","paperswithcode_id":"scan","key":""},{"id":"scb_mt_enth_2020","tags":["annotations_creators:crowdsourced","annotations_creators:expert-generated","annotations_creators:found","annotations_creators:machine-generated","language_creators:expert-generated","language_creators:found","language_creators:machine-generated","languages:en","languages:th","licenses:cc-by-sa-4.0","multilinguality:translation","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@article{lowphansirikul2020scb,\n  title={scb-mt-en-th-2020: A Large English-Thai Parallel Corpus},\n  author={Lowphansirikul, Lalita and Polpanumas, Charin and Rutherford, Attapol T and Nutanong, Sarana},\n  journal={arXiv preprint arXiv:2007.03541},\n  year={2020}\n}","description":"scb-mt-en-th-2020: A Large English-Thai Parallel Corpus\nThe primary objective of our work is to build a large-scale English-Thai dataset for machine translation.\nWe construct an English-Thai machine translation dataset with over 1 million segment pairs, curated from various sources,\nnamely news, Wikipedia articles, SMS messages, task-based dialogs, web-crawled data and government documents.\nMethodology for gathering data, building parallel texts and removing noisy sentence pairs are presented in a reproducible manner.\nWe train machine translation models based on this dataset. Our models' performance are comparable to that of\nGoogle Translation API (as of May 2020) for Thai-English and outperform Google when the Open Parallel Corpus (OPUS) is\nincluded in the training data for both Thai-English and English-Thai translation.\nThe dataset, pre-trained models, and source code to reproduce our work are available for public use.","paperswithcode_id":"scb-mt-en-th-2020","key":""},{"id":"schema_guided_dstc8","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","language_creators:machine-generated","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_categories:structure-prediction","task_categories:text-classification","task_ids:dialogue-modeling","task_ids:multi-class-classification","task_ids:parsing"],"citation":"@inproceedings{aaai/RastogiZSGK20,\n  author    = {Abhinav Rastogi and\n               Xiaoxue Zang and\n               Srinivas Sunkara and\n               Raghav Gupta and\n               Pranav Khaitan},\n  title     = {Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided\n               Dialogue Dataset},\n  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}\n               2020, The Thirty-Second Innovative Applications of Artificial Intelligence\n               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational\n               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,\n               February 7-12, 2020},\n  pages     = {8689--8696},\n  publisher = {{AAAI} Press},\n  year      = {2020},\n  url       = {https://aaai.org/ojs/index.php/AAAI/article/view/6394}\n}","description":"The Schema-Guided Dialogue dataset (SGD) was developed for the Dialogue State Tracking task of the Eights Dialogue Systems Technology Challenge (dstc8).\nThe SGD dataset consists of over 18k annotated multi-domain, task-oriented conversations between a human and a virtual assistant.\nThese conversations involve interactions with services and APIs spanning 17 domains, ranging from banks and events to media, calendar, travel, and weather.\nFor most of these domains, the SGD dataset contains multiple different APIs, many of which have overlapping functionalities but different interfaces,\nwhich reflects common real-world scenarios.","paperswithcode_id":"sgd","key":""},{"id":"scicite","tags":["languages:en"],"citation":"@InProceedings{Cohan2019Structural,\n  author={Arman Cohan and Waleed Ammar and Madeleine Van Zuylen and Field Cady},\n  title={Structural Scaffolds for Citation Intent Classification in Scientific Publications},\n  booktitle={NAACL},\n  year={2019}\n}","description":"This is a dataset for classifying citation intents in academic papers.\nThe main citation intent label for each Json object is specified with the label\nkey while the citation context is specified in with a context key. Example:\n{\n 'string': 'In chacma baboons, male-infant relationships can be linked to both\n    formation of friendships and paternity success [30,31].'\n 'sectionName': 'Introduction',\n 'label': 'background',\n 'citingPaperId': '7a6b2d4b405439',\n 'citedPaperId': '9d1abadc55b5e0',\n ...\n }\nYou may obtain the full information about the paper using the provided paper ids\nwith the Semantic Scholar API (https://api.semanticscholar.org/).\nThe labels are:\nMethod, Background, Result","paperswithcode_id":"scicite","key":""},{"id":"scielo","tags":["annotations_creators:found","language_creators:found","languages:en","languages:es","languages:pt","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{soares2018large,\n  title={A Large Parallel Corpus of Full-Text Scientific Articles},\n  author={Soares, Felipe and Moreira, Viviane and Becker, Karin},\n  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018)},\n  year={2018}\n}","description":"A parallel corpus of full-text scientific articles collected from Scielo database in the following languages: English, Portuguese and Spanish. The corpus is sentence aligned for all language pairs, as well as trilingual aligned for a small subset of sentences. Alignment was carried out using the Hunalign algorithm.","key":""},{"id":"scientific_papers","tags":["languages:en"],"citation":"@article{Cohan_2018,\n   title={A Discourse-Aware Attention Model for Abstractive Summarization of\n            Long Documents},\n   url={http://dx.doi.org/10.18653/v1/n18-2097},\n   DOI={10.18653/v1/n18-2097},\n   journal={Proceedings of the 2018 Conference of the North American Chapter of\n          the Association for Computational Linguistics: Human Language\n          Technologies, Volume 2 (Short Papers)},\n   publisher={Association for Computational Linguistics},\n   author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},\n   year={2018}\n}","description":"Scientific papers datasets contains two sets of long and structured documents.\nThe datasets are obtained from ArXiv and PubMed OpenAccess repositories.\n\nBoth \"arxiv\" and \"pubmed\" have two features:\n  - article: the body of the document, pagragraphs seperated by \"/n\".\n  - abstract: the abstract of the document, pagragraphs seperated by \"/n\".\n  - section_names: titles of sections, seperated by \"/n\".","key":""},{"id":"scifact","tags":["languages:en"],"citation":"@inproceedings{Wadden2020FactOF,\n  title={Fact or Fiction: Verifying Scientific Claims},\n  author={David Wadden and Shanchuan Lin and Kyle Lo and Lucy Lu Wang and Madeleine van Zuylen and Arman Cohan and Hannaneh Hajishirzi},\n  booktitle={EMNLP},\n  year={2020},\n}","description":"SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts, and annotated with labels and rationales.","key":""},{"id":"sciq","tags":["languages:en"],"citation":"@inproceedings{SciQ,\n    title={Crowdsourcing Multiple Choice Science Questions},\n    author={Johannes Welbl, Nelson F. Liu, Matt Gardner},\n    year={2017},\n    journal={arXiv:1707.06209v1}\n}","description":"The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided.","paperswithcode_id":"sciq","key":""},{"id":"scitail","tags":["languages:en"],"citation":"inproceedings{scitail,\n     Author = {Tushar Khot and Ashish Sabharwal and Peter Clark},\n     Booktitle = {AAAI},\n     Title = {{SciTail}: A Textual Entailment Dataset from Science Question Answering},\n     Year = {2018}\n}","description":"The SciTail dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question\nand the correct answer choice are converted into an assertive statement to form the hypothesis. We use information\nretrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We\ncrowdsource the annotation of such premise-hypothesis pair as supports (entails) or not (neutral), in order to create\nthe SciTail dataset. The dataset contains 27,026 examples with 10,101 examples with entails label and 16,925 examples\nwith neutral label","paperswithcode_id":"scitail","key":""},{"id":"scitldr","tags":["annotations_creators:no-annotation","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@article{cachola2020tldr,\n  title={{TLDR}: Extreme Summarization of Scientific Documents},\n  author={Isabel Cachola and Kyle Lo and Arman Cohan and Daniel S. Weld},\n  journal={arXiv:2004.15011},\n  year={2020},\n}","description":"A new multi-target dataset of 5.4K TLDRs over 3.2K papers.\nSCITLDR contains both author-written and expert-derived TLDRs,\nwhere the latter are collected using a novel annotation protocol\nthat produces high-quality summaries while minimizing annotation burden.","paperswithcode_id":"scitldr","key":""},{"id":"search_qa","tags":["languages:en"],"description":"We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind\nCNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article\nand generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google.\nFollowing this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context\n tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation\n as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human\n and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.","paperswithcode_id":"searchqa","key":""},{"id":"selqa","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@InProceedings{7814688,\n  author={T. {Jurczyk} and M. {Zhai} and J. D. {Choi}},\n  booktitle={2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)},\n  title={SelQA: A New Benchmark for Selection-Based Question Answering},\n  year={2016},\n  volume={},\n  number={},\n  pages={820-827},\n  doi={10.1109/ICTAI.2016.0128}\n}","description":"The SelQA dataset provides crowdsourced annotation for two selection-based question answer tasks,\nanswer sentence selection and answer triggering.","paperswithcode_id":"selqa","key":""},{"id":"sem_eval_2010_task_8","tags":["languages:en"],"citation":"@inproceedings{hendrickx-etal-2010-semeval,\n    title = \"{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals\",\n    author = \"Hendrickx, Iris  and\n      Kim, Su Nam  and\n      Kozareva, Zornitsa  and\n      Nakov, Preslav  and\n      {\\'O} S{\\'e}aghdha, Diarmuid  and\n      Pad{\\'o}, Sebastian  and\n      Pennacchiotti, Marco  and\n      Romano, Lorenza  and\n      Szpakowicz, Stan\",\n    booktitle = \"Proceedings of the 5th International Workshop on Semantic Evaluation\",\n    month = jul,\n    year = \"2010\",\n    address = \"Uppsala, Sweden\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/S10-1006\",\n    pages = \"33--38\",\n}","description":"The SemEval-2010 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals.\nThe task was designed to compare different approaches to semantic relation classification\nand to provide a standard testbed for future research.","paperswithcode_id":"semeval-2010-task-8","key":""},{"id":"sem_eval_2014_task_1","tags":["annotations_creators:crowdsourced","language_creators:expert-generated","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-ImageFlickr and SemEval-2012 STS MSR-Video Descriptions","task_categories:text-classification","task_categories:text-scoring","task_ids:natural-language-inference","task_ids:semantic-similarity-scoring"],"citation":"@inproceedings{inproceedings,\nauthor = {Marelli, Marco and Bentivogli, Luisa and Baroni, Marco and Bernardi, Raffaella and Menini, Stefano and Zamparelli, Roberto},\nyear = {2014},\nmonth = {08},\npages = {},\ntitle = {SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment},\ndoi = {10.3115/v1/S14-2001}\n}","description":"The SemEval-2014 Task 1 focuses on Evaluation of Compositional Distributional Semantic Models\non Full Sentences through Semantic Relatedness and Entailment. The task was designed to\npredict the degree of relatedness between two sentences and to detect the entailment\nrelation holding between them.","key":""},{"id":"sem_eval_2020_task_11","tags":["annotations_creators:expert-generated","language_creators:original","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_categories:structure-prediction","task_ids:text-classification-other-propaganda-technique-classification","task_ids:structure-prediction-other-propaganda-span-identification"],"citation":"@misc{martino2020semeval2020,\n      title={SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles},\n      author={G. Da San Martino and A. Barrón-Cedeño and H. Wachsmuth and R. Petrov and P. Nakov},\n      year={2020},\n      eprint={2009.02696},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Propagandistic news articles use specific techniques to convey their message,\nsuch as whataboutism, red Herring, and name calling, among many others.\nThe Propaganda Techniques Corpus (PTC) allows to study automatic algorithms to\ndetect them. We provide a permanent leaderboard to allow researchers both to\nadvertise their progress and to be up-to-speed with the state of the art on the\ntasks offered (see below for a definition).","key":""},{"id":"sent_comp","tags":["annotations_creators:machine-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:other","task_ids:other-other-sentence-compression"],"citation":"@inproceedings{filippova-altun-2013-overcoming,\n    title = \"Overcoming the Lack of Parallel Data in Sentence Compression\",\n    author = \"Filippova, Katja  and\n      Altun, Yasemin\",\n    booktitle = \"Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing\",\n    month = oct,\n    year = \"2013\",\n    address = \"Seattle, Washington, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D13-1155\",\n    pages = \"1481--1491\",\n}","description":"Large corpus of uncompressed and compressed sentences from news articles.","paperswithcode_id":"sentence-compression","key":""},{"id":"senti_lex","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:af","languages:an","languages:ar","languages:az","languages:be","languages:bg","languages:bn","languages:br","languages:bs","languages:ca","languages:cs","languages:cy","languages:da","languages:de","languages:el","languages:eo","languages:es","languages:et","languages:eu","languages:fa","languages:fi","languages:fo","languages:fr","languages:fy","languages:ga","languages:gd","languages:gl","languages:gu","languages:he","languages:hi","languages:hr","languages:ht","languages:hu","languages:hy","languages:ia","languages:id","languages:io","languages:is","languages:it","languages:ja","languages:ka","languages:km","languages:kn","languages:ko","languages:ku","languages:ky","languages:la","languages:lb","languages:lt","languages:lv","languages:mk","languages:mr","languages:ms","languages:mt","languages:nl","languages:nn","languages:no","languages:pl","languages:pt","languages:rm","languages:ro","languages:ru","languages:sk","languages:sl","languages:sq","languages:sr","languages:sv","languages:sw","languages:ta","languages:te","languages:th","languages:tk","languages:tl","languages:tr","languages:uk","languages:ur","languages:uz","languages:vi","languages:vo","languages:wa","languages:yi","languages:zh","languages:zhw","licenses:gpl-3.0","multilinguality:multilingual","size_categories:1K<n<10K","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{inproceedings,\nauthor = {Chen, Yanqing and Skiena, Steven},\nyear = {2014},\nmonth = {06},\npages = {383-389},\ntitle = {Building Sentiment Lexicons for All Major Languages},\nvolume = {2},\njournal = {52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference},\ndoi = {10.3115/v1/P14-2063}\n}","description":"This dataset add sentiment lexicons for 81 languages generated via graph propagation based on a knowledge graph--a graphical representation of real-world entities and the links between them.","key":""},{"id":"senti_ws","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:found","languages:de","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_categories:text-scoring","task_ids:sentiment-scoring","task_ids:structure-prediction-other-pos-tagging"],"citation":"@INPROCEEDINGS{remquahey2010,\ntitle = {SentiWS -- a Publicly Available German-language Resource for Sentiment Analysis},\nbooktitle = {Proceedings of the 7th International Language Resources and Evaluation (LREC'10)},\nauthor = {Remus, R. and Quasthoff, U. and Heyer, G.},\nyear = {2010}\n}","description":"SentimentWortschatz, or SentiWS for short, is a publicly available German-language resource for sentiment analysis, and pos-tagging. The POS tags are [\"NN\", \"VVINF\", \"ADJX\", \"ADV\"] -> [\"noun\", \"verb\", \"adjective\", \"adverb\"], and positive and negative polarity bearing words are weighted within the interval of [-1, 1].","key":""},{"id":"sentiment140","tags":["languages:en"],"citation":"@article{go2009twitter,\n  title={Twitter sentiment classification using distant supervision},\n  author={Go, Alec and Bhayani, Richa and Huang, Lei},\n  journal={CS224N project report, Stanford},\n  volume={1},\n  number={12},\n  pages={2009},\n  year={2009}\n}","description":"Sentiment140 consists of Twitter messages with emoticons, which are used as noisy labels for\nsentiment classification. For more detailed information please refer to the paper.","paperswithcode_id":"sentiment140","key":""},{"id":"sepedi_ner","tags":["annotations_creators:expert-generated","language_creators:found","languages:nso","licenses:other-Creative Commons Attribution 2.5 South Africa License","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{sepedi_ner,\n  author    = {D.J. Prinsloo and\n              Roald Eiselen},\n  title     = {NCHLT Sepedi Named Entity Annotated Corpus},\n  booktitle = {Eiselen, R. 2016. Government domain named entity recognition for South African languages. Proceedings of the 10th      Language Resource and Evaluation Conference, Portorož, Slovenia.},\n  year      = {2016},\n  url       = {https://repo.sadilar.org/handle/20.500.12185/328},\n}","description":"Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.","key":""},{"id":"sesotho_ner_corpus","tags":["annotations_creators:expert-generated","language_creators:found","languages:st","licenses:other-Creative Commons Attribution 2.5 South Africa License","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{sesotho_ner_corpus,\n  author    = {M. Setaka and\n                Roald Eiselen},\n  title     = {NCHLT Sesotho Named Entity Annotated Corpus},\n  booktitle = {Eiselen, R. 2016. Government domain named entity recognition for South African languages. Proceedings of the 10th      Language Resource and Evaluation Conference, Portorož, Slovenia.},\n  year      = {2016},\n  url       = {https://repo.sadilar.org/handle/20.500.12185/334},\n}","description":"Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.","key":""},{"id":"setimes","tags":["annotations_creators:found","language_creators:found","languages:bg","languages:bs","languages:el","languages:en","languages:hr","languages:mk","languages:ro","languages:sq","languages:sr","languages:tr","licenses:cc-by-sa-4.0","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"description":"SETimes – A Parallel Corpus of English and South-East European Languages\nThe corpus is based on the content published on the SETimes.com news portal. The news portal publishes “news and views from Southeast Europe” in ten languages: Bulgarian, Bosnian, Greek, English, Croatian, Macedonian, Romanian, Albanian and Serbian. This version of the corpus tries to solve the issues present in an older version of the corpus (published inside OPUS, described in the LREC 2010 paper by Francis M. Tyers and Murat Serdar Alperen). The following procedures were applied to resolve existing issues:\n\n- stricter extraction process – no HTML residues present\n- language identification on every non-English document – non-English online documents contain English material in case the article was not translated into that language\n- resolving encoding issues in Croatian and Serbian – diacritics were partially lost due to encoding errors – text was rediacritized.","key":""},{"id":"setswana_ner_corpus","tags":["annotations_creators:expert-generated","language_creators:found","languages:tn","licenses:other-Creative Commons Attribution 2.5 South Africa License","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{sepedi_ner_corpus,\n  author    = {S.S.B.M. Phakedi and\n              Roald Eiselen},\n  title     = {NCHLT Setswana Named Entity Annotated Corpus},\n  booktitle = {Eiselen, R. 2016. Government domain named entity recognition for South African languages. Proceedings of the 10th      Language Resource and Evaluation Conference, Portorož, Slovenia.},\n  year      = {2016},\n  url       = {https://repo.sadilar.org/handle/20.500.12185/341},\n}","description":"Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.","key":""},{"id":"sharc","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:expert-generated","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa","task_ids:question-answering-other-conversational-qa"],"citation":"@misc{saeidi2018interpretation,\n      title={Interpretation of Natural Language Rules in Conversational Machine Reading},\n      author={Marzieh Saeidi and Max Bartolo and Patrick Lewis and Sameer Singh and Tim Rocktäschel and Mike Sheldon and Guillaume Bouchard and Sebastian Riedel},\n      year={2018},\n      eprint={1809.01494},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"ShARC is a Conversational Question Answering dataset focussing on question answering from texts containing rules. The goal is to answer questions by possibly asking follow-up questions first. It is assumed assume that the question is often underspecified, in the sense that the question does not provide enough information to be answered directly. However, an agent can use the supporting rule text to infer what needs to be asked in order to determine the final answer.","paperswithcode_id":"sharc","key":""},{"id":"sharc_modified","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|sharc","task_categories:question-answering","task_ids:extractive-qa","task_ids:question-answering-other-conversational-qa"],"citation":"@inproceedings{verma-etal-2020-neural,\n    title = \"Neural Conversational {QA}: Learning to Reason vs Exploiting Patterns\",\n    author = \"Verma, Nikhil  and\n      Sharma, Abhishek  and\n      Madan, Dhiraj  and\n      Contractor, Danish  and\n      Kumar, Harshit  and\n      Joshi, Sachindra\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.589\",\n    pages = \"7263--7269\",\n    abstract = \"Neural Conversational QA tasks such as ShARC require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the ShARC QA task, we found indications that the model(s) learn spurious clues/patterns in the data-set. Further, a heuristic-based program, built to exploit these patterns, had comparative performance to that of the neural models. In this paper we share our findings about the four types of patterns in the ShARC corpus and how the neural models exploit them. Motivated by the above findings, we create and share a modified data-set that has fewer spurious patterns than the original data-set, consequently allowing models to learn better.\",\n}","description":"ShARC, a conversational QA task, requires a system to answer user questions based on rules expressed in natural language text. However, it is found that in the ShARC dataset there are multiple spurious patterns that could be exploited by neural models. SharcModified is a new dataset which reduces the patterns identified in the original dataset. To reduce the sensitivity of neural models, for each occurence of an instance conforming to any of the patterns, we automatically construct alternatives where we choose to either replace the current instance with an alternative instance which does not exhibit the pattern; or retain the original instance. The modified ShARC has two versions sharc-mod and history-shuffled. For morre details refer to Appendix A.3 .","key":""},{"id":"sick","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:CC-BY-NC-SA-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|image-flickr-8k","source_datasets:extended|semeval2012-sts-msr-video","task_categories:text-classification","task_ids:natural-language-inference"],"citation":"@inproceedings{marelli-etal-2014-sick,\n    title = \"A {SICK} cure for the evaluation of compositional distributional semantic models\",\n    author = \"Marelli, Marco  and\n      Menini, Stefano  and\n      Baroni, Marco  and\n      Bentivogli, Luisa  and\n      Bernardi, Raffaella  and\n      Zamparelli, Roberto\",\n    booktitle = \"Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)\",\n    month = may,\n    year = \"2014\",\n    address = \"Reykjavik, Iceland\",\n    publisher = \"European Language Resources Association (ELRA)\",\n    url = \"http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf\",\n    pages = \"216--223\",\n}","description":"Shared and internationally recognized benchmarks are fundamental for the development of any computational system.\nWe aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them.\nSICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of CDSMs.\nBy means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral).\nThe SICK data set was used in SemEval-2014 Task 1, and it freely available for research purposes.","paperswithcode_id":"sick","key":""},{"id":"silicone","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:10K<n<100K","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_categories:text-classification","task_categories:text-scoring","task_ids:dialogue-modeling","task_ids:language-modeling","task_ids:text-classification-other-dialogue-act-classification","task_ids:text-classification-other-emotion-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{chapuis-etal-2020-hierarchical,\n    title = \"Hierarchical Pre-training for Sequence Labelling in Spoken Dialog\",\n    author = \"Chapuis, Emile  and\n      Colombo, Pierre  and\n      Manica, Matteo  and\n      Labeau, Matthieu  and\n      Clavel, Chlo{\\'e}\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.findings-emnlp.239\",\n    doi = \"10.18653/v1/2020.findings-emnlp.239\",\n    pages = \"2636--2648\",\n    abstract = \"Sequence labelling tasks like Dialog Act and Emotion/Sentiment identification are a\n        key component of spoken dialog systems. In this work, we propose a new approach to learn\n        generic representations adapted to spoken dialog, which we evaluate on a new benchmark we\n        call Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE benchmark (SILICONE).\n        SILICONE is model-agnostic and contains 10 different datasets of various sizes.\n        We obtain our representations with a hierarchical encoder based on transformer architectures,\n        for which we extend two well-known pre-training objectives. Pre-training is performed on\n        OpenSubtitles: a large corpus of spoken dialog containing over 2.3 billion of tokens. We\n        demonstrate how hierarchical encoders achieve competitive results with consistently fewer\n        parameters compared to state-of-the-art models and we show their importance for both\n        pre-training and fine-tuning.\",\n}","description":"The Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE (SILICONE) benchmark is a collection\n of resources for training, evaluating, and analyzing natural language understanding systems\n specifically designed for spoken language. All datasets are in the English language and cover a\n variety of domains including daily life, scripted scenarios, joint task completion, phone call\n conversations, and televsion dialogue. Some datasets additionally include emotion and/or sentimant\n labels.","key":""},{"id":"simple_questions_v2","tags":["annotations_creators:machine-generated","language_creators:found","languages:en","licenses:cc-by-3.0-at","multilinguality:monolingual","size_categories:100K<n<1M","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@misc{bordes2015largescale,\n      title={Large-scale Simple Question Answering with Memory Networks},\n      author={Antoine Bordes and Nicolas Usunier and Sumit Chopra and Jason Weston},\n      year={2015},\n      eprint={1506.02075},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}","description":"SimpleQuestions is a dataset for simple QA, which consists\nof a total of 108,442 questions written in natural language by human\nEnglish-speaking annotators each paired with a corresponding fact,\nformatted as (subject, relationship, object), that provides the answer\nbut also a complete explanation.  Fast have been extracted from the\nKnowledge Base Freebase (freebase.com).  We randomly shuffle these\nquestions and use 70% of them (75910) as training set, 10% as\nvalidation set (10845), and the remaining 20% as test set.","paperswithcode_id":"simplequestions","key":""},{"id":"siswati_ner_corpus","tags":["annotations_creators:expert-generated","language_creators:found","languages:ss","licenses:other-Creative Commons Attribution 2.5 South Africa License","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{siswati_ner_corpus,\n  author    = {B.B. Malangwane and\n               M.N. Kekana and\n               S.S. Sedibe and\n               B.C. Ndhlovu and\n              Roald Eiselen},\n  title     = {NCHLT Siswati Named Entity Annotated Corpus},\n  booktitle = {Eiselen, R. 2016. Government domain named entity recognition for South African languages. Proceedings of the 10th      Language Resource and Evaluation Conference, Portorož, Slovenia.},\n  year      = {2016},\n  url       = {https://repo.sadilar.org/handle/20.500.12185/346},\n}","description":"Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.","key":""},{"id":"smartdata","tags":["annotations_creators:expert-generated","language_creators:found","languages:de","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@InProceedings{SCHIERSCH18.85,\n  author = {Martin Schiersch and Veselina Mironova and Maximilian Schmitt and Philippe Thomas and Aleksandra Gabryszak and Leonhard Hennig},\n  title = \"{A German Corpus for Fine-Grained Named Entity Recognition and Relation Extraction of Traffic and Industry Events}\",\n  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},\n  year = {2018},\n  month = {May 7-12, 2018},\n  address = {Miyazaki, Japan},\n  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {979-10-95546-00-9},\n  language = {english}\n  }","description":"DFKI SmartData Corpus is a dataset of 2598 German-language documents\nwhich has been annotated with fine-grained geo-entities, such as streets,\nstops and routes, as well as standard named entity types. It has also\nbeen annotated with a set of 15 traffic- and industry-related n-ary\nrelations and events, such as Accidents, Traffic jams, Acquisitions,\nand Strikes. The corpus consists of newswire texts, Twitter messages,\nand traffic reports from radio stations, police and railway companies.\nIt allows for training and evaluating both named entity recognition\nalgorithms that aim for fine-grained typing of geo-entities, as well\nas n-ary relation extraction systems.","key":""},{"id":"sms_spam","tags":["annotations_creators:crowdsourced","annotations_creators:found","language_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-nus-sms-corpus","task_categories:text-classification","task_ids:intent-classification"],"citation":"@inproceedings{Almeida2011SpamFiltering,\n  title={Contributions to the Study of SMS Spam Filtering: New Collection and Results},\n  author={Tiago A. Almeida and Jose Maria Gomez Hidalgo and Akebo Yamakami},\n  year={2011},\n  booktitle = \"Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG'11)\",\n}","description":"The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research.\nIt has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam.","paperswithcode_id":"sms-spam-collection-data-set","key":""},{"id":"snips_built_in_intents","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:cc0-1.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_ids:intent-classification"],"citation":"@article{DBLP:journals/corr/abs-1805-10190,\n  author    = {Alice Coucke and\n               Alaa Saade and\n               Adrien Ball and\n               Th{\\'{e}}odore Bluche and\n               Alexandre Caulier and\n               David Leroy and\n               Cl{\\'{e}}ment Doumouro and\n               Thibault Gisselbrecht and\n               Francesco Caltagirone and\n               Thibaut Lavril and\n               Ma{\\\"{e}}l Primet and\n               Joseph Dureau},\n  title     = {Snips Voice Platform: an embedded Spoken Language Understanding system\n               for private-by-design voice interfaces},\n  journal   = {CoRR},\n  volume    = {abs/1805.10190},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1805.10190},\n  archivePrefix = {arXiv},\n  eprint    = {1805.10190},\n  timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-10190.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"Snips' built in intents dataset was initially used to compare different voice assistants and released as a public dataset hosted at\nhttps://github.com/sonos/nlu-benchmark 2016-12-built-in-intents. The dataset contains 328 utterances over 10 intent classes. The\nrelated paper mentioned on the github page is https://arxiv.org/abs/1805.10190 and a related Medium post is\nhttps://medium.com/snips-ai/benchmarking-natural-language-understanding-systems-d35be6ce568d .","paperswithcode_id":"snips","key":""},{"id":"snli","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:extended|other-flicker-30k","source_datasets:extended|other-visual-genome","task_categories:text-classification","task_ids:natural-language-inference"],"citation":"@inproceedings{snli:emnlp2015,\n    Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and Manning, Christopher D.},\n    Booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n    Publisher = {Association for Computational Linguistics},\n    Title = {A large annotated corpus for learning natural language inference},\n    Year = {2015}\n}","description":"The SNLI corpus (version 1.0) is a collection of 570k human-written English\nsentence pairs manually labeled for balanced classification with the labels\nentailment, contradiction, and neutral, supporting the task of natural language\ninference (NLI), also known as recognizing textual entailment (RTE).","paperswithcode_id":"snli","key":""},{"id":"snow_simplified_japanese_corpus","tags":["annotations_creators:crowdsourced","annotations_creators:other","language_creators:found","languages:en","languages:ja","licenses:cc-by-4.0","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{maruyama-yamamoto-2018-simplified,\n    title = \"Simplified Corpus with Core Vocabulary\",\n    author = \"Maruyama, Takumi  and\n      Yamamoto, Kazuhide\",\n    booktitle = \"Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)\",\n    month = may,\n    year = \"2018\",\n    address = \"Miyazaki, Japan\",\n    publisher = \"European Language Resources Association (ELRA)\",\n    url = \"https://www.aclweb.org/anthology/L18-1185\",\n}\n\n@inproceedings{yamamoto-2017-simplified-japanese,\n    title = \"やさしい⽇本語対訳コーパスの構築\",\n    author = \"⼭本 和英  and\n      丸⼭ 拓海  and\n      ⾓張 ⻯晴  and\n      稲岡 夢⼈  and\n      ⼩川 耀⼀朗  and\n      勝⽥ 哲弘  and\n      髙橋 寛治\",\n    booktitle = \"言語処理学会第23回年次大会\",\n    month = 3月,\n    year = \"2017\",\n    address = \"茨城, 日本\",\n    publisher = \"言語処理学会\",\n    url = \"https://www.anlp.jp/proceedings/annual_meeting/2017/pdf_dir/B5-1.pdf\",\n}\n\n@inproceedings{katsuta-yamamoto-2018-crowdsourced,\n    title = \"Crowdsourced Corpus of Sentence Simplification with Core Vocabulary\",\n    author = \"Katsuta, Akihiro  and\n      Yamamoto, Kazuhide\",\n    booktitle = \"Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)\",\n    month = may,\n    year = \"2018\",\n    address = \"Miyazaki, Japan\",\n    publisher = \"European Language Resources Association (ELRA)\",\n    url = \"https://www.aclweb.org/anthology/L18-1072\",\n}","description":"About SNOW T15: The simplified corpus for the Japanese language. The corpus has 50,000 manually simplified and aligned sentences. This corpus contains the original sentences, simplified sentences and English translation of the original sentences. It can be used for automatic text simplification as well as translating simple Japanese into English and vice-versa. The core vocabulary is restricted to 2,000 words where it is selected by accounting for several factors such as meaning preservation, variation, simplicity and the UniDic word segmentation criterion.\nFor details, refer to the explanation page of Japanese simplification (http://www.jnlp.org/research/Japanese_simplification). The original texts are from \"small_parallel_enja: 50k En/Ja Parallel Corpus for Testing SMT Methods\", which is a bilingual corpus for machine translation. About SNOW T23: An expansion corpus of 35,000 sentences rewritten in easy Japanese (simple Japanese vocabulary) based on SNOW T15. The original texts are from \"Tanaka Corpus\" (http://www.edrdg.org/wiki/index.php/Tanaka_Corpus).","key":""},{"id":"so_stacksample","tags":["annotations_creators:no-annotation","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:question-answering","task_ids:abstractive-qa","task_ids:open-domain-qa"],"description":"Dataset with the text of 10% of questions and answers from the Stack Overflow programming Q&A website.\n\nThis is organized as three tables:\n\nQuestions contains the title, body, creation date, closed date (if applicable), score, and owner ID for all non-deleted Stack Overflow questions whose Id is a multiple of 10.\nAnswers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.\nTags contains the tags on each of these questions.","key":""},{"id":"social_bias_frames","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-classification","task_ids:explanation-generation","task_ids:hate-speech-detection"],"citation":"@inproceedings{sap2020socialbiasframes,\n   title={Social Bias Frames: Reasoning about Social and Power Implications of Language},\n   author={Sap, Maarten and Gabriel, Saadia and Qin, Lianhui and Jurafsky, Dan and Smith, Noah A and Choi, Yejin},\n   year={2020},\n   booktitle={ACL},\n}","description":"Social Bias Frames is a new way of representing the biases and offensiveness that are implied in language.\nFor example, these frames are meant to distill the implication that \"women (candidates) are less qualified\"\nbehind the statement \"we shouldn’t lower our standards to hire more women.\"","key":""},{"id":"social_i_qa","tags":["languages:en"],"citation":"","description":"We introduce Social IQa: Social Interaction QA, a new question-answering benchmark for testing social commonsense intelligence. Contrary to many prior benchmarks that focus on physical or taxonomic knowledge, Social IQa focuses on reasoning about people’s actions and their social implications. For example, given an action like \"Jesse saw a concert\" and a question like \"Why did Jesse do this?\", humans can easily infer that Jesse wanted \"to see their favorite performer\" or \"to enjoy the music\", and not \"to see what's happening inside\" or \"to see if it works\". The actions in Social IQa span a wide variety of social situations, and answer candidates contain both human-curated answers and adversarially-filtered machine-generated candidates. Social IQa contains over 37,000 QA pairs for evaluating models’ abilities to reason about the social implications of everyday events and situations. (Less)","paperswithcode_id":"social-iqa","key":""},{"id":"sofc_materials_articles","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:sequence-modeling","task_categories:structure-prediction","task_categories:text-classification","task_ids:named-entity-recognition","task_ids:slot-filling","task_ids:topic-classification"],"citation":"@misc{friedrich2020sofcexp,\n      title={The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain},\n      author={Annemarie Friedrich and Heike Adel and Federico Tomazic and Johannes Hingerl and Renou Benteau and Anika Maruscyk and Lukas Lange},\n      year={2020},\n      eprint={2006.03039},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"The SOFC-Exp corpus consists of 45 open-access scholarly articles annotated by domain experts.\nA corpus and an inter-annotator agreement study demonstrate the complexity of the suggested\nnamed entity recognition and slot filling tasks as well as high annotation quality is presented\nin the accompanying paper.","key":""},{"id":"sogou_news","tags":[],"citation":"@misc{zhang2015characterlevel,\n    title={Character-level Convolutional Networks for Text Classification},\n    author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n    year={2015},\n    eprint={1509.01626},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}","description":"The Sogou News dataset is a mixture of 2,909,551 news articles from the SogouCA and SogouCS news corpora, in 5 categories.\nThe number of training samples selected for each class is 90,000 and testing 12,000. Note that the Chinese characters have been converted to Pinyin.\nclassification labels of the news are determined by their domain names in the URL. For example, the news with\nURL http://sports.sohu.com is categorized as a sport class.","key":""},{"id":"spanish_billion_words","tags":["annotations_creators:no-annotation","language_creators:expert-generated","languages:es","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10M<n<100M","source_datasets:original","task_categories:other","task_categories:sequence-modeling","task_ids:language-modeling","task_ids:other-other-pretraining-language-models"],"citation":"@misc{cardellinoSBWCE,\n     author = {Cardellino, Cristian},\n     title = {Spanish {B}illion {W}ords {C}orpus and {E}mbeddings},\n     url = {https://crscardellino.github.io/SBWCE/},\n     month = {August},\n     year = {2019}\n}","description":"An unannotated Spanish corpus of nearly 1.5 billion words, compiled from different resources from the web.\nThis resources include the spanish portions of SenSem, the Ancora Corpus, some OPUS Project Corpora and the Europarl,\nthe Tibidabo Treebank, the IULA Spanish LSP Treebank, and dumps from the Spanish Wikipedia, Wikisource and Wikibooks.\nThis corpus is a compilation of 100 text files. Each line of these files represents one of the 50 million sentences from the corpus.","paperswithcode_id":"sbwce","key":""},{"id":"spc","tags":["annotations_creators:found","language_creators:found","languages:af","languages:en","languages:el","languages:zh","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"This is a collection of parallel corpora collected by Hercules Dalianis and his research group for bilingual dictionary construction.\nMore information in: Hercules Dalianis, Hao-chun Xing, Xin Zhang: Creating a Reusable English-Chinese Parallel Corpus for Bilingual Dictionary Construction, In Proceedings of LREC2010 (source: http://people.dsv.su.se/~hercules/SEC/) and Konstantinos Charitakis (2007): Using Parallel Corpora to Create a Greek-English Dictionary with UPLUG, In Proceedings of NODALIDA 2007. Afrikaans-English: Aldin Draghoender and Mattias Kanhov: Creating a reusable English – Afrikaans parallel corpora for bilingual dictionary construction\n\n4 languages, 3 bitexts\ntotal number of files: 6\ntotal number of tokens: 1.32M\ntotal number of sentence fragments: 0.15M","key":""},{"id":"species_800","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@article{pafilis2013species,\n         title={The SPECIES and ORGANISMS resources for fast and accurate identification of taxonomic names in text},\n         author={Pafilis, Evangelos and Frankild, Sune P and Fanini, Lucia and Faulwetter, Sarah and Pavloudi, Christina and Vasileiadou, Aikaterini and Arvanitidis, Christos and Jensen, Lars Juhl},\n         journal={PloS one},\n         volume={8},\n         number={6},\n         pages={e65390},\n         year={2013},\n         publisher={Public Library of Science}\n}","description":"We have developed an efficient algorithm and implementation of a dictionary-based approach to named entity recognition,\nwhich we here use to identifynames of species and other taxa in text. The tool, SPECIES, is more than an order of\nmagnitude faster and as accurate as existing tools. The precision and recall was assessed both on an existing gold-standard\ncorpus and on a new corpus of 800 abstracts, which were manually annotated after the development of the tool. The corpus\ncomprises abstracts from journals selected to represent many taxonomic groups, which gives insights into which types of\norganism names are hard to detect and which are easy. Finally, we have tagged organism names in the entire Medline database\nand developed a web resource, ORGANISMS, that makes the results accessible to the broad community of biologists.","key":""},{"id":"spider","tags":["annotations_creators:expert-generated","language_creators:expert-generated","language_creators:machine-generated","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-stuctured-to-text"],"citation":"@article{yu2018spider,\n  title={Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task},\n  author={Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},\n  journal={arXiv preprint arXiv:1809.08887},\n  year={2018}\n}","description":"Spider is a large-scale complex and cross-domain semantic parsing and text-toSQL dataset annotated by 11 college students","paperswithcode_id":"spider-1","key":""},{"id":"squad","tags":["pretty_name:SQuAD","annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|wikipedia","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@article{2016arXiv160605250R,\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                 Konstantin and {Liang}, Percy},\n        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n      journal = {arXiv e-prints},\n         year = 2016,\n          eid = {arXiv:1606.05250},\n        pages = {arXiv:1606.05250},\narchivePrefix = {arXiv},\n       eprint = {1606.05250},\n}","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.","paperswithcode_id":"squad","key":""},{"id":"squad_adversarial","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|squad","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@inproceedings{jia-liang-2017-adversarial,\n    title = \"Adversarial Examples for Evaluating Reading Comprehension Systems\",\n    author = \"Jia, Robin  and\n      Liang, Percy\",\n    booktitle = \"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\",\n    month = sep,\n    year = \"2017\",\n    address = \"Copenhagen, Denmark\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D17-1215\",\n    doi = \"10.18653/v1/D17-1215\",\n    pages = \"2021--2031\",\n    abstract = \"Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.\",\n}","description":"Here are two different adversaries, each of which uses a different procedure to pick the sentence it adds to the paragraph:\nAddSent: Generates up to five candidate adversarial sentences that don't answer the question, but have a lot of words in common with the question. Picks the one that most confuses the model.\nAddOneSent: Similar to AddSent, but just picks one of the candidate sentences at random. This adversary is does not query the model in any way.","key":""},{"id":"squad_es","tags":[],"citation":"@article{2016arXiv160605250R,\n       author = {Casimiro Pio , Carrino and  Marta R. , Costa-jussa and  Jose A. R. , Fonollosa},\n        title = \"{Automatic Spanish Translation of the SQuAD Dataset for Multilingual\nQuestion Answering}\",\n      journal = {arXiv e-prints},\n         year = 2019,\n          eid = {arXiv:1912.05200v1},\n        pages = {arXiv:1912.05200v1},\narchivePrefix = {arXiv},\n       eprint = {1912.05200v2},\n}","description":"automatic translation of the Stanford Question Answering Dataset (SQuAD) v2 into Spanish","paperswithcode_id":"squad-es","key":""},{"id":"squad_it","tags":["annotations_creators:machine-generated","language_creators:machine-generated","languages:it-IT","licenses:unknown","multilinguality:monolingual","size_categories:unknown","source_datasets:extended|squad","task_categories:question-answering","task_ids:open-domain-qa","task_ids:extractive-qa"],"citation":"@InProceedings{10.1007/978-3-030-03840-3_29,\n    author={Croce, Danilo and Zelenanska, Alexandra and Basili, Roberto},\n    editor={Ghidini, Chiara and Magnini, Bernardo and Passerini, Andrea and Traverso, Paolo\",\n    title={Neural Learning for Question Answering in Italian},\n    booktitle={AI*IA 2018 -- Advances in Artificial Intelligence},\n    year={2018},\n    publisher={Springer International Publishing},\n    address={Cham},\n    pages={389--402},\n    isbn={978-3-030-03840-3}\n}","description":"SQuAD-it is derived from the SQuAD dataset and it is obtained through semi-automatic translation of the SQuAD dataset\ninto Italian. It represents a large-scale dataset for open question answering processes on factoid questions in Italian.\n The dataset contains more than 60,000 question/answer pairs derived from the original English dataset. The dataset is\n split into training and test sets to support the replicability of the benchmarking of QA systems:","paperswithcode_id":"squad-it","key":""},{"id":"squad_kor_v1","tags":["annotations_creators:crowdsourced","language_creators:found","languages:ko","licenses:cc-by-nd-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@article{lim2019korquad1,\n  title={Korquad1. 0: Korean qa dataset for machine reading comprehension},\n  author={Lim, Seungyoung and Kim, Myungji and Lee, Jooyoul},\n  journal={arXiv preprint arXiv:1909.07005},\n  year={2019}\n}","description":"KorQuAD 1.0 is a large-scale Korean dataset for machine reading comprehension task consisting of human generated questions for Wikipedia articles. We benchmark the data collecting process of SQuADv1.0 and crowdsourced 70,000+ question-answer pairs. 1,637 articles and 70,079 pairs of question answers were collected. 1,420 articles are used for the training set, 140 for the dev set, and 77 for the test set. 60,407 question-answer pairs are for the training set, 5,774 for the dev set, and 3,898 for the test set.","paperswithcode_id":"korquad","key":""},{"id":"squad_kor_v2","tags":["annotations_creators:crowdsourced","language_creators:found","languages:ko","licenses:cc-by-nd-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|squad_kor_v1","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@article{NODE09353166,\n    author={Youngmin Kim,Seungyoung Lim;Hyunjeong Lee;Soyoon Park;Myungji Kim},\n    title={{KorQuAD 2.0: Korean QA Dataset for Web Document Machine Comprehension}},\n    booltitle={{Journal of KIISE 제47권 제6호}},\n    journal={{Journal of KIISE}},\n    volume={{47}},\n    issue={{6}},\n    publisher={The Korean Institute of Information Scientists and Engineers},\n    year={2020},\n    ISSN={{2383-630X}},\n    pages={577-586},\n    url={http://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE09353166}}","description":"KorQuAD 2.0 is a Korean question and answering dataset consisting of a total of 100,000+ pairs. There are three major differences from KorQuAD 1.0, which is the standard Korean Q & A data. The first is that a given document is a whole Wikipedia page, not just one or two paragraphs. Second, because the document also contains tables and lists, it is necessary to understand the document structured with HTML tags. Finally, the answer can be a long text covering not only word or phrase units, but paragraphs, tables, and lists. As a baseline model, BERT Multilingual is used, released by Google as an open source. It shows 46.0% F1 score, a very low score compared to 85.7% of the human F1 score. It indicates that this data is a challenging task. Additionally, we increased the performance by no-answer data augmentation. Through the distribution of this data, we intend to extend the limit of MRC that was limited to plain text to real world tasks of various lengths and formats.","key":""},{"id":"squad_v1_pt","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:pt","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa","task_ids:open-domain-qa"],"citation":"@article{2016arXiv160605250R,\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                 Konstantin and {Liang}, Percy},\n        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n      journal = {arXiv e-prints},\n         year = 2016,\n          eid = {arXiv:1606.05250},\n        pages = {arXiv:1606.05250},\narchivePrefix = {arXiv},\n       eprint = {1606.05250},\n}","description":"Portuguese translation of the SQuAD dataset. The translation was performed automatically using the Google Cloud API.","key":""},{"id":"squad_v2","tags":["pretty_name:SQuAD2.0","annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa","task_ids:extractive-qa"],"citation":"@article{2016arXiv160605250R,\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                 Konstantin and {Liang}, Percy},\n        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n      journal = {arXiv e-prints},\n         year = 2016,\n          eid = {arXiv:1606.05250},\n        pages = {arXiv:1606.05250},\narchivePrefix = {arXiv},\n       eprint = {1606.05250},\n}","description":"combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers\n to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but\n also determine when no answer is supported by the paragraph and abstain from answering.","paperswithcode_id":"squad","key":""},{"id":"squadshifts","tags":["languages:en"],"citation":"@inproceedings{miller2020effect,\n  author = {J. Miller and K. Krauth and B. Recht and L. Schmidt},\n  booktitle = {International Conference on Machine Learning (ICML)},\n  title = {The Effect of Natural Distribution Shift on Question Answering Models},\n  year = {2020},\n}","paperswithcode_id":"squad-shifts","key":""},{"id":"srwac","tags":["annotations_creators:no-annotation","language_creators:found","languages:sr","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:100M<n<1B","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@misc{11356/1063,\n title = {Serbian web corpus {srWaC} 1.1},\n author = {Ljube{\\v s}i{\\'c}, Nikola and Klubi{\\v c}ka, Filip},\n url = {http://hdl.handle.net/11356/1063},\n note = {Slovenian language resource repository {CLARIN}.{SI}},\n copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},\n year = {2016} }","description":"The Serbian web corpus srWaC was built by crawling the .rs top-level domain in 2014. The corpus was near-deduplicated on paragraph level, normalised via diacritic restoration, morphosyntactically annotated and lemmatised. The corpus is shuffled by paragraphs. Each paragraph contains metadata on the URL, domain and language identification (Serbian vs. Croatian).\nVersion 1.0 of this corpus is described in http://www.aclweb.org/anthology/W14-0405. Version 1.1 contains newer and better linguistic annotations.","key":""},{"id":"sst","tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","multilinguality:monolingual","size_categories:10K<n<100K","size_categories:100K<n<1M","task_categories:text-classification","task_categories:text-scoring","task_ids:sentiment-classification","task_ids:sentiment-scoring"],"citation":"@inproceedings{socher-etal-2013-recursive,\n    title = \"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\",\n    author = \"Socher, Richard and Perelygin, Alex and Wu, Jean and\n      Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher\",\n    booktitle = \"Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing\",\n    month = oct,\n    year = \"2013\",\n    address = \"Seattle, Washington, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D13-1170\",\n    pages = \"1631--1642\",\n}","description":"The Stanford Sentiment Treebank, the first corpus with fully labeled parse trees that allows for a\ncomplete analysis of the compositional effects of sentiment in language.","paperswithcode_id":"sst","key":""},{"id":"stereoset","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-stereotype-detection"],"citation":"@article{nadeem2020Stereoset,\n  title={Stereoset: Measuring stereotypical bias in pretrained language models},\n  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},\n  journal={arXiv preprint arXiv:2004.09456},\n  year={2020}\n}","description":"Stereoset is a dataset that measures stereotype bias in language models. Stereoset consists of 17,000 sentences that\nmeasures model preferences across gender, race, religion, and profession.","paperswithcode_id":"stereoset","key":""},{"id":"stsb_mt_sv","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:machine-generated","languages:sv","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-sts-b","task_categories:text-scoring","task_ids:semantic-similarity-scoring"],"citation":"@article{isbister2020not,\n  title={Why Not Simply Translate? A First Swedish Evaluation Benchmark for Semantic Similarity},\n  author={Isbister, Tim and Sahlgren, Magnus},\n  journal={arXiv preprint arXiv:2009.03116},\n  year={2020}\n}","key":""},{"id":"stsb_multi_mt","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:found","language_creators:machine-generated","languages:en","languages:de","languages:es","languages:fr","languages:it","languages:nl","languages:pl","languages:pt","languages:ru","languages:zh","licenses:custom","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:extended|other-sts-b","task_categories:text-scoring","task_ids:semantic-similarity-scoring"],"citation":"@InProceedings{huggingface:dataset:stsb_multi_mt,\ntitle = {Machine translated multilingual STS benchmark dataset.},\nauthor={Philip May},\nyear={2021},\nurl={https://github.com/PhilipMay/stsb-multi-mt}\n}","description":"These are different multilingual translations and the English original of the STSbenchmark dataset. Translation has been done with deepl.com.","key":""},{"id":"style_change_detection","tags":[],"citation":"@inproceedings{bevendorff2020shared,\n  title={Shared Tasks on Authorship Analysis at PAN 2020},\n  author={Bevendorff, Janek and Ghanem, Bilal and Giachanou, Anastasia and Kestemont, Mike and Manjavacas, Enrique and Potthast, Martin and Rangel, Francisco and Rosso, Paolo and Specht, G{\\\"u}nther and Stamatatos, Efstathios and others},\n  booktitle={European Conference on Information Retrieval},\n  pages={508--516},\n  year={2020},\n  organization={Springer}\n}","description":"The goal of the style change detection task is to identify text positions within a given multi-author document at which the author switches. Detecting these positions is a crucial part of the authorship identification process, and for multi-author document analysis in general.\n\nAccess to the dataset needs to be requested from zenodo.","key":""},{"id":"subjqa","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","source_datasets:extended|yelp_review_full","source_datasets:extended|other-amazon_reviews_ucsd","source_datasets:extended|other-tripadvisor_reviews","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@inproceedings{bjerva20subjqa,\n    title = \"SubjQA: A Dataset for Subjectivity and Review Comprehension\",\n    author = \"Bjerva, Johannes  and\n      Bhutani, Nikita  and\n      Golahn, Behzad  and\n      Tan, Wang-Chiew  and\n      Augenstein, Isabelle\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\",\n    month = November,\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n}","description":"SubjQA is a question answering dataset that focuses on subjective questions and answers.\nThe dataset consists of roughly 10,000 questions over reviews from 6 different domains: books, movies, grocery,\nelectronics, TripAdvisor (i.e. hotels), and restaurants.","paperswithcode_id":"subjqa","key":""},{"id":"super_glue","tags":["languages:en"],"citation":"@article{wang2019superglue,\n  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},\n  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},\n  journal={arXiv preprint arXiv:1905.00537},\n  year={2019}\n}\n\nNote that each SuperGLUE dataset has its own citation. Please see the source to\nget the correct citation for each contained dataset.","description":"SuperGLUE (https://super.gluebenchmark.com/) is a new benchmark styled after\nGLUE with a new set of more difficult language understanding tasks, improved\nresources, and a new public leaderboard.","paperswithcode_id":"superglue","key":""},{"id":"superb","tags":["annotations_creators:other","language_creators:other","languages:en","licenses:unknown","multilinguality:monolingual","pretty_name:SUPERB","size_categories:unknown","source_datasets:original","source_datasets:extended|librispeech_asr","task_categories:speech-processing","task_ids:automatic-speech-recognition","task_ids:phoneme-recognition","task_ids:keyword-spotting","task_ids:query-by-example-spoken-term-detection","task_ids:speaker-identification","task_ids:automatic-speaker-verification","task_ids:speaker-diarization","task_ids:intent-classification","task_ids:slot-filling","task_ids:emotion-recognition"],"citation":"@article{DBLP:journals/corr/abs-2105-01051,\n  author    = {Shu{-}Wen Yang and\n               Po{-}Han Chi and\n               Yung{-}Sung Chuang and\n               Cheng{-}I Jeff Lai and\n               Kushal Lakhotia and\n               Yist Y. Lin and\n               Andy T. Liu and\n               Jiatong Shi and\n               Xuankai Chang and\n               Guan{-}Ting Lin and\n               Tzu{-}Hsien Huang and\n               Wei{-}Cheng Tseng and\n               Ko{-}tik Lee and\n               Da{-}Rong Liu and\n               Zili Huang and\n               Shuyan Dong and\n               Shang{-}Wen Li and\n               Shinji Watanabe and\n               Abdelrahman Mohamed and\n               Hung{-}yi Lee},\n  title     = {{SUPERB:} Speech processing Universal PERformance Benchmark},\n  journal   = {CoRR},\n  volume    = {abs/2105.01051},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.01051},\n  archivePrefix = {arXiv},\n  eprint    = {2105.01051},\n  timestamp = {Thu, 01 Jul 2021 13:30:22 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-01051.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"Self-supervised learning (SSL) has proven vital for advancing research in\nnatural language processing (NLP) and computer vision (CV). The paradigm\npretrains a shared model on large volumes of unlabeled data and achieves\nstate-of-the-art (SOTA) for various tasks with minimal adaptation. However, the\nspeech processing community lacks a similar setup to systematically explore the\nparadigm. To bridge this gap, we introduce Speech processing Universal\nPERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the\nperformance of a shared model across a wide range of speech processing tasks\nwith minimal architecture changes and labeled data. Among multiple usages of the\nshared model, we especially focus on extracting the representation learned from\nSSL due to its preferable re-usability. We present a simple framework to solve\nSUPERB tasks by learning task-specialized lightweight prediction heads on top of\nthe frozen shared model. Our results demonstrate that the framework is promising\nas SSL representations show competitive generalizability and accessibility\nacross SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a\nbenchmark toolkit to fuel the research in representation learning and general\nspeech processing.\n\nNote that in order to limit the required storage for preparing this dataset, the\naudio is stored in the .flac format and is not converted to a float32 array. To\nconvert, the audio file to a float32 array, please make use of the `.map()`\nfunction as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```","key":""},{"id":"swag","tags":["annotations_creators:crowdsourced","annotations_creators:machine-generated","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:natural-language-inference"],"citation":"@inproceedings{zellers2018swagaf,\n    title={SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference},\n    author={Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},\n    booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    year={2018}\n}","description":"Given a partial description like \"she opened the hood of the car,\"\nhumans can reason about the situation and anticipate what might come\nnext (\"then, she examined the engine\"). SWAG (Situations With Adversarial Generations)\nis a large-scale dataset for this task of grounded commonsense\ninference, unifying natural language inference and physically grounded reasoning.\n\nThe dataset consists of 113k multiple choice questions about grounded situations\n(73k training, 20k validation, 20k test).\nEach question is a video caption from LSMDC or ActivityNet Captions,\nwith four answer choices about what might happen next in the scene.\nThe correct answer is the (real) video caption for the next event in the video;\nthe three incorrect answers are adversarially generated and human verified,\nso as to fool machines but not humans. SWAG aims to be a benchmark for\nevaluating grounded commonsense NLI and for learning representations.\n\nThe full data contain more information,\nbut the regular configuration will be more interesting for modeling\n(note that the regular data are shuffled). The test set for leaderboard submission\nis under the regular configuration.","paperswithcode_id":"swag","key":""},{"id":"swahili","tags":["annotations_creators:no-annotation","language_creators:expert-generated","languages:sw","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@InProceedings{huggingface:dataset,\ntitle = Language modeling data for Swahili (Version 1),\nauthors={Shivachi Casper Shikali, & Mokhosi Refuoe.\n},\nyear={2019},\nlink = http://doi.org/10.5281/zenodo.3553423\n}","description":"The Swahili dataset developed specifically for language modeling task.\nThe dataset contains 28,000 unique words with 6.84M, 970k, and 2M words for the train,\nvalid and test partitions respectively which represent the ratio 80:10:10.\nThe entire dataset is lowercased, has no punctuation marks and,\nthe start and end of sentence markers have been incorporated to facilitate easy tokenization during language modeling.","key":""},{"id":"swahili_news","tags":["annotations_creators:expert-generated","language_creators:found","languages:sw","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"citation":"@dataset{davis_david_2020_4300294,\n  author       = {Davis David},\n  title        = {Swahili : News Classification Dataset},\n  month        = dec,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.4300294},\n  url          = {https://doi.org/10.5281/zenodo.4300294}\n}","description":"Swahili is spoken by 100-150 million people across East Africa. In Tanzania, it is one of two national languages (the other is English) and it is the official language of instruction in all schools. News in Swahili is an important part of the media sphere in Tanzania.\n\nNews contributes to education, technology, and the economic growth of a country, and news in local languages plays an important cultural role in many Africa countries. In the modern age, African languages in news and other spheres are at risk of being lost as English becomes the dominant language in online spaces.\n\n The Swahili news dataset was created to reduce the gap of using the Swahili language to create NLP technologies and help AI practitioners in Tanzania and across Africa continent to practice their NLP skills to solve different problems in organizations or societies related to Swahili language. Swahili News were collected from different websites that provide news in the Swahili language. I was able to find some websites that provide news in Swahili only and others in different languages including Swahili.\n\nThe dataset was created for a specific task of text classification, this means each news content can be categorized into six different topics (Local news, International news , Finance news, Health news, Sports news, and Entertainment news). The dataset comes with a specified train/test split. The train set contains 75% of the dataset and test set contains 25% of the dataset.","key":""},{"id":"swda","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:cc-by-nc-sa-3.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:extended|other-Switchboard-1 Telephone Speech Corpus, Release 2","task_categories:text-classification","task_ids:multi-label-classification"],"citation":"@techreport{Jurafsky-etal:1997,\n    Address = {Boulder, CO},\n    Author = {Jurafsky, Daniel and Shriberg, Elizabeth and Biasca, Debra},\n    Institution = {University of Colorado, Boulder Institute of Cognitive Science},\n    Number = {97-02},\n    Title = {Switchboard {SWBD}-{DAMSL} Shallow-Discourse-Function Annotation Coders Manual, Draft 13},\n    Year = {1997}}\n\n@article{Shriberg-etal:1998,\n    Author = {Shriberg, Elizabeth and Bates, Rebecca and Taylor, Paul and Stolcke, Andreas and Jurafsky, Daniel and Ries, Klaus and Coccaro, Noah and Martin, Rachel and Meteer, Marie and Van Ess-Dykema, Carol},\n    Journal = {Language and Speech},\n    Number = {3--4},\n    Pages = {439--487},\n    Title = {Can Prosody Aid the Automatic Classification of Dialog Acts in Conversational Speech?},\n    Volume = {41},\n    Year = {1998}}\n\n@article{Stolcke-etal:2000,\n    Author = {Stolcke, Andreas and Ries, Klaus and Coccaro, Noah and Shriberg, Elizabeth and Bates, Rebecca and Jurafsky, Daniel and Taylor, Paul and Martin, Rachel and Meteer, Marie and Van Ess-Dykema, Carol},\n    Journal = {Computational Linguistics},\n    Number = {3},\n    Pages = {339--371},\n    Title = {Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech},\n    Volume = {26},\n    Year = {2000}}","description":"The Switchboard Dialog Act Corpus (SwDA) extends the Switchboard-1 Telephone Speech Corpus, Release 2 with\nturn/utterance-level dialog-act tags. The tags summarize syntactic, semantic, and pragmatic information about the\nassociated turn. The SwDA project was undertaken at UC Boulder in the late 1990s.\nThe SwDA is not inherently linked to the Penn Treebank 3 parses of Switchboard, and it is far from straightforward to\nalign the two resources. In addition, the SwDA is not distributed with the Switchboard's tables of metadata about the\nconversations and their participants.","key":""},{"id":"swedish_ner_corpus","tags":["annotations_creators:expert-generated","language_creators:found","languages:sv","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"description":"Webbnyheter 2012 from Spraakbanken, semi-manually annotated and adapted for CoreNLP Swedish NER. Semi-manually defined in this case as: Bootstrapped from Swedish Gazetters then manually correcte/reviewed by two independent native speaking swedish annotators. No annotator agreement calculated.","key":""},{"id":"swedish_reviews","tags":["annotations_creators:found","language_creators:found","languages:sv","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"key":""},{"id":"tab_fact","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:fact-checking"],"citation":"@inproceedings{2019TabFactA,\n  title={TabFact : A Large-scale Dataset for Table-based Fact Verification},\n  author={Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou and William Yang Wang},\n  booktitle = {International Conference on Learning Representations (ICLR)},\n  address = {Addis Ababa, Ethiopia},\n  month = {April},\n  year = {2020}\n}","description":"The problem of verifying whether a textual hypothesis holds the truth based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are restricted to dealing with unstructured textual evidence (e.g., sentences and passages, a pool of passages), while verification using structured forms of evidence, such as tables, graphs, and databases, remains unexplored. TABFACT is large scale dataset with 16k Wikipedia tables as evidence for 118k human annotated statements designed for fact verification with semi-structured evidence. The statements are labeled as either ENTAILED or REFUTED. TABFACT is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning.","paperswithcode_id":"tabfact","key":""},{"id":"tamilmixsentiment","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","languages:ta","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{chakravarthi-etal-2020-corpus,\n    title = \"Corpus Creation for Sentiment Analysis in Code-Mixed {T}amil-{E}nglish Text\",\n    author = \"Chakravarthi, Bharathi Raja  and\n      Muralidaran, Vigneshwaran  and\n      Priyadharshini, Ruba  and\n      McCrae, John Philip\",\n    booktitle = \"Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources association\",\n    url = \"https://www.aclweb.org/anthology/2020.sltu-1.28\",\n    pages = \"202--210\",\n    abstract = \"Understanding the sentiment of a comment from a video or an image is an essential task in many applications. Sentiment analysis of a text can be useful for various decision-making processes. One such application is to analyse the popular sentiments of videos on social media based on viewer comments. However, comments from social media do not follow strict rules of grammar, and they contain mixing of more than one language, often written in non-native scripts. Non-availability of annotated code-mixed data for a low-resourced language like Tamil also adds difficulty to this problem. To overcome this, we created a gold standard Tamil-English code-switched, sentiment-annotated corpus containing 15,744 comment posts from YouTube. In this paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark.\",\n    language = \"English\",\n    ISBN = \"979-10-95546-35-1\",\n}","description":"The first gold standard Tamil-English code-switched, sentiment-annotated corpus containing 15,744 comment posts from YouTube. Train: 11,335 Validation: 1,260 and Test: 3,149.  This makes the largest general domain sentiment dataset for this relatively low-resource language with code-mixing phenomenon.  The dataset contains all the three types of code-mixed sentences - Inter-Sentential switch, Intra-Sentential switch and Tag switching. Most comments were written in Roman script with either Tamil grammar with English lexicon or English grammar with Tamil lexicon. Some comments were written in Tamil script with English expressions in between.","key":""},{"id":"tanzil","tags":["annotations_creators:found","language_creators:found","languages:am","languages:ar","languages:az","languages:bg","languages:bn","languages:bs","languages:cs","languages:de","languages:dv","languages:en","languages:es","languages:fa","languages:fr","languages:ha","languages:hi","languages:id","languages:it","languages:ja","languages:ko","languages:ku","languages:ml","languages:ms","languages:nl","languages:no","languages:pl","languages:pt","languages:ro","languages:ru","languages:sd","languages:so","languages:sq","languages:sv","languages:sw","languages:ta","languages:tg","languages:th","languages:tr","languages:tt","languages:ug","languages:ur","languages:uz","languages:zh","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)","description":"This is a collection of Quran translations compiled by the Tanzil project\nThe translations provided at this page are for non-commercial purposes only. If used otherwise, you need to obtain necessary permission from the translator or the publisher.\n\nIf you are using more than three of the following translations in a website or application, we require you to put a link back to this page to make sure that subsequent users have access to the latest updates.\n\n42 languages, 878 bitexts\ntotal number of files: 105\ntotal number of tokens: 22.33M\ntotal number of sentence fragments: 1.01M","key":""},{"id":"tapaco","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:af","languages:ar","languages:az","languages:be","languages:ber","languages:bg","languages:bn","languages:br","languages:ca","languages:cbk","languages:cmn","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:eo","languages:es","languages:et","languages:eu","languages:fi","languages:fr","languages:gl","languages:gos","languages:he","languages:hi","languages:hr","languages:hu","languages:hy","languages:ia","languages:id","languages:ie","languages:io","languages:is","languages:it","languages:ja","languages:jbo","languages:kab","languages:ko","languages:kw","languages:la","languages:lfn","languages:lt","languages:mk","languages:mr","languages:nb","languages:nds","languages:nl","languages:orv","languages:ota","languages:pes","languages:pl","languages:pt","languages:rn","languages:ro","languages:ru","languages:sl","languages:sr","languages:sv","languages:tk","languages:tl","languages:tlh","languages:toki","languages:tr","languages:tt","languages:ug","languages:uk","languages:ur","languages:vi","languages:vo","languages:war","languages:wuu","languages:yue","licenses:cc-by-2.0","multilinguality:multilingual","size_categories:n<1K","size_categories:1M<n<10M","size_categories:1K<n<10K","size_categories:10K<n<100K","size_categories:100K<n<1M","source_datasets:extended|other-tatoeba","task_categories:conditional-text-generation","task_categories:text-classification","task_ids:conditional-text-generation-other-given-a-sentence-generate-a-paraphrase-either-in-same-language-or-another-language","task_ids:machine-translation","task_ids:semantic-similarity-classification"],"citation":"@dataset{scherrer_yves_2020_3707949,\n  author       = {Scherrer, Yves},\n  title        = {{TaPaCo: A Corpus of Sentential Paraphrases for 73 Languages}},\n  month        = mar,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {1.0},\n  doi          = {10.5281/zenodo.3707949},\n  url          = {https://doi.org/10.5281/zenodo.3707949}\n}","description":"A freely available paraphrase corpus for 73 languages extracted from the Tatoeba database. Tatoeba is a crowdsourcing project mainly geared towards language learners. Its aim is to provide example sentences and translations for particular linguistic constructions and words. The paraphrase corpus is created by populating a graph with Tatoeba sentences and equivalence links between sentences “meaning the same thing”. This graph is then traversed to extract sets of paraphrases. Several language-independent filters and pruning steps are applied to remove uninteresting sentences. A manual evaluation performed on three languages shows that between half and three quarters of inferred paraphrases are correct and that most remaining ones are either correct but trivial, or near-paraphrases that neutralize a morphological distinction. The corpus contains a total of 1.9 million sentences, with 200 – 250 000 sentences per language. It covers a range of languages for which, to our knowledge,no other paraphrase dataset exists.","paperswithcode_id":"tapaco","key":""},{"id":"tashkeela","tags":["annotations_creators:no-annotation","language_creators:found","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling","task_ids:other-diacritics-prediction"],"citation":"@article{zerrouki2017tashkeela,\n  title={Tashkeela: Novel corpus of Arabic vocalized texts, data for auto-diacritization systems},\n  author={Zerrouki, Taha and Balla, Amar},\n  journal={Data in brief},\n  volume={11},\n  pages={147},\n  year={2017},\n  publisher={Elsevier}\n}","description":"Arabic vocalized texts.\nit contains 75 million of fully vocalized words mainly97 books from classical and modern Arabic language.","key":""},{"id":"taskmaster1","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:dialogue-modeling"],"citation":"@inproceedings{48484,\ntitle\t= {Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset},\nauthor\t= {Bill Byrne and Karthik Krishnamoorthi and Chinnadhurai Sankar and Arvind Neelakantan and Daniel Duckworth and Semih Yavuz and Ben Goodrich and Amit Dubey and Kyu-Young Kim and Andy Cedilnik},\nyear\t= {2019}\n}","description":"Taskmaster-1 is a  goal-oriented conversational dataset. It includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken \"Wizard of Oz\" (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is \"self-dialog\" in which crowdsourced workers write the entire dialog themselves.","paperswithcode_id":"taskmaster-1","key":""},{"id":"taskmaster2","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_ids:dialogue-modeling"],"citation":"@inproceedings{48484,\ntitle\t= {Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset},\nauthor\t= {Bill Byrne and Karthik Krishnamoorthi and Chinnadhurai Sankar and Arvind Neelakantan and Daniel Duckworth and Semih Yavuz and Ben Goodrich and Amit Dubey and Kyu-Young Kim and Andy Cedilnik},\nyear\t= {2019}\n}","description":"Taskmaster is dataset for goal oriented conversations. The Taskmaster-2 dataset consists of 17,289 dialogs in the seven domains which include restaurants, food ordering, movies, hotels, flights, music and sports. Unlike Taskmaster-1, which includes both written \"self-dialogs\" and spoken two-person dialogs, Taskmaster-2 consists entirely of spoken two-person dialogs. In addition, while Taskmaster-1 is almost exclusively task-based, Taskmaster-2 contains a good number of search- and recommendation-oriented dialogs. All dialogs in this release were created using a Wizard of Oz (WOz) methodology in which crowdsourced workers played the role of a 'user' and trained call center operators played the role of the 'assistant'. In this way, users were led to believe they were interacting with an automated system that “spoke” using text-to-speech (TTS) even though it was in fact a human behind the scenes. As a result, users could express themselves however they chose in the context of an automated interface.","paperswithcode_id":"taskmaster-2","key":""},{"id":"taskmaster3","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_ids:dialogue-modeling"],"citation":"@inproceedings{48484,\ntitle\t= {Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset},\nauthor\t= {Bill Byrne and Karthik Krishnamoorthi and Chinnadhurai Sankar and Arvind Neelakantan and Daniel Duckworth and Semih Yavuz and Ben Goodrich and Amit Dubey and Kyu-Young Kim and Andy Cedilnik},\nyear\t= {2019}\n}","description":"Taskmaster is dataset for goal oriented conversations. The Taskmaster-3 dataset consists of 23,757 movie ticketing dialogs. By \"movie ticketing\" we mean conversations where the customer's goal is to purchase tickets after deciding on theater, time, movie name, number of tickets, and date, or opt out of the transaction. This collection was created using the \"self-dialog\" method. This means a single, crowd-sourced worker is paid to create a conversation writing turns for both speakers, i.e. the customer and the ticketing agent.","key":""},{"id":"tatoeba","tags":["annotations_creators:found","language_creators:found","languages:ab","languages:acm","languages:ady","languages:af","languages:afb","languages:afh","languages:aii","languages:ain","languages:ajp","languages:akl","languages:aln","languages:am","languages:an","languages:ang","languages:aoz","languages:apc","languages:ar","languages:arq","languages:ary","languages:arz","languages:as","languages:ast","languages:avk","languages:awa","languages:ayl","languages:az","languages:ba","languages:bal","languages:bar","languages:be","languages:ber","languages:bg","languages:bho","languages:bjn","languages:bm","languages:bn","languages:bo","languages:br","languages:brx","languages:bs","languages:bua","languages:bvy","languages:bzt","languages:ca","languages:cay","languages:cbk","languages:ce","languages:ceb","languages:ch","languages:chg","languages:chn","languages:cho","languages:chr","languages:cjy","languages:ckb","languages:ckt","languages:cmn","languages:co","languages:cpi","languages:crh","languages:crk","languages:cs","languages:csb","languages:cv","languages:cy","languages:cycl","languages:da","languages:de","languages:dng","languages:drt","languages:dsb","languages:dtp","languages:dv","languages:dws","languages:ee","languages:egl","languages:el","languages:emx","languages:en","languages:enm","languages:eo","languages:es","languages:et","languages:eu","languages:ext","languages:fi","languages:fj","languages:fkv","languages:fo","languages:fr","languages:frm","languages:fro","languages:frr","languages:fuc","languages:fur","languages:fuv","languages:fy","languages:ga","languages:gag","languages:gan","languages:gbm","languages:gcf","languages:gd","languages:gil","languages:gl","languages:gn","languages:gom","languages:gos","languages:got","languages:grc","languages:gsw","languages:gu","languages:gv","languages:ha","languages:hak","languages:haw","languages:hbo","languages:he","languages:hi","languages:hif","languages:hil","languages:hnj","languages:hoc","languages:hr","languages:hrx","languages:hsb","languages:hsn","languages:ht","languages:hu","languages:hy","languages:ia","languages:iba","languages:id","languages:ie","languages:ig","languages:ii","languages:ike","languages:ilo","languages:io","languages:is","languages:it","languages:izh","languages:ja","languages:jam","languages:jbo","languages:jdt","languages:jpa","languages:jv","languages:ka","languages:kaa","languages:kab","languages:kam","languages:kek","languages:kha","languages:kjh","languages:kk","languages:kl","languages:km","languages:kmr","languages:kn","languages:ko","languages:koi","languages:kpv","languages:krc","languages:krl","languages:ksh","languages:ku","languages:kum","languages:kw","languages:kxi","languages:ky","languages:kzj","languages:la","languages:laa","languages:lad","languages:lb","languages:ldn","languages:lfn","languages:lg","languages:lij","languages:liv","languages:lkt","languages:lld","languages:lmo","languages:ln","languages:lo","languages:lt","languages:ltg","languages:lut","languages:lv","languages:lzh","languages:lzz","languages:mad","languages:mai","languages:max","languages:mdf","languages:mfe","languages:mg","languages:mgm","languages:mh","languages:mhr","languages:mi","languages:mic","languages:min","languages:mk","languages:ml","languages:mn","languages:mni","languages:mnw","languages:moh","languages:mr","languages:mt","languages:mvv","languages:mwl","languages:mww","languages:my","languages:myv","languages:na","languages:nah","languages:nan","languages:nb","languages:nch","languages:nds","languages:ngt","languages:ngu","languages:niu","languages:nl","languages:nlv","languages:nn","languages:nog","languages:non","languages:nov","languages:npi","languages:nst","languages:nus","languages:nv","languages:ny","languages:nys","languages:oar","languages:oc","languages:ofs","languages:ood","languages:or","languages:orv","languages:os","languages:osp","languages:ota","languages:otk","languages:pa","languages:pag","languages:pal","languages:pam","languages:pap","languages:pau","languages:pcd","languages:pdc","languages:pes","languages:phn","languages:pi","languages:pl","languages:pms","languages:pnb","languages:ppl","languages:prg","languages:ps","languages:pt","languages:qu","languages:quc","languages:qya","languages:rap","languages:rif","languages:rm","languages:rn","languages:ro","languages:rom","languages:ru","languages:rue","languages:rw","languages:sa","languages:sah","languages:sc","languages:scn","languages:sco","languages:sd","languages:sdh","languages:se","languages:sg","languages:sgs","languages:shs","languages:shy","languages:si","languages:sjn","languages:sl","languages:sm","languages:sma","languages:sn","languages:so","languages:sq","languages:sr","languages:stq","languages:su","languages:sux","languages:sv","languages:swg","languages:swh","languages:syc","languages:ta","languages:te","languages:tet","languages:tg","languages:th","languages:thv","languages:ti","languages:tig","languages:tk","languages:tl","languages:tlh","languages:tly","languages:tmr","languages:tmw","languages:tn","languages:to","languages:toi","languages:toki","languages:tpi","languages:tpw","languages:tr","languages:ts","languages:tt","languages:tts","languages:tvl","languages:ty","languages:tyv","languages:tzl","languages:udm","languages:ug","languages:uk","languages:umb","languages:ur","languages:uz","languages:vec","languages:vep","languages:vi","languages:vo","languages:vro","languages:wa","languages:war","languages:wo","languages:wuu","languages:xal","languages:xh","languages:xqa","languages:yi","languages:yo","languages:yue","languages:zlm","languages:zsm","languages:zu","languages:zza","licenses:cc-by-2.0","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"This is a collection of translated sentences from Tatoeba\n359 languages, 3,403 bitexts\ntotal number of files: 750\ntotal number of tokens: 65.54M\ntotal number of sentence fragments: 8.96M","paperswithcode_id":"tatoeba","key":""},{"id":"ted_hrlr","tags":[],"citation":"@inproceedings{Ye2018WordEmbeddings,\n  author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},\n  title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},\n  booktitle = {HLT-NAACL},\n  year    = {2018},\n  }","description":"Data sets derived from TED talk transcripts for comparing similar language pairs\nwhere one is high resource and the other is low resource.","key":""},{"id":"ted_iwlst2013","tags":["annotations_creators:found","language_creators:found","languages:ar","languages:en","languages:de","languages:es","languages:fa","languages:fr","languages:it","languages:nl","languages:pl","languages:pt","languages:ro","languages:ru","languages:sl","languages:tr","languages:zh","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)","description":"A parallel corpus of TED talk subtitles provided by CASMACAT: http://www.casmacat.eu/corpus/ted2013.html. The files are originally provided by https://wit3.fbk.eu.\n\n15 languages, 14 bitexts\ntotal number of files: 28\ntotal number of tokens: 67.67M\ntotal number of sentence fragments: 3.81M","key":""},{"id":"ted_multi","tags":[],"citation":"@InProceedings{qi-EtAl:2018:N18-2,\n  author    = {Qi, Ye  and  Sachan, Devendra  and  Felix, Matthieu  and  Padmanabhan, Sarguna  and  Neubig, Graham},\n  title     = {When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?},\n  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},\n  month     = {June},\n  year      = {2018},\n  address   = {New Orleans, Louisiana},\n  publisher = {Association for Computational Linguistics},\n  pages     = {529--535},\n  abstract  = {The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases -- providing gains of up to 20 BLEU points in the most favorable setting.},\n  url       = {http://www.aclweb.org/anthology/N18-2084}\n}","description":"Massively multilingual (60 language) data set derived from TED Talk transcripts.\nEach record consists of parallel arrays of language and text. Missing and\nincomplete translations will be filtered out.","key":""},{"id":"ted_talks_iwslt","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","language_creators:expert-generated","languages:mr","languages:eu","languages:hr","languages:rup","languages:szl","languages:lo","languages:ms","languages:ht","languages:hy","languages:mg","languages:arq","languages:uk","languages:ku","languages:ig","languages:sr","languages:ug","languages:ne","languages:pt-br","languages:sq","languages:af","languages:km","languages:en","languages:tt","languages:ja","languages:inh","languages:mn","languages:eo","languages:ka","languages:nb","languages:fil","languages:uz","languages:fi","languages:tl","languages:el","languages:tg","languages:bn","languages:si","languages:gu","languages:sk","languages:kn","languages:ar","languages:hup","languages:zh-tw","languages:sl","languages:be","languages:bo","languages:fr","languages:ps","languages:tr","languages:ltg","languages:la","languages:ko","languages:lv","languages:nl","languages:fa","languages:ru","languages:et","languages:vi","languages:pa","languages:my","languages:sw","languages:az","languages:sv","languages:ga","languages:sh","languages:it","languages:da","languages:lt","languages:kk","languages:mk","languages:tlh","languages:he","languages:ceb","languages:bg","languages:fr-ca","languages:ha","languages:ml","languages:mt","languages:as","languages:pt","languages:zh-cn","languages:cnh","languages:ro","languages:hi","languages:es","languages:id","languages:bs","languages:so","languages:cs","languages:te","languages:ky","languages:hu","languages:th","languages:pl","languages:nn","languages:ca","languages:is","languages:ta","languages:de","languages:srp","languages:ast","languages:bi","languages:lb","languages:art-x-bork","languages:am","languages:oc","languages:zh","languages:ur","languages:gl","licenses:cc-by-nc-4.0","multilinguality:translation","size_categories:1K<n<10K","size_categories:n<1K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{cettolo-etal-2012-wit3,\n    title = \"{WIT}3: Web Inventory of Transcribed and Translated Talks\",\n    author = \"Cettolo, Mauro  and\n      Girardi, Christian  and\n      Federico, Marcello\",\n    booktitle = \"Proceedings of the 16th Annual conference of the European Association for Machine Translation\",\n    month = may # \" 28{--}30\",\n    year = \"2012\",\n    address = \"Trento, Italy\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"https://www.aclweb.org/anthology/2012.eamt-1.60\",\n    pages = \"261--268\",\n}","description":"The core of WIT3 is the TED Talks corpus, that basically redistributes the original content published by the TED Conference website (http://www.ted.com). Since 2007,\nthe TED Conference, based in California, has been posting all video recordings of its talks together with subtitles in English\nand their translations in more than 80 languages. Aside from its cultural and social relevance, this content, which is published under the Creative Commons BYNC-ND license, also represents a precious\nlanguage resource for the machine translation research community, thanks to its size, variety of topics, and covered languages.\nThis effort repurposes the original content in a way which is more convenient for machine translation researchers.","key":""},{"id":"telugu_books","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:te","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@InProceedings{huggingface:dataset,\ntitle = {Indic NLP - Natural Language Processing for Indian Languages},\nauthors = {Sudalai Rajkumar, Anusha Motamarri},\nyear={2019}\n}","description":"This dataset is created by scraping telugu novels from teluguone.com this dataset can be used for nlp tasks like topic modeling, word embeddings, transfer learning etc","key":""},{"id":"telugu_news","tags":["annotations_creators:machine-generated","language_creators:other","languages:te","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:sequence-modeling","task_categories:text-classification","task_ids:language-modeling","task_ids:multi-class-classification","task_ids:topic-classification"],"citation":"@InProceedings{kaggle:dataset,\ntitle = {Telugu News - Natural Language Processing for Indian Languages},\nauthors={Sudalai Rajkumar, Anusha Motamarri},\nyear={2019}\n}","description":"This dataset contains Telugu language news articles along with respective\ntopic labels (business, editorial, entertainment, nation, sport) extracted from\nthe daily Andhra Jyoti. This dataset could be used to build Classification and Language Models.","key":""},{"id":"tep_en_fa_para","tags":["annotations_creators:found","language_creators:found","languages:en","languages:fa","licenses:unknown","multilinguality:translation","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{“TEP: Tehran English-Persian Parallel Corpus”,\ntitle = {TEP: Tehran English-Persian Parallel Corpus”, in proceedings of 12th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2011)},\nauthors={M. T. Pilevar, H. Faili, and A. H. Pilevar, },\nyear={2011}\n}","description":"TEP: Tehran English-Persian parallel corpus. The first free Eng-Per corpus, provided by the Natural Language and Text Processing Laboratory, University of Tehran.","key":""},{"id":"thai_toxicity_tweet","tags":["annotations_creators:expert-generated","language_creators:found","languages:th","licenses:cc-by-nc-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@article{sirihattasak2019annotation,\n  title={Annotation and Classification of Toxicity for Thai Twitter},\n  author={Sirihattasak, Sugan and Komachi, Mamoru and Ishikawa, Hiroshi},\n  year={2019}\n}","description":"Thai Toxicity Tweet Corpus contains 3,300 tweets annotated by humans with guidelines including a 44-word dictionary.\nThe author obtained 2,027 and 1,273 toxic and non-toxic tweets, respectively; these were labeled by three annotators. The result of corpus\nanalysis indicates that tweets that include toxic words are not always toxic. Further, it is more likely that a tweet is toxic, if it contains\ntoxic words indicating their original meaning. Moreover, disagreements in annotation are primarily because of sarcasm, unclear existing\ntarget, and word sense ambiguity.\n\nNotes from data cleaner: The data is included into [huggingface/datasets](https://www.github.com/huggingface/datasets) in Dec 2020.\nBy this time, 506 of the tweets are not available publicly anymore. We denote these by `TWEET_NOT_FOUND` in `tweet_text`.\nProcessing can be found at [this PR](https://github.com/tmu-nlp/ThaiToxicityTweetCorpus/pull/1).","key":""},{"id":"thainer","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:found","language_creators:expert-generated","languages:th","licenses:cc-by-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-tirasaroj-aroonmanakun","task_categories:structure-prediction","task_ids:named-entity-recognition","task_ids:part-of-speech-tagging"],"citation":"@misc{Wannaphong Phatthiyaphaibun_2019,\n    title={wannaphongcom/thai-ner: ThaiNER 1.3},\n    url={https://zenodo.org/record/3550546},\n    DOI={10.5281/ZENODO.3550546},\n    abstractNote={Thai Named Entity Recognition},\n    publisher={Zenodo},\n    author={Wannaphong Phatthiyaphaibun},\n    year={2019},\n    month={Nov}\n}","description":"ThaiNER (v1.3) is a 6,456-sentence named entity recognition dataset created from expanding the 2,258-sentence\n[unnamed dataset](http://pioneer.chula.ac.th/~awirote/Data-Nutcha.zip) by\n[Tirasaroj and Aroonmanakun (2012)](http://pioneer.chula.ac.th/~awirote/publications/).\nIt is used to train NER taggers in [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp).\nThe NER tags are annotated by [Tirasaroj and Aroonmanakun (2012)]((http://pioneer.chula.ac.th/~awirote/publications/))\nfor 2,258 sentences and the rest by [@wannaphong](https://github.com/wannaphong/).\nThe POS tags are done by [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp)'s `perceptron` engine trained on `orchid_ud`.\n[@wannaphong](https://github.com/wannaphong/) is now the only maintainer of this dataset.","key":""},{"id":"thaiqa_squad","tags":["annotations_creators:expert-generated","language_creators:found","languages:th","licenses:cc-by-nc-sa-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-thaiqa","task_categories:question-answering","task_ids:extractive-qa","task_ids:open-domain-qa"],"citation":"No clear citation guidelines from source:\nhttps://aiforthai.in.th/corpus.php\nSQuAD version:\nhttps://github.com/PyThaiNLP/thaiqa_squad","description":"`thaiqa_squad` is an open-domain, extractive question answering dataset (4,000 questions in `train` and 74 questions in `dev`) in\n[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format, originally created by [NECTEC](https://www.nectec.or.th/en/) from\nWikipedia articles and adapted to [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format by [PyThaiNLP](https://github.com/PyThaiNLP/).","key":""},{"id":"thaisum","tags":["annotations_creators:no-annotation","language_creators:found","languages:th","licenses:mit","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_categories:sequence-modeling","task_ids:language-modeling","task_ids:summarization"],"citation":"@mastersthesis{chumpolsathien_2020,\n    title={Using Knowledge Distillation from Keyword Extraction to Improve the Informativeness of Neural Cross-lingual Summarization},\n    author={Chumpolsathien, Nakhun},\n    year={2020},\n    school={Beijing Institute of Technology}","description":"ThaiSum is a large-scale corpus for Thai text summarization obtained from several online news websites namely Thairath,\nThaiPBS, Prachathai, and The Standard. This dataset consists of over 350,000 article and summary pairs\nwritten by journalists.","key":""},{"id":"tilde_model","tags":["annotations_creators:found","language_creators:found","languages:bg","languages:cs","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:fi","languages:fr","languages:hr","languages:hu","languages:is","languages:it","languages:lt","languages:lv","languages:mt","languages:nl","languages:no","languages:pl","languages:pt","languages:ro","languages:ru","languages:sk","languages:sl","languages:sq","languages:sr","languages:sv","languages:tr","languages:uk","licenses:cc-by-sa-4.0","multilinguality:multilingual","size_categories:n<1K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"Roberts Rozis, Raivis Skadins, 2017, Tilde MODEL - Multilingual Open Data for EU Languages. Proceedings of the 21th Nordic Conference of Computational Linguistics NODALIDA 2017","description":"This is the Tilde MODEL Corpus – Multilingual Open Data for European Languages.\n\nThe data has been collected from sites allowing free use and reuse of its content, as well as from Public Sector web sites. The activities have been undertaken as part of the ODINE Open Data Incubator for Europe, which aims to support the next generation of digital businesses and fast-track the development of new products and services. The corpus includes the following parts:\nTilde MODEL - EESC is a multilingual corpus compiled from document texts of European Economic and Social Committee document portal. Source: http://dm.eesc.europa.eu/\nTilde MODEL - RAPID multilingual parallel corpus is compiled from all press releases of Press Release Database of European Commission released between 1975 and end of 2016 as available from http://europa.eu/rapid/\nTilde MODEL - ECB multilingual parallel corpus is compiled from the multilingual pages of European Central Bank web site http://ebc.europa.eu/\nTilde MODEL - EMA is a corpus compiled from texts of European Medicines Agency document portal as available in http://www.ema.europa.eu/ at the end of 2016\nTilde MODEL - World Bank is a corpus compiled from texts of World Bank as available in http://www.worldbank.org/ in 2017\nTilde MODEL - AirBaltic.com Travel Destinations is a multilingual parallel corpus compiled from description texts of AirBaltic.com travel destinations as available in https://www.airbaltic.com/en/destinations/ in 2017\nTilde MODEL - LiveRiga.com is a multilingual parallel corpus compiled from Riga tourist attractions description texts of http://liveriga.com/ web site in 2017\nTilde MODEL - Lithuanian National Philharmonic Society is a parallel corpus compiled from texts of Lithuanian National Philharmonic Society web site http://www.filharmonija.lt/ in 2017\nTilde MODEL - mupa.hu is a parallel corpus from texts of Müpa Budapest - web site of Hungarian national culture house and concert venue https://www.mupa.hu/en/ compiled in spring of 2017\nTilde MODEL - fold.lv is a parallel corpus from texts of fold.lv portal http://www.fold.lv/en/ of the best of Latvian and foreign creative industries as compiled in spring of 2017\nTilde MODEL - czechtourism.com is a multilingual parallel corpus from texts of http://czechtourism.com/ portal compiled in spring of 2017\n30 languages, 274 bitexts\ntotal number of files: 125\ntotal number of tokens: 1.43G\ntotal number of sentence fragments: 62.44M","paperswithcode_id":"tilde-model-corpus","key":""},{"id":"times_of_india_news_headlines","tags":["annotations_creators:no-annotation","language_creators:expert-generated","languages:en","licenses:cc0-1.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:conditional-text-generation","task_categories:text-retrieval","task_ids:document-retrieval","task_ids:explanation-generation","task_ids:fact-checking-retrieval","task_ids:other-stuctured-to-text","task_ids:text-simplification"],"citation":"@data{DVN/DPQMQH_2020,\nauthor = {Kulkarni, Rohit},\npublisher = {Harvard Dataverse},\ntitle = {{Times of India News Headlines}},\nyear = {2020},\nversion = {V1},\ndoi = {10.7910/DVN/DPQMQH},\nurl = {https://doi.org/10.7910/DVN/DPQMQH}\n}","description":"This news dataset is a persistent historical archive of noteable events in the Indian subcontinent from start-2001 to mid-2020, recorded in realtime by the journalists of India. It contains approximately 3.3 million events published by Times of India. Times Group as a news agency, reaches out a very wide audience across Asia and drawfs every other agency in the quantity of english articles published per day. Due to the heavy daily volume over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues and talking points and how they have unfolded over time. It is possible to chop this dataset into a smaller piece for a more focused analysis, based on one or more facets.","key":""},{"id":"timit_asr","tags":["pretty_name:TIMIT","annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:other-LDC-User-Agreement-for-Non-Members","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:speech-processing","task_ids:automatic-speech-recognition"],"citation":"@inproceedings{\n  title={TIMIT Acoustic-Phonetic Continuous Speech Corpus},\n  author={Garofolo, John S., et al},\n  ldc_catalog_no={LDC93S1},\n  DOI={https://doi.org/10.35111/17gk-bn40},\n  journal={Linguistic Data Consortium, Philadelphia},\n  year={1983}\n}","description":"The TIMIT corpus of reading speech has been developed to provide speech data for acoustic-phonetic research studies\nand for the evaluation of automatic speech recognition systems.\n\nTIMIT contains high quality recordings of 630 individuals/speakers with 8 different American English dialects,\nwith each individual reading upto 10 phonetically rich sentences.\n\nMore info on TIMIT dataset can be understood from the \"README\" which can be found here:\nhttps://catalog.ldc.upenn.edu/docs/LDC93S1/readme.txt","paperswithcode_id":"timit","key":""},{"id":"tiny_shakespeare","tags":[],"citation":"@misc{\n  author={Karpathy, Andrej},\n  title={char-rnn},\n  year={2015},\n  howpublished={\\\\url{https://github.com/karpathy/char-rnn}}\n}","description":"40,000 lines of Shakespeare from a variety of Shakespeare's plays. Featured in Andrej Karpathy's blog post 'The Unreasonable Effectiveness of Recurrent Neural Networks': http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n\nTo use for e.g. character modelling:\n\n```\nd = datasets.load_dataset(name='tiny_shakespeare')['train']\nd = d.map(lambda x: datasets.Value('strings').unicode_split(x['text'], 'UTF-8'))\n# train split includes vocabulary for other splits\nvocabulary = sorted(set(next(iter(d)).numpy()))\nd = d.map(lambda x: {'cur_char': x[:-1], 'next_char': x[1:]})\nd = d.unbatch()\nseq_len = 100\nbatch_size = 2\nd = d.batch(seq_len)\nd = d.batch(batch_size)\n```","key":""},{"id":"tlc","tags":["annotations_creators:expert-generated","annotations_creators:no-annotation","language_creators:expert-generated","languages:th","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@misc{\n  author={Sawatphol, Jitkapat},\n  title={Thai Literature Corpora},\n  year={2019},\n  howpublished={\\\\url{https://attapol.github.io/tlc.html}}\n}","description":"Thai Literature Corpora (TLC): Corpora of machine-ingestible Thai classical literature texts.\n\nRelease: 6/25/19\n\nIt consists of two datasets:\n\n## TLC set\nIt is texts from [Vajirayana Digital Library](https://vajirayana.org/), stored by chapters and stanzas (non-tokenized).\n\ntlc v.2.0 (6/17/19 : a total of 34 documents, 292,270 lines, 31,790,734 characters)\ntlc v.1.0 (6/11/19 : a total of 25 documents, 113,981 lines, 28,775,761 characters)\n\n## TNHC set\nIt is texts from Thai National Historical Corpus, stored by lines (manually tokenized).\n\ntnhc v.1.0 (6/25/19 : a total of 47 documents, 756,478 lines, 13,361,142 characters)","key":""},{"id":"tmu_gfm_dataset","tags":["annotations_creators:crowdsourced","language_creators:machine-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-grammatical-error-correction"],"citation":"@inproceedings{yoshimura-etal-2020-reference,\n    title = \"{SOME}: Reference-less Sub-Metrics Optimized for Manual Evaluations of Grammatical Error Correction\",\n    author = \"Yoshimura, Ryoma  and\n      Kaneko, Masahiro  and\n      Kajiwara, Tomoyuki  and\n      Komachi, Mamoru\",\n    booktitle = \"Proceedings of the 28th International Conference on Computational Linguistics\",\n    month = dec,\n    year = \"2020\",\n    address = \"Barcelona, Spain (Online)\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.coling-main.573\",\n    pages = \"6516--6522\",\n    abstract = \"We propose a reference-less metric trained on manual evaluations of system outputs for grammatical error correction (GEC). Previous studies have shown that reference-less metrics are promising; however, existing metrics are not optimized for manual evaluations of the system outputs because no dataset of the system output exists with manual evaluation. This study manually evaluates outputs of GEC systems to optimize the metrics. Experimental results show that the proposed metric improves correlation with the manual evaluation in both system- and sentence-level meta-evaluation. Our dataset and metric will be made publicly available.\",\n}","description":"A dataset for GEC metrics with manual evaluations of grammaticality, fluency, and meaning preservation for system outputs. More detail about the creation of the dataset can be found in Yoshimura et al. (2020).","key":""},{"id":"totto","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:table-to-text"],"citation":"@inproceedings{parikh2020totto,\n  title={{ToTTo}: A Controlled Table-To-Text Generation Dataset},\n  author={Parikh, Ankur P and Wang, Xuezhi and Gehrmann, Sebastian and Faruqui, Manaal and Dhingra, Bhuwan and Yang, Diyi and Das, Dipanjan},\n  booktitle={Proceedings of EMNLP},\n  year={2020}\n }","description":"ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description.","paperswithcode_id":"totto","key":""},{"id":"trec","tags":["languages:en"],"citation":"@inproceedings{li-roth-2002-learning,\n    title = \"Learning Question Classifiers\",\n    author = \"Li, Xin  and\n      Roth, Dan\",\n    booktitle = \"{COLING} 2002: The 19th International Conference on Computational Linguistics\",\n    year = \"2002\",\n    url = \"https://www.aclweb.org/anthology/C02-1150\",\n}\n@inproceedings{hovy-etal-2001-toward,\n    title = \"Toward Semantics-Based Answer Pinpointing\",\n    author = \"Hovy, Eduard  and\n      Gerber, Laurie  and\n      Hermjakob, Ulf  and\n      Lin, Chin-Yew  and\n      Ravichandran, Deepak\",\n    booktitle = \"Proceedings of the First International Conference on Human Language Technology Research\",\n    year = \"2001\",\n    url = \"https://www.aclweb.org/anthology/H01-1069\",\n}","description":"The Text REtrieval Conference (TREC) Question Classification dataset contains 5500 labeled questions in training set and another 500 for test set. The dataset has 6 labels, 47 level-2 labels. Average length of each sentence is 10, vocabulary size of 8700.\n\nData are collected from four sources: 4,500 English questions published by USC (Hovy et al., 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as the test set.","paperswithcode_id":"trecqa","key":""},{"id":"trivia_qa","tags":["languages:en"],"citation":"@article{2017arXivtriviaqa,\n       author = {{Joshi}, Mandar and {Choi}, Eunsol and {Weld},\n                 Daniel and {Zettlemoyer}, Luke},\n        title = \"{triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}\",\n      journal = {arXiv e-prints},\n         year = 2017,\n          eid = {arXiv:1705.03551},\n        pages = {arXiv:1705.03551},\narchivePrefix = {arXiv},\n       eprint = {1705.03551},\n}","description":"TriviaqQA is a reading comprehension dataset containing over 650K\nquestion-answer-evidence triples. TriviaqQA includes 95K question-answer\npairs authored by trivia enthusiasts and independently gathered evidence\ndocuments, six per question on average, that provide high quality distant\nsupervision for answering the questions.","paperswithcode_id":"triviaqa","key":""},{"id":"tsac","tags":["annotations_creators:expert-generated","language_creators:found","languages:aeb","licenses:lgpl-3.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{medhaffar-etal-2017-sentiment,\n    title = \"Sentiment Analysis of {T}unisian Dialects: Linguistic Ressources and Experiments\",\n    author = \"Medhaffar, Salima  and\n      Bougares, Fethi  and\n      Est{`e}ve, Yannick  and\n      Hadrich-Belguith, Lamia\",\n    booktitle = \"Proceedings of the Third {A}rabic Natural Language Processing Workshop\",\n    month = apr,\n    year = \"2017\",\n    address = \"Valencia, Spain\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W17-1307\",\n    doi = \"10.18653/v1/W17-1307\",\n    pages = \"55--61\",\n    abstract = \"Dialectal Arabic (DA) is significantly different from the Arabic language taught in schools and used in written communication and formal speech (broadcast news, religion, politics, etc.). There are many existing researches in the field of Arabic language Sentiment Analysis (SA); however, they are generally restricted to Modern Standard Arabic (MSA) or some dialects of economic or political interest. In this paper we are interested in the SA of the Tunisian Dialect. We utilize Machine Learning techniques to determine the polarity of comments written in Tunisian Dialect. First, we evaluate the SA systems performances with models trained using freely available MSA and Multi-dialectal data sets. We then collect and annotate a Tunisian Dialect corpus of 17.000 comments from Facebook. This corpus allows us a significant accuracy improvement compared to the best model trained on other Arabic dialects or MSA data. We believe that this first freely available corpus will be valuable to researchers working in the field of Tunisian Sentiment Analysis and similar areas.\",\n}","description":"Tunisian Sentiment Analysis Corpus.\n\nAbout 17k user comments manually annotated to positive and negative polarities. This corpus is collected from Facebook users comments written on official pages of Tunisian radios and TV channels namely Mosaique FM, JawhraFM, Shemes FM, HiwarElttounsi TV and Nessma TV. The corpus is collected from a period spanning January 2015 until June 2016.","paperswithcode_id":"tsac","key":""},{"id":"ttc4900","tags":["annotations_creators:found","language_creators:found","languages:tr","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-news-category-classification"],"description":"The data set is taken from kemik group\nhttp://www.kemik.yildiz.edu.tr/\nThe data are pre-processed for the text categorization, collocations are found, character set is corrected, and so forth.\nWe named TTC4900 by mimicking the name convention of TTC 3600 dataset shared by the study http://journals.sagepub.com/doi/abs/10.1177/0165551515620551","key":""},{"id":"tunizi","tags":["annotations_creators:expert-generated","language_creators:found","languages:aeb","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{Chayma2020,\ntitle={TUNIZI: a Tunisian Arabizi sentiment analysis Dataset},\nauthor={Fourati, Chayma and Messaoudi, Abir and Haddad, Hatem},\nbooktitle={AfricaNLP Workshop, Putting Africa on the NLP Map. ICLR 2020, Virtual Event},\nvolume = {arXiv:3091079},\nyear = {2020},\nurl = {https://arxiv.org/submit/3091079},\n}","description":"On social media, Arabic speakers tend to express themselves in their own local dialect. To do so, Tunisians use \"Tunisian Arabizi\", which consists in supplementing numerals to the Latin script rather than the Arabic alphabet. TUNIZI is the first Tunisian Arabizi Dataset including 3K sentences, balanced, covering different topics, preprocessed and annotated as positive and negative.","paperswithcode_id":"tunizi","key":""},{"id":"tuple_ie","tags":["annotations_creators:found","language_creators:machine-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:other","task_ids:other-other-open-information-extraction"],"citation":"@article{Khot2017AnsweringCQ,\n  title={Answering Complex Questions Using Open Information Extraction},\n  author={Tushar Khot and A. Sabharwal and Peter Clark},\n  journal={ArXiv},\n  year={2017},\n  volume={abs/1704.05572}\n}","description":"The TupleInf Open IE dataset contains Open IE tuples extracted from 263K sentences that were used by the solver in “Answering Complex Questions Using Open Information Extraction” (referred as Tuple KB, T). These sentences were collected from a large Web corpus using training questions from 4th and 8th grade as queries. This dataset contains 156K sentences collected for 4th grade questions and 107K sentences for 8th grade questions. Each sentence is followed by the Open IE v4 tuples using their simple format.","paperswithcode_id":"tupleinf-open-ie-dataset","key":""},{"id":"turk","tags":["annotations_creators:machine-generated","language_creators:found","languages:en","licenses:gnu-gpl-v3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:text-simplification"],"citation":" @article{Xu-EtAl:2016:TACL,\n author = {Wei Xu and Courtney Napoles and Ellie Pavlick and Quanze Chen and Chris Callison-Burch},\n title = {Optimizing Statistical Machine Translation for Text Simplification},\n journal = {Transactions of the Association for Computational Linguistics},\n volume = {4},\n year = {2016},\n url = {https://cocoxu.github.io/publications/tacl2016-smt-simplification.pdf},\n pages = {401--415}\n }\n}","description":"TURKCorpus is a dataset for evaluating sentence simplification systems that focus on lexical paraphrasing,\nas described in \"Optimizing Statistical Machine Translation for Text Simplification\". The corpus is composed of 2000 validation and 359 test original sentences that were each simplified 8 times by different annotators.","key":""},{"id":"turkish_movie_sentiment","tags":["annotations_creators:found","language_creators:found","languages:tr","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification","task_ids:sentiment-scoring"],"description":"This data set is a dataset from kaggle consisting of Turkish movie reviews and scored between 0-5.","key":""},{"id":"turkish_ner","tags":["annotations_creators:machine-generated","language_creators:expert-generated","languages:tr","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@InProceedings@article{DBLP:journals/corr/SahinTYES17,\n  author    = {H. Bahadir Sahin and\n               Caglar Tirkaz and\n               Eray Yildiz and\n               Mustafa Tolga Eren and\n               Omer Ozan Sonmez},\n  title     = {Automatically Annotated Turkish Corpus for Named Entity Recognition\n               and Text Categorization using Large-Scale Gazetteers},\n  journal   = {CoRR},\n  volume    = {abs/1702.02363},\n  year      = {2017},\n  url       = {http://arxiv.org/abs/1702.02363},\n  archivePrefix = {arXiv},\n  eprint    = {1702.02363},\n  timestamp = {Mon, 13 Aug 2018 16:46:36 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/SahinTYES17.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"Turkish Wikipedia Named-Entity Recognition and Text Categorization\n(TWNERTC) dataset is a collection of automatically categorized and annotated\nsentences obtained from Wikipedia. The authors constructed large-scale\ngazetteers by using a graph crawler algorithm to extract\nrelevant entity and domain information\nfrom a semantic knowledge base, Freebase.\nThe constructed gazetteers contains approximately\n300K entities with thousands of fine-grained entity types\nunder 77 different domains.","key":""},{"id":"turkish_product_reviews","tags":["annotations_creators:found","language_creators:found","languages:tr","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification","pretty_name:Turkish Product Reviews"],"description":"Turkish Product Reviews.\nThis repository contains 235.165 product reviews collected online. There are 220.284 positive, 14881 negative reviews.","key":""},{"id":"turkish_shrinked_ner","tags":["annotations_creators:machine-generated","language_creators:expert-generated","languages:tr","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:extended|other-turkish_ner","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"\\","description":"Shrinked version (48 entity type) of the turkish_ner.\n\nOriginal turkish_ner dataset: Automatically annotated Turkish corpus for named entity recognition and text categorization using large-scale gazetteers. The constructed gazetteers contains approximately 300K entities with thousands of fine-grained entity types under 25 different domains.\n\nShrinked entity types are: academic, academic_person, aircraft, album_person, anatomy, animal, architect_person, capital, chemical, clothes, country, culture, currency, date, food, genre, government, government_person, language, location, material, measure, medical, military, military_person, nation, newspaper, organization, organization_person, person, production_art_music, production_art_music_person, quantity, religion, science, shape, ship, software, space, space_person, sport, sport_name, sport_person, structure, subject, tech, train, vehicle","key":""},{"id":"turku_ner_corpus","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:fi","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{luoma-etal-2020-broad,\ntitle = \"A Broad-coverage Corpus for {F}innish Named Entity Recognition\",\nauthor = {Luoma, Jouni and Oinonen, Miika and Pyyk{\\\"o}nen, Maria and Laippala, Veronika and Pyysalo, Sampo},\nbooktitle = \"Proceedings of The 12th Language Resources and Evaluation Conference\",\nyear = \"2020\",\nurl = \"https://www.aclweb.org/anthology/2020.lrec-1.567\",\npages = \"4615--4624\",\n}","description":"An open, broad-coverage corpus for Finnish named entity recognition presented in Luoma et al. (2020) A Broad-coverage Corpus for Finnish Named Entity Recognition.","key":""},{"id":"tweet_eval","tags":["language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:1K<n<10K","size_categories:10K<n<100K","size_categories:n<1K","source_datasets:extended|other-tweet-datasets","task_categories:text-classification","task_ids:multi-class-classification","task_ids:sentiment-classification","task_ids:intent-classification"],"citation":"@inproceedings{barbieri2020tweeteval,\n  title={{TweetEval:Unified Benchmark and Comparative Evaluation for Tweet Classification}},\n  author={Barbieri, Francesco and Camacho-Collados, Jose and Espinosa-Anke, Luis and Neves, Leonardo},\n  booktitle={Proceedings of Findings of EMNLP},\n  year={2020}\n}","description":"TweetEval consists of seven heterogenous tasks in Twitter, all framed as multi-class tweet classification. All tasks have been unified into the same benchmark, with each dataset presented in the same format and with fixed training, validation and test splits.","paperswithcode_id":"tweeteval","key":""},{"id":"tweet_qa","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@misc{xiong2019tweetqa,\n      title={TWEETQA: A Social Media Focused Question Answering Dataset},\n      author={Wenhan Xiong and Jiawei Wu and Hong Wang and Vivek Kulkarni and Mo Yu and Shiyu Chang and Xiaoxiao Guo and William Yang Wang},\n      year={2019},\n      eprint={1907.06292},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":" TweetQA is the first dataset for QA on social media data by leveraging news media and crowdsourcing.","paperswithcode_id":"tweetqa","key":""},{"id":"tweets_ar_en_parallel","tags":["annotations_creators:expert-generated","annotations_creators:no-annotation","language_creators:found","languages:ar","languages:en","licenses:apache-2.0","multilinguality:translation","size_categories:100K<n<1M","source_datasets:original","task_categories:other","task_ids:other-other-machine-translation"],"citation":"@inproceedings{Mubarak2020bilingualtweets,\ntitle={Constructing a Bilingual Corpus of Parallel Tweets},\nauthor={Mubarak, Hamdy and Hassan, Sabit and Abdelali, Ahmed},\nbooktitle={Proceedings of 13th Workshop on Building and Using Comparable Corpora (BUCC)},\naddress={Marseille, France},\nyear={2020}\n}","description":"    Twitter users often post parallel tweets—tweets that contain the same content but are\n    written in different languages. Parallel tweets can be an important resource for developing\n    machine translation (MT) systems among other natural language processing (NLP) tasks. This\n    resource is a result of a generic method for collecting parallel tweets. Using the method,\n    we compiled a bilingual corpus of English-Arabic parallel tweets and a list of Twitter accounts\n    who post English-Arabic tweets regularly. Additionally, we annotate a subset of Twitter accounts\n    with their countries of origin and topic of interest, which provides insights about the population\n    who post parallel tweets.","key":""},{"id":"tweets_hate_speech_detection","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:gpl-3.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@InProceedings{Z\nRoshan Sharma:dataset,\ntitle = {Sentimental Analysis of Tweets for Detecting Hate/Racist Speeches},\nauthors={Roshan Sharma},\nyear={2018}\n}","description":"The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\nFormally, given a training sample of tweets and labels, where label ‘1’ denotes the tweet is racist/sexist and label ‘0’ denotes the tweet is not racist/sexist, your objective is to predict the labels on the given test dataset.","paperswithcode_id":"bilingual-corpus-of-arabic-english-parallel","key":""},{"id":"twi_text_c3","tags":["annotations_creators:expert-generated","language_creators:found","languages:tw","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@inproceedings{alabi-etal-2020-massive,\n    title = \"Massive vs. Curated Embeddings for Low-Resourced Languages: the Case of Yoruba and {T}wi\",\n    author = \"Alabi, Jesujoba  and\n      Amponsah-Kaakyire, Kwabena  and\n      Adelani, David  and\n      Espa{\\\\~n}a-Bonet, Cristina\",\n    booktitle = \"Proceedings of the 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.335\",\n    pages = \"2754--2762\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}","description":"Twi Text C3 is the largest Twi texts collected and used to train FastText embeddings in the\nYorubaTwi Embedding paper: https://www.aclweb.org/anthology/2020.lrec-1.335/","key":""},{"id":"twi_wordsim353","tags":["annotations_creators:crowdsourced","language_creators:expert-generated","languages:en","languages:tw","licenses:unknown","multilinguality:multilingual","size_categories:n<1K","task_categories:text-scoring","task_ids:semantic-similarity-scoring"],"citation":"@inproceedings{alabi-etal-2020-massive,\n    title = \"Massive vs. Curated Embeddings for Low-Resourced Languages: the Case of {Y}or{\\\\`u}b{\\\\'a} and {T}wi\",\n    author = \"Alabi, Jesujoba  and\n      Amponsah-Kaakyire, Kwabena  and\n      Adelani, David  and\n      Espa{\\\\~n}a-Bonet, Cristina\",\n    booktitle = \"Proceedings of the 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.335\",\n    pages = \"2754--2762\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}","description":"A translation of the word pair similarity dataset wordsim-353 to Twi.\n\nThe dataset was presented in the paper\nAlabi et al.: Massive vs. Curated Embeddings for Low-Resourced\nLanguages: the Case of Yorùbá and Twi (LREC 2020).","key":""},{"id":"tydiqa","tags":["pretty_name:TyDi QA","annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","languages:ar","languages:bn","languages:fi","languages:id","languages:ja","languages:sw","languages:ko","languages:ru","languages:te","languages:th","licenses:apache-2.0","multilinguality:multilingual","size_categories:unknown","source_datasets:extended|wikipedia","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@article{tydiqa,\ntitle   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},\nauthor  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}\nyear    = {2020},\njournal = {Transactions of the Association for Computational Linguistics}\n}","description":"TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\ndon’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\nthe use of translation (unlike MLQA and XQuAD).","paperswithcode_id":"tydi-qa","key":""},{"id":"ubuntu_dialogs_corpus","tags":["languages:en"],"citation":"@article{DBLP:journals/corr/LowePSP15,\n  author    = {Ryan Lowe and\n               Nissan Pow and\n               Iulian Serban and\n               Joelle Pineau},\n  title     = {The Ubuntu Dialogue Corpus: {A} Large Dataset for Research in Unstructured\n               Multi-Turn Dialogue Systems},\n  journal   = {CoRR},\n  volume    = {abs/1506.08909},\n  year      = {2015},\n  url       = {http://arxiv.org/abs/1506.08909},\n  archivePrefix = {arXiv},\n  eprint    = {1506.08909},\n  timestamp = {Mon, 13 Aug 2018 16:48:23 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/LowePSP15.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter.","paperswithcode_id":"ubuntu-dialogue-corpus","key":""},{"id":"udhr","tags":["annotations_creators:no-annotation","language_creators:found","languages:aa","languages:ab","languages:ace","languages:acu","languages:ada","languages:ady","languages:af","languages:agr","languages:aii","languages:ajg","languages:als","languages:alt","languages:am","languages:amc","languages:ame","languages:ami","languages:amr","languages:arb","languages:arl","languages:arn","languages:ast","languages:auc","languages:ayr","languages:azj","languages:ban","languages:bax","languages:bba","languages:bci","languages:bcl","languages:be","languages:bem","languages:bfa","languages:bg","languages:bho","languages:bi","languages:bin","languages:blt","languages:bm","languages:bn","languages:bo","languages:boa","languages:br","languages:bs","languages:buc","languages:bug","languages:bum","languages:ca","languages:cab","languages:cak","languages:cbi","languages:cbr","languages:cbs","languages:cbt","languages:cbu","languages:ccp","languages:ceb","languages:cfm","languages:ch","languages:chj","languages:chk","languages:chr","languages:cic","languages:cjk","languages:cjs","languages:cjy","languages:ckb","languages:cmn","languages:cnh","languages:cni","languages:co","languages:cof","languages:cot","languages:cpu","languages:crh","languages:cri","languages:crs","languages:cs","languages:csa","languages:csw","languages:ctd","languages:cy","languages:da","languages:dag","languages:ddn","languages:de","languages:dga","languages:dip","languages:duu","languages:dv","languages:dyo","languages:dyu","languages:dz","languages:ee","languages:ekk","languages:el","languages:emk","languages:en","languages:eo","languages:es","languages:ese","languages:eu","languages:eve","languages:evn","languages:fat","languages:fi","languages:fj","languages:fkv","languages:fo","languages:fon","languages:fr","languages:fuf","languages:fur","languages:fuv","languages:fy","languages:ga","languages:gaa","languages:gag","languages:gan","languages:gaz","languages:gd","languages:gjn","languages:gkp","languages:gl","languages:gld","languages:gu","languages:guc","languages:gug","languages:guu","languages:gv","languages:gyr","languages:ha","languages:hak","languages:haw","languages:he","languages:hea","languages:hi","languages:hil","languages:hlt","languages:hms","languages:hna","languages:hni","languages:hnj","languages:hns","languages:hr","languages:hsb","languages:ht","languages:hu","languages:hus","languages:huu","languages:hy","languages:ia","languages:ibb","languages:id","languages:ig","languages:ii","languages:ike","languages:ilo","languages:io","languages:is","languages:it","languages:ja","languages:jiv","languages:jv","languages:ka","languages:kbd","languages:kbp","languages:kde","languages:kdh","languages:kea","languages:kek","languages:kha","languages:khk","languages:kjh","languages:kk","languages:kkh","languages:kl","languages:km","languages:kmb","languages:kmr","languages:kn","languages:knc","languages:kng","languages:ko","languages:koi","languages:koo","languages:kqn","languages:kqs","languages:kri","languages:krl","languages:ktu","languages:kwi","languages:ky","languages:la","languages:lad","languages:lb","languages:lg","languages:lia","languages:lij","languages:lld","languages:ln","languages:lns","languages:lo","languages:lob","languages:lot","languages:loz","languages:lt","languages:lua","languages:lue","languages:lun","languages:lus","languages:lvs","languages:mad","languages:mag","languages:mai","languages:mam","languages:maz","languages:mcd","languages:mcf","languages:men","languages:mfq","languages:mh","languages:mi","languages:mic","languages:min","languages:miq","languages:mk","languages:ml","languages:mnw","languages:mos","languages:mr","languages:mt","languages:mto","languages:mxi","languages:mxv","languages:my","languages:mzi","languages:nan","languages:nb","languages:nba","languages:nds","languages:ng","languages:nhn","languages:nio","languages:niu","languages:njo","languages:nku","languages:nl","languages:nn","languages:not","languages:npi","languages:nr","languages:nso","languages:nv","languages:ny","languages:nym","languages:nyn","languages:nzi","languages:oaa","languages:oc","languages:ojb","languages:oki","languages:orh","languages:os","languages:ote","languages:pa","languages:pam","languages:pap","languages:pau","languages:pbb","languages:pbu","languages:pcd","languages:pcm","languages:pes","languages:pis","languages:piu","languages:pl","languages:plt","languages:pnb","languages:pon","languages:pov","languages:ppl","languages:prq","languages:prs","languages:pt","languages:qu","languages:quc","languages:qug","languages:quh","languages:quy","languages:quz","languages:qva","languages:qvc","languages:qvh","languages:qvm","languages:qvn","languages:qwh","languages:qxn","languages:qxu","languages:rar","languages:rgn","languages:rm","languages:rmn","languages:rn","languages:ro","languages:ru","languages:rup","languages:rw","languages:sa","languages:sah","languages:sco","languages:se","languages:sey","languages:sg","languages:shk","languages:shn","languages:shp","languages:si","languages:sk","languages:skr","languages:sl","languages:sm","languages:sn","languages:snk","languages:snn","languages:so","languages:sr","languages:src","languages:srr","languages:ss","languages:st","languages:su","languages:suk","languages:sus","languages:sv","languages:swb","languages:swh","languages:ta","languages:taj","languages:tbz","languages:tca","languages:tdt","languages:te","languages:tem","languages:tet","languages:tg","languages:th","languages:ti","languages:tiv","languages:tk","languages:tl","languages:tly","languages:tn","languages:to","languages:tob","languages:toi","languages:toj","languages:top","languages:tpi","languages:tr","languages:ts","languages:tsz","languages:tt","languages:tw","languages:ty","languages:tyv","languages:tzh","languages:tzm","languages:tzo","languages:ug","languages:uk","languages:umb","languages:und","languages:ur","languages:ura","languages:uzn","languages:vai","languages:ve","languages:vec","languages:vep","languages:vi","languages:vmw","languages:wa","languages:war","languages:wo","languages:wuu","languages:wwa","languages:xh","languages:xsm","languages:yad","languages:yao","languages:yap","languages:ydd","languages:ykg","languages:yo","languages:yua","languages:yue","languages:zam","languages:zdj","languages:zgh","languages:zlm","languages:zro","languages:ztu","languages:zu","languages:zyb","licenses:unknown","multilinguality:multilingual","size_categories:n<1K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"description":"The Universal Declaration of Human Rights (UDHR) is a milestone document in the history of human rights. Drafted by\nrepresentatives with different legal and cultural backgrounds from all regions of the world, it set out, for the\nfirst time, fundamental human rights to be universally protected. The Declaration was adopted by the UN General\nAssembly in Paris on 10 December 1948 during its 183rd plenary meeting. The dataset includes translations of the\ndocument in 464 languages and dialects.\n\n© 1996 – 2009 The Office of the High Commissioner for Human Rights\n\nThis plain text version prepared by the “UDHR in Unicode” project, https://www.unicode.org/udhr.","key":""},{"id":"um005","tags":["annotations_creators:no-annotation","language_creators:other","languages:en","languages:ur","licenses:unknown","multilinguality:multilingual","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@unpublished{JaZeWordOrderIssues2011,\nauthor      = {Bushra Jawaid and Daniel Zeman},\ntitle       = {Word-Order Issues in {English}-to-{Urdu} Statistical Machine Translation},\nyear        = {2011},\njournal     = {The Prague Bulletin of Mathematical Linguistics},\nnumber      = {95},\ninstitution = {Univerzita Karlova},\naddress     = {Praha, Czechia},\nissn        = {0032-6585},\n}","description":"UMC005 English-Urdu is a parallel corpus of texts in English and Urdu language with sentence alignments. The corpus can be used for experiments with statistical machine translation.\n\nThe texts come from four different sources:\n- Quran\n- Bible\n- Penn Treebank (Wall Street Journal)\n- Emille corpus\n\nThe authors provide the religious texts of Quran and Bible for direct download. Because of licensing reasons, Penn and Emille texts cannot be redistributed freely. However, if you already hold a license for the original corpora, we are able to provide scripts that will recreate our data on your disk. Our modifications include but are not limited to the following:\n\n- Correction of Urdu translations and manual sentence alignment of the Emille texts.\n- Manually corrected sentence alignment of the other corpora.\n- Our data split (training-development-test) so that our published experiments can be reproduced.\n- Tokenization (optional, but needed to reproduce our experiments).\n- Normalization (optional) of e.g. European vs. Urdu numerals, European vs. Urdu punctuation, removal of Urdu diacritics.","paperswithcode_id":"umc005-english-urdu","key":""},{"id":"un_ga","tags":["annotations_creators:found","language_creators:found","languages:ar","languages:en","languages:es","languages:fr","languages:ru","languages:zh","licenses:unknown","multilinguality:translation","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{title = \"United Nations General Assembly Resolutions: a six-language parallel corpus\",\nabstract = \"In this paper we describe a six-ways parallel public-domain corpus consisting of 2100 United Nations General Assembly Resolutions with translations in the six official languages of the United Nations, with an average of around 3 million tokens per language. The corpus is available in a preprocessed, formatting-normalized TMX format with paragraphs aligned across multiple languages. We describe the background to the corpus and its content, the process of its construction, and some of its interesting properties.\",\nauthor = \"Alexandre Rafalovitch and Robert Dale\",\nyear = \"2009\",\nlanguage = \"English\",\nbooktitle = \"MT Summit XII proceedings\",\npublisher = \"International Association of Machine Translation\",\n}","description":"United nations general assembly resolutions: A six-language parallel corpus.\nThis is a collection of translated documents from the United Nations originally compiled into a translation memory by Alexandre Rafalovitch, Robert Dale (see http://uncorpora.org).\n6 languages, 15 bitexts\ntotal number of files: 6\ntotal number of tokens: 18.87M\ntotal number of sentence fragments: 0.44M","key":""},{"id":"un_multi","tags":["annotations_creators:found","language_creators:found","languages:ar","languages:de","languages:en","languages:es","languages:fr","languages:ru","languages:zh","licenses:unknown","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{eisele-chen-2010-multiun,\n    title = \"{M}ulti{UN}: A Multilingual Corpus from United Nation Documents\",\n    author = \"Eisele, Andreas  and\n      Chen, Yu\",\n    booktitle = \"Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)\",\n    month = may,\n    year = \"2010\",\n    address = \"Valletta, Malta\",\n    publisher = \"European Language Resources Association (ELRA)\",\n    url = \"http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf\",\n    abstract = \"This paper describes the acquisition, preparation and properties of a corpus extracted from the official documents of the United Nations (UN). This corpus is available in all 6 official languages of the UN, consisting of around 300 million words per language. We describe the methods we used for crawling, document formatting, and sentence alignment. This corpus also includes a common test set for machine translation. We present the results of a French-Chinese machine translation experiment performed on this corpus.\",\n}\n\n@InProceedings{TIEDEMANN12.463,\n  author = {J�rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n}","description":"This is a collection of translated documents from the United Nations. This corpus is available in all 6 official languages of the UN, consisting of around 300 million words per language","paperswithcode_id":"multiun","key":""},{"id":"un_pc","tags":["annotations_creators:found","language_creators:found","languages:ar","languages:en","languages:es","languages:fr","languages:ru","languages:zh","licenses:unknown","multilinguality:multilingual","size_categories:10M<n<100M","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@inproceedings{ziemski-etal-2016-united,\n    title = \"The {U}nited {N}ations Parallel Corpus v1.0\",\n    author = \"Ziemski, Micha{\\\\l}  and\n      Junczys-Dowmunt, Marcin  and\n      Pouliquen, Bruno\",\n    booktitle = \"Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)\",\n    month = may,\n    year = \"2016\",\n    address = \"Portoro{\\v{z}}, Slovenia\",\n    publisher = \"European Language Resources Association (ELRA)\",\n    url = \"https://www.aclweb.org/anthology/L16-1561\",\n    pages = \"3530--3534\",\n    abstract = \"This paper describes the creation process and statistics of the official United Nations Parallel Corpus, the first parallel corpus composed from United Nations documents published by the original data creator. The parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish. The corpus is freely available for download under a liberal license. Apart from the pairwise aligned documents, a fully aligned subcorpus for the six official UN languages is distributed. We provide baseline BLEU scores of our Moses-based SMT systems trained with the full data of language pairs involving English and for all possible translation directions of the six-way subcorpus.\",\n}","description":"This parallel corpus consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish.","paperswithcode_id":"united-nations-parallel-corpus","key":""},{"id":"universal_dependencies","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:af","languages:akk","languages:aqz","languages:sq","languages:am","languages:grc","languages:apu","languages:ar","languages:hy","languages:aii","languages:bm","languages:eu","languages:be","languages:bho","languages:br","languages:bg","languages:bxr","languages:yue","languages:ca","languages:zh","languages:ckt","languages:lzh","languages:cop","languages:hr","languages:cs","languages:da","languages:nl","languages:en","languages:myv","languages:et","languages:fo","languages:fi","languages:fr","languages:gl","languages:de","languages:got","languages:el","languages:he","languages:qhe","languages:hi","languages:hu","languages:is","languages:id","languages:ga","languages:it","languages:ja","languages:krl","languages:kk","languages:kfm","languages:koi","languages:kpv","languages:ko","languages:kmr","languages:la","languages:lv","languages:lt","languages:olo","languages:mt","languages:gv","languages:mr","languages:gun","languages:mdf","languages:myu","languages:pcm","languages:nyq","languages:sme","languages:no","languages:cu","languages:fro","languages:orv","languages:otk","languages:fa","languages:pl","languages:pt","languages:ro","languages:ru","languages:sa","languages:gd","languages:sr","languages:sms","languages:sk","languages:sl","languages:soj","languages:ajp","languages:es","languages:swl","languages:sv","languages:gsw","languages:tl","languages:ta","languages:te","languages:th","languages:tpn","languages:qtd","languages:tr","languages:uk","languages:hsb","languages:ur","languages:ug","languages:vi","languages:wbp","languages:cy","languages:wo","languages:yo","licenses:unknown","multilinguality:multilingual","size_categories:1K<n<10K","source_datasets:original","task_categories:other","task_ids:constituency-parsing","task_ids:dependency-parsing"],"citation":"@misc{11234/1-3424,\ntitle = {Universal Dependencies 2.7},\nauthor = {Zeman, Daniel and Nivre, Joakim and Abrams, Mitchell and Ackermann, Elia and Aepli, No{\\\"e}mi and Aghaei, Hamid and Agi{\\'c}, {\\v Z}eljko and Ahmadi, Amir and Ahrenberg, Lars and Ajede, Chika Kennedy and Aleksandravi{\\v c}i{\\=u}t{\\.e}, Gabriel{\\.e} and Alfina, Ika and Antonsen, Lene and Aplonova, Katya and Aquino, Angelina and Aragon, Carolina and Aranzabe, Maria Jesus and Arnard{\\'o}ttir, {\\t H}{\\'o}runn and Arutie, Gashaw and Arwidarasti, Jessica Naraiswari and Asahara, Masayuki and Ateyah, Luma and Atmaca, Furkan and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Balasubramani, Keerthana and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Basmov, Victoria and Batchelor, Colin and Bauer, John and Bedir, Seyyit Talha and Bengoetxea, Kepa and Berk, G{\\\"o}zde and Berzak, Yevgeni and Bhat, Irshad Ahmad and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Bielinskien{\\.e}, Agn{\\.e} and Bjarnad{\\'o}ttir, Krist{\\'{\\i}}n and Blokland, Rogier and Bobicev, Victoria and Boizou, Lo{\\\"{\\i}}c and Borges V{\\\"o}lker, Emanuel and B{\\\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Brokait{\\.e}, Kristina and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cavalcanti, Tatiana and Cebiroglu Eryigit, Gulsen and Cecchini, Flavio Massimiliano and Celano, Giuseppe G. A. and Ceplo, Slavomir and Cetin, Savas and Cetinoglu, Ozlem and Chalub, Fabricio and Chi, Ethan and Cho, Yongseok and Choi, Jinho and Chun, Jayeol and Cignarella, Alessandra T. and Cinkova, Silvie and Collomb, Aurelie and Coltekin, Cagr{\\i} and Connor, Miriam and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and Derin, Mehmet Oguz and de Souza, Elvis and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dinakaramani, Arawinda and Dione, Bamba and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eckhoff, Hanne and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erina, Olga and Erjavec, Tomaz and Etienne, Aline and Evelyn, Wograine and Facundes, Sidney and Farkas, Rich{\\'a}rd and Fernanda, Mar{\\'{\\i}}lia and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\\'a}udia and Fujita, Kazunori and Gajdosov{\\'a}, Katar{\\'{\\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\\\"a}rdenfors, Moa and Garza, Sebastian and Gerardi, Fabr{\\'{\\i}}cio Ferraz and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\\\"o}k{\\i}rmak, Memduh and Goldberg, Yoav and G{\\'o}mez Guinovart, Xavier and Gonz{\\'a}lez Saavedra,\nBerta and Grici{\\=u}t{\\.e}, Bernadeta and Grioni, Matias and Grobol, Lo{\\\"{\\i}}c and Gr{\\=u}z{\\={\\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\\'e}line and G{\\\"u}ng{\\\"o}r, Tunga and Habash, Nizar and Hafsteinsson, Hinrik and Haji{\\v c}, Jan and Haji{\\v c} jr., Jan and H{\\\"a}m{\\\"a}l{\\\"a}inen, Mika and H{\\`a} M{\\~y}, Linh and Han, Na-Rae and Hanifmuti, Muhammad Yudistira and Hardwick, Sam and Harris, Kim and Haug, Dag and Heinecke, Johannes and Hellwig, Oliver and Hennig, Felix and Hladk{\\'a}, Barbora and Hlav{\\'a}{\\v c}ov{\\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Huber, Eva and Hwang, Jena and Ikeda, Takumi and Ingason, Anton Karl and Ion, Radu and Irimia, Elena and Ishola, {\\d O}l{\\'a}j{\\'{\\i}}d{\\'e} and Jel{\\'{\\i}}nek, Tom{\\'a}{\\v s} and Johannsen, Anders and J{\\'o}nsd{\\'o}ttir, Hildur and J{\\o}rgensen, Fredrik and Juutinen, Markus and K, Sarveswaran and Ka{\\c s}{\\i}kara, H{\\\"u}ner and Kaasen, Andre and Kabaeva, Nadezhda and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Katz, Boris and Kayadelen, Tolga and Kenney, Jessica and Kettnerov{\\'a}, V{\\'a}clava and Kirchner, Jesse and Klementieva, Elena and K{\\\"o}hn, Arne and K{\\\"o}ksal, Abdullatif and Kopacewicz, Kamil and Korkiakangas, Timo and Kotsyba, Natalia and Kovalevskait{\\.e}, Jolanta and Krek, Simon and Krishnamurthy, Parameswari and Kwak, Sookyoung and Laippala, Veronika and Lam, Lucia and Lambertino, Lorenzo and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\\^e} H{\\`{\\^o}}ng, Phương and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Levina, Maria and Li, Cheuk Ying and Li, Josie and Li, Keying and Li, Yuan and Lim, {KyungTae} and Linden, Krister and Ljubesic, Nikola and Loginova, Olga and Luthfi, Andry and Luukko, Mikko and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and Maranduc, Catalina and Marcek, David and Marheinecke, Katrin and Mart{\\'{\\i}}nez Alonso, H{\\'e}ctor and Martins, Andr{\\'e} and Masek, Jan and Matsuda, Hiroshi and Matsumoto, Yuji and {McDonald}, Ryan and {McGuinness}, Sarah and Mendonca, Gustavo and Miekka, Niko and Mischenkova, Karina and Misirpashayeva, Margarita and Missil{\\\"a}, Anna and Mititelu, Catalin and Mitrofan, Maria and Miyao, Yusuke and Mojiri Foroushani, {AmirHossein} and Moloodi, Amirsaeid and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Keiko Sophie and Mori, Shinsuke and Morioka, Tomohiko and Moro, Shigeki and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Munro, Robert and Murawaki, Yugo and M{\\\"u}{\\\"u}risep, Kaili and Nainwani, Pinkey and Nakhl{\\'e}, Mariam and Navarro Hor{\\~n}iacek, Juan Ignacio and Nedoluzhko,\nAnna and Ne{\\v s}pore-B{\\=e}rzkalne, Gunta and Nguy{\\~{\\^e}}n Th{\\d i}, Lương and Nguy{\\~{\\^e}}n Th{\\d i} Minh, Huy{\\`{\\^e}}n and Nikaido, Yoshihiro and Nikolaev, Vitaly and Nitisaroj, Rattima and Nourian, Alireza and Nurmi, Hanna and Ojala, Stina and Ojha, Atul Kr. and Ol{\\'u}{\\`o}kun, Ad{\\'e}day{\\d o}̀ and Omura, Mai and Onwuegbuzia, Emeka and Osenova, Petya and {\\\"O}stling, Robert and {\\O}vrelid, Lilja and {\\\"O}zate{\\c s}, {\\c S}aziye Bet{\\\"u}l and {\\\"O}zg{\\\"u}r, Arzucan and {\\\"O}zt{\\\"u}rk Ba{\\c s}aran, Balk{\\i}z and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Paulino-Passos, Guilherme and Peljak-{\\L}api{\\'n}ska, Angelika and Peng, Siyao and Perez, Cenel-Augusto and Perkova, Natalia and Perrier, Guy and Petrov, Slav and Petrova, Daria and Phelan, Jason and Piitulainen, Jussi and Pirinen, Tommi A and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Ponomareva, Larisa and Popel, Martin and Pretkalnina, Lauma and Pr{\\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and Qi, Peng and R{\\\"a}{\\\"a}bis, Andriela and Rademaker, Alexandre and Rama, Taraka and Ramasamy, Loganathan and Ramisch, Carlos and Rashel, Fam and Rasooli, Mohammad Sadegh and Ravishankar, Vinit and Real, Livy and Rebeja, Petru and Reddy, Siva and Rehm, Georg and Riabov, Ivan and Rie{\\ss}ler, Michael and Rimkut{\\.e}, Erika and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and R{\\\"o}gnvaldsson, Eir{\\'{\\i}}kur and Romanenko, Mykhailo and Rosa, Rudolf and Roșca, Valentin and Rovati, Davide and Rudina, Olga and Rueter, Jack and R{\\'u}narsson, Kristjan and Sadde, Shoval and Safari, Pegah and Sagot, Benoit and Sahala, Aleksi and Saleh, Shadi and Salomoni, Alessio and Samardzi{\\'c}, Tanja and Samson, Stephanie and Sanguinetti, Manuela and S{\\\"a}rg,\nDage and Saul{\\={\\i}}te, Baiba and Sawanakunanon, Yanin and Scannell, Kevin and Scarlata, Salvatore and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shirasu, Hiroyuki and Shohibussirri, Muh and Sichinava, Dmitry and Sigurðsson, Einar Freyr and Silveira, Aline and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\\'o}, Katalin and {\\v S}imkov{\\'a}, M{\\'a}ria and Simov, Kiril and Skachedubova, Maria and Smith, Aaron and Soares-Bastos, Isabela and Spadine, Carolyn and Steingr{\\'{\\i}}msson, Stein{\\t h}{\\'o}r and Stella, Antonio and Straka, Milan and Strickland, Emmett and Strnadov{\\'a}, Jana and Suhr, Alane and Sulestio, Yogi Lesmana and Sulubacak, Umut and Suzuki, Shingo and Sz{\\'a}nt{\\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tamburini, Fabio and Tan, Mary Ann C. and Tanaka, Takaaki and Tella, Samson and Tellier, Isabelle and Thomas, Guillaume and Torga, Liisi and Toska, Marsida and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and T{\\\"u}rk, Utku and Tyers, Francis and Uematsu, Sumire and Untilov, Roman and Uresov{\\'a}, Zdenka and Uria, Larraitz and Uszkoreit, Hans and Utka, Andrius and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Villemonte de la Clergerie, Eric and Vincze, Veronika and Wakasa, Aya and Wallenberg, Joel C. and Wallin, Lars and Walsh, Abigail and Wang, Jing Xian and Washington, Jonathan North and Wendt, Maximilan and Widmer, Paul and Williams, Seyi and Wir{\\'e}n, Mats and Wittern, Christian and Woldemariam, Tsegay and Wong, Tak-sum and Wr{\\'o}blewska, Alina and Yako, Mary and Yamashita, Kayo and Yamazaki, Naoki and Yan, Chunxiao and Yasuoka, Koichi and Yavrumyan, Marat M. and Yu, Zhuoran and Zabokrtsk{\\'y}, Zdenek and Zahra, Shorouq and Zeldes, Amir and Zhu, Hanzhi and Zhuravleva, Anna},\nurl = {http://hdl.handle.net/11234/1-3424},\nnote = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\\'U}FAL}), Faculty of Mathematics and Physics, Charles University},\ncopyright = {Licence Universal Dependencies v2.7},\nyear = {2020} }","description":"Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008).","paperswithcode_id":"universal-dependencies","key":""},{"id":"universal_morphologies","tags":["annotations_creators:expert-generated","language_creators:found","languages:ady","languages:ang","languages:ar","languages:arn","languages:ast","languages:az","languages:ba","languages:be","languages:bn","languages:bo","languages:br","languages:bg","languages:ca","languages:cs","languages:cu","languages:ckb","languages:kw","languages:crh","languages:csb","languages:cy","languages:da","languages:de","languages:dsb","languages:el","languages:en","languages:et","languages:eu","languages:fo","languages:fa","languages:fi","languages:fr","languages:frm","languages:fro","languages:frr","languages:fy","languages:fur","languages:gal","languages:gd","languages:ga","languages:gv","languages:gmh","languages:gml","languages:got","languages:grc","languages:hai","languages:sh","languages:he","languages:hi","languages:hu","languages:hy","languages:is","languages:it","languages:izh","languages:kl","languages:kn","languages:ka","languages:kk","languages:kbd","languages:kjh","languages:klr","languages:kmr","languages:krl","languages:la","languages:lv","languages:lt","languages:liv","languages:lld","languages:lud","languages:mk","languages:mt","languages:mwf","languages:nap","languages:nv","languages:nds","languages:nl","languages:nn","languages:nb","languages:oc","languages:olo","languages:osx","languages:pl","languages:pt","languages:ps","languages:qu","languages:ro","languages:ru","languages:sa","languages:sga","languages:sl","languages:sme","languages:es","languages:sq","languages:swc","languages:sv","languages:syc","languages:tt","languages:te","languages:tg","languages:tk","languages:tr","languages:uk","languages:ur","languages:uz","languages:vec","languages:vep","languages:vot","languages:xcl","languages:xno","languages:yi","languages:zu","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:1K<n<10K","size_categories:n<1K","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_categories:text-classification","task_ids:multi-class-classification","task_ids:multi-label-classification","task_ids:structure-prediction-other-morphology"],"citation":"@article{sylak2016composition,\n  title={The composition and use of the universal morphological feature schema (unimorph schema)},\n  author={Sylak-Glassman, John},\n  journal={Johns Hopkins University},\n  year={2016}\n}","description":"The Universal Morphology (UniMorph) project is a collaborative effort to improve how NLP handles complex morphology in the world’s languages.\nThe goal of UniMorph is to annotate morphological data in a universal schema that allows an inflected word from any language to be defined by its lexical meaning,\ntypically carried by the lemma, and by a rendering of its inflectional form in terms of a bundle of morphological features from our schema.\nThe specification of the schema is described in Sylak-Glassman (2016).","key":""},{"id":"urdu_fake_news","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:ur","licenses:unknown","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:text-classification","task_ids:fact-checking","task_ids:intent-classification"],"citation":"@article{MaazUrdufake2020,\nauthor = {Amjad, Maaz and Sidorov, Grigori and Zhila, Alisa and  G’{o}mez-Adorno, Helena and Voronkov, Ilia  and Gelbukh, Alexander},\ntitle = {Bend the Truth: A Benchmark Dataset for Fake News Detection in Urdu and Its Evaluation},\njournal={Journal of Intelligent & Fuzzy Systems},\nvolume={39},\nnumber={2},\npages={2457-2469},\ndoi = {10.3233/JIFS-179905},\nyear={2020},\npublisher={IOS Press}\n}","description":"Urdu fake news datasets that contain news of 5 different news domains.\nThese domains are Sports, Health, Technology, Entertainment, and Business.\nThe real news are collected by combining manual approaches.","key":""},{"id":"urdu_sentiment_corpus","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:ur","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{khan2020usc,\n  title={Urdu Sentiment Corpus (v1.0): Linguistic Exploration and Visualization of Labeled Datasetfor Urdu Sentiment Analysis.},\n  author={Khan, Muhammad Yaseen and Nizami, Muhammad Suffian},\n  booktitle={2020 IEEE 2nd International Conference On Information Science & Communication Technology (ICISCT)},\n  pages={},\n  year={2020},\n  organization={IEEE}\n}","description":"“Urdu Sentiment Corpus” (USC) shares the dat of Urdu tweets for the sentiment analysis and polarity detection.\nThe dataset is consisting of tweets and overall, the dataset is comprising over 17, 185 tokens\nwith 52% records as positive, and 48 % records as negative.","paperswithcode_id":"urdu-sentiment-corpus","key":""},{"id":"web_nlg","tags":[],"citation":"@inproceedings{web_nlg,\n  author    = {Claire Gardent and\n               Anastasia Shimorina and\n               Shashi Narayan and\n               Laura Perez{-}Beltrachini},\n  editor    = {Regina Barzilay and\n               Min{-}Yen Kan},\n  title     = {Creating Training Corpora for {NLG} Micro-Planners},\n  booktitle = {Proceedings of the 55th Annual Meeting of the\n               Association for Computational Linguistics,\n               {ACL} 2017, Vancouver, Canada, July 30 - August 4,\n               Volume 1: Long Papers},\n  pages     = {179--188},\n  publisher = {Association for Computational Linguistics},\n  year      = {2017},\n  url       = {https://doi.org/10.18653/v1/P17-1017},\n  doi       = {10.18653/v1/P17-1017}\n}","description":"The WebNLG challenge consists in mapping data to text. The training data consists\nof Data/Text pairs where the data is a set of triples extracted from DBpedia and the text is a verbalisation\nof these triples. For instance, given the 3 DBpedia triples shown in (a), the aim is to generate a text such as (b).\n\na. (John_E_Blaha birthDate 1942_08_26) (John_E_Blaha birthPlace San_Antonio) (John_E_Blaha occupation Fighter_pilot)\nb. John E Blaha, born in San Antonio on 1942-08-26, worked as a fighter pilot\n\nAs the example illustrates, the task involves specific NLG subtasks such as sentence segmentation\n(how to chunk the input data into sentences), lexicalisation (of the DBpedia properties),\naggregation (how to avoid repetitions) and surface realisation\n(how to build a syntactically correct and natural sounding text).","key":""},{"id":"web_of_science","tags":["languages:en"],"citation":"@inproceedings{kowsari2017HDLTex,\ntitle={HDLTex: Hierarchical Deep Learning for Text Classification},\nauthor={Kowsari, Kamran and Brown, Donald E and Heidarysafa, Mojtaba and Jafari Meimandi, Kiana and and Gerber, Matthew S and Barnes, Laura E},\nbooktitle={Machine Learning and Applications (ICMLA), 2017 16th IEEE International Conference on},\nyear={2017},\norganization={IEEE}\n}","description":"The Web Of Science (WOS) dataset is a collection of data  of published papers\navailable from the Web of Science. WOS has been released in three versions: WOS-46985, WOS-11967 and WOS-5736. WOS-46985 is the\nfull dataset. WOS-11967 and WOS-5736 are two subsets of WOS-46985.","paperswithcode_id":"web-of-science-dataset","key":""},{"id":"web_questions","tags":["languages:en"],"citation":"@inproceedings{berant-etal-2013-semantic,\n    title = \"Semantic Parsing on {F}reebase from Question-Answer Pairs\",\n    author = \"Berant, Jonathan  and\n      Chou, Andrew  and\n      Frostig, Roy  and\n      Liang, Percy\",\n    booktitle = \"Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing\",\n    month = oct,\n    year = \"2013\",\n    address = \"Seattle, Washington, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D13-1160\",\n    pages = \"1533--1544\",\n}","description":"This dataset consists of 6,642 question/answer pairs.\nThe questions are supposed to be answerable by Freebase, a large knowledge graph.\nThe questions are mostly centered around a single named entity.\nThe questions are popular ones asked on the web (at least in 2013).","paperswithcode_id":"webquestions","key":""},{"id":"weibo_ner","tags":["annotations_creators:expert-generated","language_creators:found","languages:zh","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"description":"Tags: PER(人名), LOC(地点名), GPE(行政区名), ORG(机构名)\nLabel\tTag\tMeaning\nPER\tPER.NAM\t名字（张三）\nPER.NOM\t代称、类别名（穷人）\nLOC\tLOC.NAM\t特指名称（紫玉山庄）\nLOC.NOM\t泛称（大峡谷、宾馆）\nGPE\tGPE.NAM\t行政区的名称（北京）\nORG\tORG.NAM\t特定机构名称（通惠医院）\nORG.NOM\t泛指名称、统称（文艺公司）","paperswithcode_id":"weibo-ner","key":""},{"id":"wi_locness","tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:en","licenses:other-wi-license","licenses:other-locness-license","multilinguality:monolingual","multilinguality:other-language-learner","size_categories:1K<n<10K","source_datasets:original","task_categories:conditional-text-generation","task_ids:conditional-text-generation-other-grammatical-error-correction","pretty_name:Cambridge English Write & Improve","pretty_name:LOCNESS"],"citation":"@inproceedings{bryant-etal-2019-bea,\n    title = \"The {BEA}-2019 Shared Task on Grammatical Error Correction\",\n    author = \"Bryant, Christopher  and\n        Felice, Mariano  and\n        Andersen, {\\\\O}istein E.  and\n        Briscoe, Ted\",\n    booktitle = \"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications\",\n    month = aug,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W19-4406\",\n    doi = \"10.18653/v1/W19-4406\",\n    pages = \"52--75\",\n    abstract = \"This paper reports on the BEA-2019 Shared Task on Grammatical Error Correction (GEC). As with the CoNLL-2014 shared task, participants are required to correct all types of errors in test data. One of the main contributions of the BEA-2019 shared task is the introduction of a new dataset, the Write{\\\\&}Improve+LOCNESS corpus, which represents a wider range of native and learner English levels and abilities. Another contribution is the introduction of tracks, which control the amount of annotated data available to participants. Systems are evaluated in terms of ERRANT F{\\\\_}0.5, which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set.\",\n}","description":"Write & Improve (Yannakoudakis et al., 2018) is an online web platform that assists non-native\nEnglish students with their writing. Specifically, students from around the world submit letters,\nstories, articles and essays in response to various prompts, and the W&I system provides instant\nfeedback. Since W&I went live in 2014, W&I annotators have manually annotated some of these\nsubmissions and assigned them a CEFR level.","paperswithcode_id":"locness-corpus","key":""},{"id":"wiki40b","tags":["languages:en"],"citation":"","description":"Clean-up text for 40+ Wikipedia languages editions of pages\ncorrespond to entities. The datasets have train/dev/test splits per language.\nThe dataset is cleaned up by page filtering to remove disambiguation pages,\nredirect pages, deleted pages, and non-entity pages. Each example contains the\nwikidata id of the entity, and the full Wikipedia article after page processing\nthat removes non-content sections and structured objects.","paperswithcode_id":"wiki-40b","key":""},{"id":"wiki_asp","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@article{hayashi20tacl,\n  title   = {WikiAsp: A Dataset for Multi-domain Aspect-based Summarization},\n  authors = {Hiroaki Hayashi and Prashant Budania and Peng Wang and Chris Ackerson and Raj Neervannan and Graham Neubig},\n  journal = {Transactions of the Association for Computational Linguistics (TACL)},\n  year    = {2020},\n  url     = {https://arxiv.org/abs/2011.07832}\n}","description":"WikiAsp is a multi-domain, aspect-based summarization dataset in the encyclopedic\ndomain. In this task, models are asked to summarize cited reference documents of a\nWikipedia article into aspect-based summaries. Each of the 20 domains include 10\ndomain-specific pre-defined aspects.","paperswithcode_id":"wikiasp","key":""},{"id":"wiki_atomic_edits","tags":["annotations_creators:found","language_creators:found","languages:de","languages:en","languages:es","languages:fr","languages:it","languages:jp","languages:ru","languages:zh","licenses:cc-by-sa-4.0","multilinguality:multilingual","size_categories:100K<n<1M","size_categories:1M<n<10M","size_categories:10M<n<100M","source_datasets:original","task_categories:conditional-text-generation","task_ids:explanation-generation","task_ids:summarization"],"citation":"@InProceedings{WikiAtomicEdits,\n  title = {{WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse}},\n  author = {Faruqui, Manaal and Pavlick, Ellie and Tenney, Ian and Das, Dipanjan},\n  booktitle = {Proc. of EMNLP},\n  year = {2018}\n}","description":"A dataset of atomic wikipedia edits containing insertions and deletions of a contiguous chunk of text in a sentence. This dataset contains ~43 million edits across 8 languages.\n\nAn atomic edit is defined as an edit e applied to a natural language expression S as the insertion, deletion, or substitution of a sub-expression P such that both the original expression S and the resulting expression e(S) are well-formed semantic constituents (MacCartney, 2009). In this corpus, we release such atomic insertions and deletions made to sentences in wikipedia.","paperswithcode_id":"wikiatomicedits","key":""},{"id":"wiki_auto","tags":["annotations_creators:machine-generated","annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:extended|other-wikipedia","task_categories:conditional-text-generation","task_ids:text-simplification"],"citation":"@inproceedings{acl/JiangMLZX20,\n  author    = {Chao Jiang and\n               Mounica Maddela and\n               Wuwei Lan and\n               Yang Zhong and\n               Wei Xu},\n  editor    = {Dan Jurafsky and\n               Joyce Chai and\n               Natalie Schluter and\n               Joel R. Tetreault},\n  title     = {Neural {CRF} Model for Sentence Alignment in Text Simplification},\n  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational\n               Linguistics, {ACL} 2020, Online, July 5-10, 2020},\n  pages     = {7943--7960},\n  publisher = {Association for Computational Linguistics},\n  year      = {2020},\n  url       = {https://www.aclweb.org/anthology/2020.acl-main.709/}\n}","description":"WikiAuto provides a set of aligned sentences from English Wikipedia and Simple English Wikipedia\nas a resource to train sentence simplification systems. The authors first crowd-sourced a set of manual alignments\nbetween sentences in a subset of the Simple English Wikipedia and their corresponding versions in English Wikipedia\n(this corresponds to the `manual` config), then trained a neural CRF system to predict these alignments.\nThe trained model was then applied to the other articles in Simple English Wikipedia with an English counterpart to\ncreate a larger corpus of aligned sentences (corresponding to the `auto`, `auto_acl`, `auto_full_no_split`, and `auto_full_with_split`  configs here).","key":""},{"id":"wiki_bio","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:conditional-text-generation","task_ids:explanation-generation","task_ids:table-to-text"],"citation":"@article{DBLP:journals/corr/LebretGA16,\n  author    = {R{\\'{e}}mi Lebret and\n               David Grangier and\n               Michael Auli},\n  title     = {Generating Text from Structured Data with Application to the Biography\n               Domain},\n  journal   = {CoRR},\n  volume    = {abs/1603.07771},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1603.07771},\n  archivePrefix = {arXiv},\n  eprint    = {1603.07771},\n  timestamp = {Mon, 13 Aug 2018 16:48:30 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/LebretGA16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"This dataset gathers 728,321 biographies from wikipedia. It aims at evaluating text generation\nalgorithms. For each article, we provide the first paragraph and the infobox (both tokenized).\nFor each article, we extracted the first paragraph (text), the infobox (structured data). Each\ninfobox is encoded as a list of (field name, field value) pairs. We used Stanford CoreNLP\n(http://stanfordnlp.github.io/CoreNLP/) to preprocess the data, i.e. we broke the text into\nsentences and tokenized both the text and the field values. The dataset was randomly split in\nthree subsets train (80%), valid (10%), test (10%).","paperswithcode_id":"wikibio","key":""},{"id":"wiki_dpr","tags":[],"citation":"@misc{karpukhin2020dense,\n    title={Dense Passage Retrieval for Open-Domain Question Answering},\n    author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},\n    year={2020},\n    eprint={2004.04906},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"This is the wikipedia split used to evaluate the Dense Passage Retrieval (DPR) model.\nIt contains 21M passages from wikipedia along with their DPR embeddings.\nThe wikipedia articles were split into multiple, disjoint text blocks of 100 words as passages.","key":""},{"id":"wiki_hop","tags":["annotations_creators:crowdsourced","language_creators:expert-generated","languages:en","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa","task_ids:question-answering-other-multi-hop"],"citation":"@misc{welbl2018constructing,\n      title={Constructing Datasets for Multi-hop Reading Comprehension Across Documents},\n      author={Johannes Welbl and Pontus Stenetorp and Sebastian Riedel},\n      year={2018},\n      eprint={1710.06481},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"WikiHop is open-domain and based on Wikipedia articles; the goal is to recover Wikidata information by hopping through documents. The goal is to answer text understanding queries by combining multiple facts that are spread across different documents.","paperswithcode_id":"wikihop","key":""},{"id":"wiki_lingua","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:ar","languages:zh","languages:cs","languages:nl","languages:en","languages:fr","languages:de","languages:hi","languages:id","languages:it","languages:ja","languages:ko","languages:pt","languages:ru","languages:es","languages:th","languages:tr","languages:vi","licenses:cc-by-3.0","multilinguality:multilingual","size_categories:1K<n<10K","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@article{ladhak-wiki-2020,\n  title   = {WikiLingua: A New Benchmark Dataset for Multilingual Abstractive Summarization},\n  authors = {Faisal Ladhak, Esin Durmus, Claire Cardie and Kathleen McKeown},\n  journal = {arXiv preprint arXiv:2010.03093},\n  year    = {2020},\n  url     = {https://arxiv.org/abs/2010.03093}\n}","description":"WikiLingua is a large-scale multilingual dataset for the evaluation of\ncrosslingual abstractive summarization systems. The dataset includes ~770k\narticle and summary pairs in 18 languages from WikiHow. The gold-standard\narticle-summary alignments across languages was done by aligning the images\nthat are used to describe each how-to step in an article.","paperswithcode_id":"wikilingua","key":""},{"id":"wiki_movies","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-3.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:question-answering","task_ids:closed-domain-qa"],"citation":"@misc{miller2016keyvalue,\n      title={Key-Value Memory Networks for Directly Reading Documents},\n      author={Alexander Miller and Adam Fisch and Jesse Dodge and Amir-Hossein Karimi and Antoine Bordes and Jason Weston},\n      year={2016},\n      eprint={1606.03126},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"The WikiMovies dataset consists of roughly 100k (templated) questions over 75k entities based on questions with answers in the open movie database (OMDb).","paperswithcode_id":"wikimovies","key":""},{"id":"wiki_qa","tags":["languages:en"],"citation":"@InProceedings{YangYihMeek:EMNLP2015:WikiQA,\n       author = {{Yi}, Yang and {Wen-tau},  Yih and {Christopher} Meek},\n        title = \"{WikiQA: A Challenge Dataset for Open-Domain Question Answering}\",\n      journal = {Association for Computational Linguistics},\n         year = 2015,\n          doi = {10.18653/v1/D15-1237},\n        pages = {2013–2018},\n}","description":"Wiki Question Answering corpus from Microsoft","paperswithcode_id":"wikiqa","key":""},{"id":"wiki_qa_ar","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:ar","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"@InProceedings{YangYihMeek:EMNLP2015:WikiQA,\n       author = {{Yi}, Yang and {Wen-tau},  Yih and {Christopher} Meek},\n        title = \"{WikiQA: A Challenge Dataset for Open-Domain Question Answering}\",\n      journal = {Association for Computational Linguistics},\n         year = 2015,\n          doi = {10.18653/v1/D15-1237},\n        pages = {2013–2018},\n}","description":"Arabic Version of WikiQA by automatic automatic machine translators and crowdsourced the selection of the best one to be incorporated into the corpus","paperswithcode_id":"wikiqaar","key":""},{"id":"wiki_snippets","tags":["languages:en"],"citation":"@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}","description":"Wikipedia version split into plain text snippets for dense semantic indexing.","key":""},{"id":"wiki_source","tags":["annotations_creators:found","language_creators:found","languages:en","languages:sv","licenses:unknown","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"@InProceedings{TIEDEMANN12.463,\n  author = {J{\\\"o}rg Tiedemann},\n  title = {Parallel Data, Tools and Interfaces in OPUS},\n  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},\n  year = {2012},\n  month = {may},\n  date = {23-25},\n  address = {Istanbul, Turkey},\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},\n  publisher = {European Language Resources Association (ELRA)},\n  isbn = {978-2-9517408-7-7},\n  language = {english}\n }","description":"2 languages, total number of files: 132\ntotal number of tokens: 1.80M\ntotal number of sentence fragments: 78.36k","key":""},{"id":"wiki_split","tags":["languages:en"],"citation":"@InProceedings{BothaEtAl2018,\n  title = {{Learning To Split and Rephrase From Wikipedia Edit History}},\n  author = {Botha, Jan A and Faruqui, Manaal and Alex, John and Baldridge, Jason and Das, Dipanjan},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  pages = {to appear},\n  note = {arXiv preprint arXiv:1808.09468},\n  year = {2018}\n}","description":"One million English sentences, each split into two sentences that together preserve the original meaning, extracted from Wikipedia\nGoogle's WikiSplit dataset was constructed automatically from the publicly available Wikipedia revision history. Although\nthe dataset contains some inherent noise, it can serve as valuable training data for models that split or merge sentences.","paperswithcode_id":"wikisplit","key":""},{"id":"wiki_summary","tags":["annotations_creators:no-annotation","language_creators:crowdsourced","languages:fa","licenses:apache-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:conditional-text-generation","task_categories:question-answering","task_ids:abstractive-qa","task_ids:explanation-generation","task_ids:extractive-qa","task_ids:machine-translation","task_ids:open-domain-qa","task_ids:summarization","task_ids:text-simplification"],"citation":"\\\r\n@misc{Bert2BertWikiSummaryPersian,\r\n  author = {Mehrdad Farahani},\r\n  title = {Summarization using Bert2Bert model on WikiSummary dataset},\r\n  year = {2020},\r\n  publisher = {GitHub},\r\n  journal = {GitHub repository},\r\n  howpublished = {https://github.com/m3hrdadfi/wiki-summary},\r\n}","description":"\\\r\nThe dataset extracted from Persian Wikipedia into the form of articles and highlights and cleaned the dataset into pairs of articles and highlights and reduced the articles' length (only version 1.0.0) and highlights' length to a maximum of 512 and 128, respectively, suitable for parsBERT.","key":""},{"id":"wikiann","tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:ace","languages:af","languages:als","languages:am","languages:an","languages:ang","languages:ar","languages:arc","languages:arz","languages:as","languages:ast","languages:ay","languages:az","languages:ba","languages:bar","languages:be","languages:bg","languages:bh","languages:bn","languages:bo","languages:br","languages:bs","languages:ca","languages:cdo","languages:ce","languages:ceb","languages:ckb","languages:co","languages:crh","languages:cs","languages:csb","languages:cv","languages:cy","languages:da","languages:de","languages:diq","languages:dv","languages:el","languages:en","languages:eo","languages:es","languages:et","languages:eu","languages:ext","languages:fa","languages:fi","languages:fo","languages:fr","languages:frr","languages:fur","languages:fy","languages:ga","languages:gan","languages:gd","languages:gl","languages:gn","languages:gu","languages:hak","languages:he","languages:hi","languages:hr","languages:hsb","languages:hu","languages:hy","languages:ia","languages:id","languages:ig","languages:ilo","languages:io","languages:is","languages:it","languages:ja","languages:jbo","languages:jv","languages:ka","languages:kk","languages:km","languages:kn","languages:ko","languages:ksh","languages:ku","languages:ky","languages:la","languages:lb","languages:li","languages:lij","languages:lmo","languages:ln","languages:lt","languages:lv","languages:mg","languages:mhr","languages:mi","languages:min","languages:mk","languages:ml","languages:mn","languages:mr","languages:ms","languages:mt","languages:mwl","languages:my","languages:mzn","languages:nap","languages:nds","languages:ne","languages:nl","languages:nn","languages:no","languages:nov","languages:oc","languages:or","languages:os","languages:other-bat-smg","languages:other-be-x-old","languages:other-cbk-zam","languages:other-eml","languages:other-fiu-vro","languages:other-map-bms","languages:other-simple","languages:other-zh-classical","languages:other-zh-min-nan","languages:other-zh-yue","languages:pa","languages:pdc","languages:pl","languages:pms","languages:pnb","languages:ps","languages:pt","languages:qu","languages:rm","languages:ro","languages:ru","languages:rw","languages:sa","languages:sah","languages:scn","languages:sco","languages:sd","languages:sh","languages:si","languages:sk","languages:sl","languages:so","languages:sq","languages:sr","languages:su","languages:sv","languages:sw","languages:szl","languages:ta","languages:te","languages:tg","languages:th","languages:tk","languages:tl","languages:tr","languages:tt","languages:ug","languages:uk","languages:ur","languages:uz","languages:vec","languages:vep","languages:vi","languages:vls","languages:vo","languages:wa","languages:war","languages:wuu","languages:xmf","languages:yi","languages:yo","languages:zea","languages:zh","licenses:unknown","multilinguality:multilingual","size_categories:n<1K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{pan-etal-2017-cross,\n    title = \"Cross-lingual Name Tagging and Linking for 282 Languages\",\n    author = \"Pan, Xiaoman  and\n      Zhang, Boliang  and\n      May, Jonathan  and\n      Nothman, Joel  and\n      Knight, Kevin  and\n      Ji, Heng\",\n    booktitle = \"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2017\",\n    address = \"Vancouver, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P17-1178\",\n    doi = \"10.18653/v1/P17-1178\",\n    pages = \"1946--1958\",\n    abstract = \"The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating {``}silver-standard{''} annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.\",\n}","description":"WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format. This version corresponds to the balanced train, dev, and test splits of Rahimi et al. (2019), which supports 176 of the 282 languages from the original WikiANN corpus.","paperswithcode_id":"wikiann-1","key":""},{"id":"wikicorpus","tags":["annotations_creators:no-annotation","annotations_creators:machine-generated","language_creators:found","languages:ca","languages:en","languages:es","licenses:gfdl-1.1","multilinguality:monolingual","size_categories:100K<n<1M","size_categories:1M<n<10M","size_categories:10M<n<100M","source_datasets:original","task_categories:sequence-modeling","task_categories:structure-prediction","task_categories:text-classification","task_ids:language-modeling","task_ids:structure-prediction-other-lemmatization","task_ids:part-of-speech-tagging","task_ids:text-classification-other-word-sense-disambiguation"],"citation":"@inproceedings{reese-etal-2010-wikicorpus,\n    title = \"{W}ikicorpus: A Word-Sense Disambiguated Multilingual {W}ikipedia Corpus\",\n    author = \"Reese, Samuel  and\n      Boleda, Gemma  and\n      Cuadros, Montse  and\n      Padr{\\'o}, Llu{\\'i}s  and\n      Rigau, German\",\n    booktitle = \"Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)\",\n    month = may,\n    year = \"2010\",\n    address = \"Valletta, Malta\",\n    publisher = \"European Language Resources Association (ELRA)\",\n    url = \"http://www.lrec-conf.org/proceedings/lrec2010/pdf/222_Paper.pdf\",\n    abstract = \"This article presents a new freely available trilingual corpus (Catalan, Spanish, English) that contains large portions of the Wikipedia and has been automatically enriched with linguistic information. To our knowledge, this is the largest such corpus that is freely available to the community: In its present version, it contains over 750 million words. The corpora have been annotated with lemma and part of speech information using the open source library FreeLing. Also, they have been sense annotated with the state of the art Word Sense Disambiguation algorithm UKB. As UKB assigns WordNet senses, and WordNet has been aligned across languages via the InterLingual Index, this sort of annotation opens the way to massive explorations in lexical semantics that were not possible before. We present a first attempt at creating a trilingual lexical resource from the sense-tagged Wikipedia corpora, namely, WikiNet. Moreover, we present two by-products of the project that are of use for the NLP community: An open source Java-based parser for Wikipedia pages developed for the construction of the corpus, and the integration of the WSD algorithm UKB in FreeLing.\",\n}","description":"The Wikicorpus is a trilingual corpus (Catalan, Spanish, English) that contains large portions of the Wikipedia (based on a 2006 dump) and has been automatically enriched with linguistic information. In its present version, it contains over 750 million words.","key":""},{"id":"wikihow","tags":[],"citation":"@misc{koupaee2018wikihow,\n    title={WikiHow: A Large Scale Text Summarization Dataset},\n    author={Mahnaz Koupaee and William Yang Wang},\n    year={2018},\n    eprint={1810.09305},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}","description":"WikiHow is a new large-scale dataset using the online WikiHow\n(http://www.wikihow.com/) knowledge base.\n\nThere are two features:\n  - text: wikihow answers texts.\n  - headline: bold lines as summary.\n\nThere are two separate versions:\n  - all: consisting of the concatenation of all paragraphs as the articles and\n         the bold lines as the reference summaries.\n  - sep: consisting of each paragraph and its summary.\n\nDownload \"wikihowAll.csv\" and \"wikihowSep.csv\" from\nhttps://github.com/mahnazkoupaee/WikiHow-Dataset and place them in manual folder\nhttps://www.tensorflow.org/datasets/api_docs/python/tfds/download/DownloadConfig.\nTrain/validation/test splits are provided by the authors.\nPreprocessing is applied to remove short articles\n(abstract length < 0.75 article length) and clean up extra commas.","paperswithcode_id":"wikihow","key":""},{"id":"wikipedia","tags":["languages:en"],"citation":"@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}","description":"Wikipedia dataset containing cleaned articles of all languages.\nThe datasets are built from the Wikipedia dump\n(https://dumps.wikimedia.org/) with one split per language. Each example\ncontains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).","key":""},{"id":"wikisql","tags":["languages:en"],"citation":"@article{zhongSeq2SQL2017,\n  author    = {Victor Zhong and\n               Caiming Xiong and\n               Richard Socher},\n  title     = {Seq2SQL: Generating Structured Queries from Natural Language using\n               Reinforcement Learning},\n  journal   = {CoRR},\n  volume    = {abs/1709.00103},\n  year      = {2017}\n}","description":"A large crowd-sourced dataset for developing natural language interfaces for relational databases","paperswithcode_id":"wikisql","key":""},{"id":"wikitext","tags":["languages:en"],"citation":"@InProceedings{wikitext,\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\n    year={2016}\n}","description":" The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.","paperswithcode_id":"wikitext-2","key":""},{"id":"wikitext_tl39","tags":["annotations_creators:no-annotation","language_creators:found","languages:fil","languages:tl","licenses:gpl-3.0","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@article{cruz2019evaluating,\n  title={Evaluating Language Model Finetuning Techniques for Low-resource Languages},\n  author={Cruz, Jan Christian Blaise and Cheng, Charibeth},\n  journal={arXiv preprint arXiv:1907.00409},\n  year={2019}\n}","description":"Large scale, unlabeled text dataset with 39 Million tokens in the training set. Inspired by the original WikiText Long Term Dependency dataset (Merity et al., 2016). TL means \"Tagalog.\" Originally published in Cruz & Cheng (2019).","paperswithcode_id":"wikitext-tl-39","key":""},{"id":"wili_2018","tags":["annotations_creators:no-annotation","language_creators:found","languages:af","languages:am","languages:an","languages:as","languages:av","languages:ay","languages:bs","languages:ce","languages:co","languages:cv","languages:dv","languages:eo","languages:gl","languages:gn","languages:gu","languages:ha","languages:hr","languages:ht","languages:ia","languages:id","languages:ie","languages:ig","languages:io","languages:ja","languages:jv","languages:km","languages:ko","languages:ku","languages:kv","languages:ky","languages:lb","languages:lg","languages:li","languages:ln","languages:lo","languages:mg","languages:mi","languages:ml","languages:mn","languages:mr","languages:ms","languages:my","languages:ne","languages:om","languages:or","languages:os","languages:pa","languages:rm","languages:rw","languages:sc","languages:sd","languages:si","languages:sk","languages:sn","languages:so","languages:sr","languages:su","languages:sw","languages:ta","languages:th","languages:tl","languages:tn","languages:to","languages:ug","languages:vi","languages:vo","languages:wa","languages:wo","languages:xh","languages:yo","languages:zh","languages:ar","languages:az","languages:ba","languages:be","languages:bg","languages:bn","languages:bo","languages:br","languages:ca","languages:cs","languages:cy","languages:da","languages:de","languages:el","languages:en","languages:es","languages:et","languages:eu","languages:fa","languages:fi","languages:fo","languages:fr","languages:fy","languages:ga","languages:gd","languages:gv","languages:he","languages:hi","languages:hu","languages:hy","languages:is","languages:it","languages:ka","languages:kk","languages:kn","languages:kw","languages:la","languages:lt","languages:lv","languages:mk","languages:mt","languages:nb","languages:nl","languages:nn","languages:nv","languages:oc","languages:pl","languages:ps","languages:pt","languages:qu","languages:ro","languages:ru","languages:sa","languages:sh","languages:sl","languages:sq","languages:sv","languages:te","languages:tg","languages:tk","languages:tr","languages:tt","languages:uk","languages:ur","languages:uz","languages:yi","languages:ace","languages:als","languages:ang","languages:arz","languages:ast","languages:azb","languages:bar","languages:bcl","languages:bho","languages:bjn","languages:bpy","languages:bxr","languages:cbk","languages:cdo","languages:ceb","languages:chr","languages:ckb","languages:crh","languages:csb","languages:diq","languages:dsb","languages:dty","languages:egl","languages:ext","languages:frp","languages:fur","languages:gag","languages:glk","languages:hak","languages:hif","languages:hsb","languages:ilo","languages:jam","languages:jbo","languages:kaa","languages:kab","languages:kbd","languages:koi","languages:kok","languages:krc","languages:ksh","languages:lad","languages:lez","languages:lij","languages:lmo","languages:lrc","languages:ltg","languages:lzh","languages:mai","languages:mdf","languages:mhr","languages:min","languages:mrj","languages:mwl","languages:myv","languages:mzn","languages:nan","languages:nap","languages:nci","languages:nds","languages:new","languages:nrm","languages:nso","languages:olo","languages:pag","languages:pam","languages:pap","languages:pcd","languages:pdc","languages:pfl","languages:pnb","languages:rue","languages:rup","languages:sah","languages:scn","languages:sco","languages:sgs","languages:sme","languages:srn","languages:stq","languages:szl","languages:tcy","languages:tet","languages:tyv","languages:udm","languages:vec","languages:vep","languages:vls","languages:vro","languages:war","languages:wuu","languages:xmf","languages:zea","languages:other-roa-tara","languages:other-zh-yue","languages:other-map-bms","languages:other-nds-nl","languages:other-be-tarask","licenses:odbl-1.0","multilinguality:multilingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-language-identification"],"citation":"@dataset{thoma_martin_2018_841984,\n  author       = {Thoma, Martin},\n  title        = {{WiLI-2018 - Wikipedia Language Identification database}},\n  month        = jan,\n  year         = 2018,\n  publisher    = {Zenodo},\n  version      = {1.0.0},\n  doi          = {10.5281/zenodo.841984},\n  url          = {https://doi.org/10.5281/zenodo.841984}\n}","description":"It is a benchmark dataset for language identification and contains 235000 paragraphs of 235 languages","paperswithcode_id":"wili-2018","key":""},{"id":"wino_bias","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:mit","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:coreference-resolution"],"citation":"@article{DBLP:journals/corr/abs-1804-06876,\n  author    = {Jieyu Zhao and\n               Tianlu Wang and\n               Mark Yatskar and\n               Vicente Ordonez and\n               Kai{-}Wei Chang},\n  title     = {Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods},\n  journal   = {CoRR},\n  volume    = {abs/1804.06876},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1804.06876},\n  archivePrefix = {arXiv},\n  eprint    = {1804.06876},\n  timestamp = {Mon, 13 Aug 2018 16:47:01 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-06876.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"WinoBias, a Winograd-schema dataset for coreference resolution focused on gender bias.\nThe corpus contains Winograd-schema style sentences with entities corresponding to people\nreferred by their occupation (e.g. the nurse, the doctor, the carpenter).","paperswithcode_id":"winobias","key":""},{"id":"winograd_wsc","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:structure-prediction","task_ids:coreference-resolution"],"citation":"@inproceedings{levesque2012winograd,\n  title={The winograd schema challenge},\n  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},\n  booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},\n  year={2012},\n  organization={Citeseer}\n}","description":"A Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is\nresolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its\nresolution. The schema takes its name from a well-known example by Terry Winograd:\n\n> The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.\n\nIf the word is ``feared'', then ``they'' presumably refers to the city council; if it is ``advocated'' then ``they''\npresumably refers to the demonstrators.","paperswithcode_id":"wsc","key":""},{"id":"winogrande","tags":["languages:en"],"citation":"@InProceedings{ai2:winogrande,\ntitle = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},\nauthors={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi\n},\nyear={2019}\n}","description":"WinoGrande is a new collection of 44k problems, inspired by Winograd Schema Challenge (Levesque, Davis, and Morgenstern\n 2011), but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a\nfill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires\ncommonsense reasoning.","paperswithcode_id":"winogrande","key":""},{"id":"wiqa","tags":["languages:en"],"citation":"@article{wiqa,\n      author    = {Niket Tandon and Bhavana Dalvi Mishra and Keisuke Sakaguchi and Antoine Bosselut and Peter Clark}\n      title     = {WIQA: A dataset for \"What if...\" reasoning over procedural text},\n      journal   = {arXiv:1909.04739v1},\n      year      = {2019},\n}","description":"The WIQA dataset V1 has 39705 questions containing a perturbation and a possible effect in the context of a paragraph.\nThe dataset is split into 29808 train questions, 6894 dev questions and 3003 test questions.","paperswithcode_id":"wiqa","key":""},{"id":"wisesight1000","tags":["annotations_creators:expert-generated","language_creators:found","languages:th","licenses:cc0-1.0","multilinguality:monolingual","size_categories:n<1K","source_datasets:extended|wisesight_sentiment","task_categories:structure-prediction","task_ids:structure-prediction-other-word-tokenization"],"citation":"@software{bact_2019_3457447,\n  author       = {Suriyawongkul, Arthit and\n                  Chuangsuwanich, Ekapol and\n                  Chormai, Pattarawat and\n                  Polpanumas, Charin},\n  title        = {PyThaiNLP/wisesight-sentiment: First release},\n  month        = sep,\n  year         = 2019,\n  publisher    = {Zenodo},\n  version      = {v1.0},\n  doi          = {10.5281/zenodo.3457447},\n  url          = {https://doi.org/10.5281/zenodo.3457447}\n}","description":"`wisesight1000` contains Thai social media texts randomly drawn from the full `wisesight-sentiment`, tokenized by human annotators.\nOut of the labels `neg` (negative), `neu` (neutral), `pos` (positive), `q` (question), 250 samples each. Some texts are removed because\nthey look like spam.Because these samples are representative of real world content, we believe having these annotaed samples will allow\nthe community to robustly evaluate tokenization algorithms.","key":""},{"id":"wisesight_sentiment","tags":["annotations_creators:expert-generated","language_creators:found","languages:th","licenses:cc0-1.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@software{bact_2019_3457447,\n  author       = {Suriyawongkul, Arthit and\n                  Chuangsuwanich, Ekapol and\n                  Chormai, Pattarawat and\n                  Polpanumas, Charin},\n  title        = {PyThaiNLP/wisesight-sentiment: First release},\n  month        = sep,\n  year         = 2019,\n  publisher    = {Zenodo},\n  version      = {v1.0},\n  doi          = {10.5281/zenodo.3457447},\n  url          = {https://doi.org/10.5281/zenodo.3457447}\n}","description":"Wisesight Sentiment Corpus: Social media messages in Thai language with sentiment category (positive, neutral, negative, question)\n* Released to public domain under Creative Commons Zero v1.0 Universal license.\n* Category (Labels): {\"pos\": 0, \"neu\": 1, \"neg\": 2, \"q\": 3}\n* Size: 26,737 messages\n* Language: Central Thai\n* Style: Informal and conversational. With some news headlines and advertisement.\n* Time period: Around 2016 to early 2019. With small amount from other period.\n* Domains: Mixed. Majority are consumer products and services (restaurants, cosmetics, drinks, car, hotels), with some current affairs.\n* Privacy:\n    * Only messages that made available to the public on the internet (websites, blogs, social network sites).\n    * For Facebook, this means the public comments (everyone can see) that made on a public page.\n    * Private/protected messages and messages in groups, chat, and inbox are not included.\n* Alternations and modifications:\n    * Keep in mind that this corpus does not statistically represent anything in the language register.\n    * Large amount of messages are not in their original form. Personal data are removed or masked.\n    * Duplicated, leading, and trailing whitespaces are removed. Other punctuations, symbols, and emojis are kept intact.\n    (Mis)spellings are kept intact.\n    * Messages longer than 2,000 characters are removed.\n    * Long non-Thai messages are removed. Duplicated message (exact match) are removed.\n* More characteristics of the data can be explore: https://github.com/PyThaiNLP/wisesight-sentiment/blob/master/exploration.ipynb","key":""},{"id":"wmt14","tags":[],"citation":"@InProceedings{bojar-EtAl:2014:W14-33,\n  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\\v{s}},\n  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},\n  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},\n  month     = {June},\n  year      = {2014},\n  address   = {Baltimore, Maryland, USA},\n  publisher = {Association for Computational Linguistics},\n  pages     = {12--58},\n  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}\n}","paperswithcode_id":"wmt-2014","key":""},{"id":"wmt15","tags":[],"citation":"@InProceedings{bojar-EtAl:2015:WMT,\n  author    = {Bojar, Ond\\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Haddow, Barry  and  Huck, Matthias  and  Hokamp, Chris  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Post, Matt  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco},\n  title     = {Findings of the 2015 Workshop on Statistical Machine Translation},\n  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},\n  month     = {September},\n  year      = {2015},\n  address   = {Lisbon, Portugal},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1--46},\n  url       = {http://aclweb.org/anthology/W15-3001}\n}","paperswithcode_id":"wmt-2015","key":""},{"id":"wmt16","tags":[],"citation":"@InProceedings{bojar-EtAl:2016:WMT1,\n  author    = {Bojar, Ond\\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and  Jimeno Yepes, Antonio  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Neveol, Aurelie  and  Neves, Mariana  and  Popel, Martin  and  Post, Matt  and  Rubino, Raphael  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco  and  Verspoor, Karin  and  Zampieri, Marcos},\n  title     = {Findings of the 2016 Conference on Machine Translation},\n  booktitle = {Proceedings of the First Conference on Machine Translation},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {131--198},\n  url       = {http://www.aclweb.org/anthology/W/W16/W16-2301}\n}","paperswithcode_id":"wmt-2016","key":""},{"id":"wmt17","tags":[],"citation":"@InProceedings{bojar-EtAl:2017:WMT1,\n  author    = {Bojar, Ond\\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huang, Shujian  and  Huck, Matthias  and  Koehn, Philipp  and  Liu, Qun  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Post, Matt  and  Rubino, Raphael  and  Specia, Lucia  and  Turchi, Marco},\n  title     = {Findings of the 2017 Conference on Machine Translation (WMT17)},\n  booktitle = {Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers},\n  month     = {September},\n  year      = {2017},\n  address   = {Copenhagen, Denmark},\n  publisher = {Association for Computational Linguistics},\n  pages     = {169--214},\n  url       = {http://www.aclweb.org/anthology/W17-4717}\n}","key":""},{"id":"wmt18","tags":[],"citation":"@InProceedings{bojar-EtAl:2018:WMT1,\n  author    = {Bojar, Ond\\v{r}ej  and  Federmann, Christian  and  Fishel, Mark\n    and Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and\n    Koehn, Philipp  and  Monz, Christof},\n  title     = {Findings of the 2018 Conference on Machine Translation (WMT18)},\n  booktitle = {Proceedings of the Third Conference on Machine Translation,\n    Volume 2: Shared Task Papers},\n  month     = {October},\n  year      = {2018},\n  address   = {Belgium, Brussels},\n  publisher = {Association for Computational Linguistics},\n  pages     = {272--307},\n  url       = {http://www.aclweb.org/anthology/W18-6401}\n}","paperswithcode_id":"wmt-2018","key":""},{"id":"wmt19","tags":[],"citation":"@ONLINE {wmt19translate,\n    author = {Wikimedia Foundation},\n    title  = {ACL 2019 Fourth Conference on Machine Translation (WMT19), Shared Task: Machine Translation of News},\n    url    = {http://www.statmt.org/wmt19/translation-task.html}\n}","key":""},{"id":"wmt20_mlqe_task1","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:found","languages:en","languages:de","languages:zh","languages:et","languages:ne","languages:ro","languages:si","languages:ru","licenses:unknown","multilinguality:translation","size_categories:1K<n<10K","source_datasets:extended|reddit","source_datasets:extended|wikipedia","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"Not available.","description":"This shared task (part of WMT20) will build on its previous editions\nto further examine automatic methods for estimating the quality\nof neural machine translation output at run-time, without relying\non reference translations. As in previous years, we cover estimation\nat various levels. Important elements introduced this year include: a new\ntask where sentences are annotated with Direct Assessment (DA)\nscores instead of labels based on post-editing; a new multilingual\nsentence-level dataset mainly from Wikipedia articles, where the\nsource articles can be retrieved for document-wide context; the\navailability of NMT models to explore system-internal information for the task.\n\nTask 1 uses Wikipedia data for 6 language pairs that includes high-resource\nEnglish--German (En-De) and English--Chinese (En-Zh), medium-resource\nRomanian--English (Ro-En) and Estonian--English (Et-En), and low-resource\nSinhalese--English (Si-En) and Nepalese--English (Ne-En), as well as a\ndataset with a combination of Wikipedia articles and Reddit articles\nfor Russian-English (En-Ru). The datasets were collected by translating\nsentences sampled from source language articles using state-of-the-art NMT\nmodels built using the fairseq toolkit and annotated with Direct Assessment (DA)\nscores by professional translators. Each sentence was annotated following the\nFLORES setup, which presents a form of DA, where at least three professional\ntranslators rate each sentence from 0-100 according to the perceived translation\nquality. DA scores are standardised using the z-score by rater. Participating systems\nare required to score sentences according to z-standardised DA scores.","key":""},{"id":"wmt20_mlqe_task2","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:found","languages:en","languages:de","languages:zh","licenses:unknown","multilinguality:translation","size_categories:1K<n<10K","source_datasets:extended|wikipedia","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"Not available.","description":"This shared task (part of WMT20) will build on its previous editions\nto further examine automatic methods for estimating the quality\nof neural machine translation output at run-time, without relying\non reference translations. As in previous years, we cover estimation\nat various levels. Important elements introduced this year include: a new\ntask where sentences are annotated with Direct Assessment (DA)\nscores instead of labels based on post-editing; a new multilingual\nsentence-level dataset mainly from Wikipedia articles, where the\nsource articles can be retrieved for document-wide context; the\navailability of NMT models to explore system-internal information for the task.\n\nTask 2 evaluates the application of QE for post-editing purposes. It consists of predicting:\n- A/ Word-level tags. This is done both on source side (to detect which words caused errors)\nand target side (to detect mistranslated or missing words).\n  - A1/ Each token is tagged as either `OK` or `BAD`. Additionally,\n  each gap between two words is tagged as `BAD` if one or more\n  missing words should have been there, and `OK` otherwise. Note\n  that number of tags for each target sentence is 2*N+1, where\n  N is the number of tokens in the sentence.\n  - A2/ Tokens are tagged as `OK` if they were correctly\n  translated, and `BAD` otherwise. Gaps are not tagged.\n- B/ Sentence-level HTER scores. HTER (Human Translation Error Rate)\nis the ratio between the number of edits (insertions/deletions/replacements)\nneeded and the reference translation length.","key":""},{"id":"wmt20_mlqe_task3","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:found","languages:en","languages:fr","licenses:unknown","multilinguality:translation","size_categories:1K<n<10K","source_datasets:extended|amazon_us_reviews","task_categories:conditional-text-generation","task_ids:machine-translation"],"citation":"Not available.","description":"This shared task (part of WMT20) will build on its previous editions\nto further examine automatic methods for estimating the quality\nof neural machine translation output at run-time, without relying\non reference translations. As in previous years, we cover estimation\nat various levels. Important elements introduced this year include: a new\ntask where sentences are annotated with Direct Assessment (DA)\nscores instead of labels based on post-editing; a new multilingual\nsentence-level dataset mainly from Wikipedia articles, where the\nsource articles can be retrieved for document-wide context; the\navailability of NMT models to explore system-internal information for the task.\n\nThe goal of this task 3 is to predict document-level quality scores as well as fine-grained annotations.","key":""},{"id":"wmt_t2t","tags":[],"citation":"@InProceedings{bojar-EtAl:2014:W14-33,\n  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\\v{s}},\n  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},\n  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},\n  month     = {June},\n  year      = {2014},\n  address   = {Baltimore, Maryland, USA},\n  publisher = {Association for Computational Linguistics},\n  pages     = {12--58},\n  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}\n}","key":""},{"id":"wnut_17","tags":["languages:en"],"citation":"@inproceedings{derczynski-etal-2017-results,\n    title = \"Results of the {WNUT}2017 Shared Task on Novel and Emerging Entity Recognition\",\n    author = \"Derczynski, Leon  and\n      Nichols, Eric  and\n      van Erp, Marieke  and\n      Limsopatham, Nut\",\n    booktitle = \"Proceedings of the 3rd Workshop on Noisy User-generated Text\",\n    month = sep,\n    year = \"2017\",\n    address = \"Copenhagen, Denmark\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W17-4418\",\n    doi = \"10.18653/v1/W17-4418\",\n    pages = \"140--147\",\n    abstract = \"This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions.\n                Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization),\n                but recall on them is a real problem in noisy text - even among annotators.\n                This drop tends to be due to novel entities and surface forms.\n                Take for example the tweet {``}so.. kktny in 30 mins?!{''} {--} even human experts find the entity {`}kktny{'}\n                hard to detect and resolve. The goal of this task is to provide a definition of emerging and of rare entities,\n                and based on that, also datasets for detecting these entities. The task as described in this paper evaluated the\n                ability of participating entries to detect and classify novel and emerging named entities in noisy text.\",\n}","description":"WNUT 17: Emerging and Rare entity recognition\n\nThis shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions.\nNamed entities form the basis of many modern approaches to other tasks (like event clustering and summarisation),\nbut recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms.\nTake for example the tweet “so.. kktny in 30 mins?” - even human experts find entity kktny hard to detect and resolve.\nThis task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text.\n\nThe goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities.","paperswithcode_id":"wnut-2017-emerging-and-rare-entity","key":""},{"id":"wongnai_reviews","tags":["languages:th","licenses:lgpl-3.0-only","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"description":"Wongnai's review dataset contains restaurant reviews and ratings, mainly in Thai language.\nThe reviews are in 5 classes ranging from 1 to 5 stars.","key":""},{"id":"woz_dialogue","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","languages:de","languages:it","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:sequence-modeling","task_categories:structure-prediction","task_categories:text-classification","task_ids:dialogue-modeling","task_ids:multi-class-classification","task_ids:parsing"],"citation":"@misc{wen2017networkbased,\n      title={A Network-based End-to-End Trainable Task-oriented Dialogue System},\n      author={Tsung-Hsien Wen and David Vandyke and Nikola Mrksic and Milica Gasic and Lina M. Rojas-Barahona and Pei-Hao Su and Stefan Ultes and Steve Young},\n      year={2017},\n      eprint={1604.04562},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"Wizard-of-Oz (WOZ) is a dataset for training task-oriented dialogue systems. The dataset is designed around the task of finding a restaurant in the Cambridge, UK area. There are three informable slots (food, pricerange,area) that users can use to constrain the search and six requestable slots (address, phone, postcode plus the three informable slots) that the user can ask a value for once a restaurant has been offered.","paperswithcode_id":"wizard-of-oz","key":""},{"id":"wrbsc","tags":["annotations_creators:expert-generated","language_creators:found","languages:pl","licenses:cc-by-sa-3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:semantic-similarity-classification"],"citation":"@misc{11321/305,\n title = {{WUT} Relations Between Sentences Corpus},\n author = {Oleksy, Marcin and Fikus, Dominika and Wolski, Michal and Podbielska, Malgorzata and Turek, Agnieszka and Kędzia, Pawel},\n url = {http://hdl.handle.net/11321/305},\n note = {{CLARIN}-{PL} digital repository},\n copyright = {Attribution-{ShareAlike} 3.0 Unported ({CC} {BY}-{SA} 3.0)},\n year = {2016}\n}","description":"WUT Relations Between Sentences Corpus contains 2827 pairs of related sentences.\nRelationships are derived from Cross-document Structure Theory (CST), which enables multi-document summarization through identification of cross-document rhetorical relationships within a cluster of related documents.\nEvery relation was marked by at least 3 annotators.","key":""},{"id":"x_stance","tags":[],"citation":"@inproceedings{vamvas2020xstance,\n    author    = \"Vamvas, Jannis and Sennrich, Rico\",\n    title     = \"{X-Stance}: A Multilingual Multi-Target Dataset for Stance Detection\",\n    booktitle = \"Proceedings of the 5th Swiss Text Analytics Conference (SwissText) \\\\& 16th Conference on Natural Language Processing (KONVENS)\",\n    address   = \"Zurich, Switzerland\",\n    year      = \"2020\",\n    month     = \"jun\",\n    url       = \"http://ceur-ws.org/Vol-2624/paper9.pdf\"\n}","description":"The x-stance dataset contains more than 150 political questions, and 67k comments written by candidates on those questions.\n\nIt can be used to train and evaluate stance detection systems.","paperswithcode_id":"x-stance","key":""},{"id":"xcopa","tags":[],"citation":"  @article{ponti2020xcopa,\n  title={{XCOPA: A} Multilingual Dataset for Causal Commonsense Reasoning},\n  author={Edoardo M. Ponti, Goran Glava\\v{s}, Olga Majewska, Qianchu Liu, Ivan Vuli\\'{c} and Anna Korhonen},\n  journal={arXiv preprint},\n  year={2020},\n  url={https://ducdauge.github.io/files/xcopa.pdf}\n}\n\n@inproceedings{roemmele2011choice,\n  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},\n  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},\n  booktitle={2011 AAAI Spring Symposium Series},\n  year={2011},\n  url={https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF},\n}","description":"  XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning\nThe Cross-lingual Choice of Plausible Alternatives dataset is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning across\nlanguages. The dataset is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around\nthe globe. The dataset is challenging as it requires both the command of world knowledge and the ability to generalise to new languages. All the details about the\ncreation of XCOPA and the implementation of the baselines are available in the paper.\\n","paperswithcode_id":"xcopa","key":""},{"id":"xed_en_fi","tags":["annotations_creators:expert-generated","language_creators:found","languages:en","languages:fi","licenses:cc-by-4.0","multilinguality:multilingual","size_categories:10K<n<100K","size_categories:1K<n<10K","source_datasets:extended|other-OpenSubtitles2016","task_categories:text-classification","task_ids:intent-classification","task_ids:multi-class-classification","task_ids:multi-label-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{ohman2020xed,\n  title={XED: A Multilingual Dataset for Sentiment Analysis and Emotion Detection},\n  author={{\\\"O}hman, Emily and P{\\\"a}mies, Marc and Kajava, Kaisla and Tiedemann, J{\\\"o}rg},\n  booktitle={The 28th International Conference on Computational Linguistics (COLING 2020)},\n  year={2020}\n}","description":"A multilingual fine-grained emotion dataset. The dataset consists of human annotated Finnish (25k) and English sentences (30k). Plutchik’s\ncore emotions are used to annotate the dataset with the addition of neutral to create a multilabel multiclass\ndataset. The dataset is carefully evaluated using language-specific BERT models and SVMs to\nshow that XED performs on par with other similar datasets and is therefore a useful tool for\nsentiment analysis and emotion detection.","paperswithcode_id":"xed","key":""},{"id":"xglue","tags":["annotations_creators:crowdsourced","annotations_creators:machine-generated","annotations_creators:expert-generated","annotations_creators:found","language_creators:found","language_creators:crowdsourced","language_creators:expert-generated","language_creators:machine-generated","languages:ar","languages:de","languages:en","languages:es","languages:hi","languages:vi","languages:zh","languages:fr","languages:ru","languages:nl","languages:bg","languages:el","languages:it","languages:pl","languages:th","languages:tr","languages:ur","languages:pt","languages:sw","licenses:cc-by-sa-4.0","licenses:unknown","licenses:other-Licence Universal Dependencies v2.5","licenses:cc-by-nc-4.0","multilinguality:multilingual","multilinguality:translation","size_categories:100K<n<1M","size_categories:10K<n<100K","source_datasets:extended|squad","source_datasets:original","source_datasets:extended|conll2003","source_datasets:extended|xnli","task_categories:question-answering","task_categories:text-classification","task_categories:structure-prediction","task_categories:conditional-text-generation","task_ids:extractive-qa","task_ids:open-domain-qa","task_ids:topic-classification","task_ids:named-entity-recognition","task_ids:summarization","task_ids:text-classification-other-paraphrase-identification","task_ids:parsing","task_ids:acceptability-classification","task_ids:conditional-text-generation-other-question-answering","task_ids:natural-language-inference"],"citation":"@article{Liang2020XGLUEAN,\n  title={XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation},\n  author={Yaobo Liang and Nan Duan and Yeyun Gong and Ning Wu and Fenfei Guo and Weizhen Qi\n  and Ming Gong and Linjun Shou and Daxin Jiang and Guihong Cao and Xiaodong Fan and Ruofei\n  Zhang and Rahul Agrawal and Edward Cui and Sining Wei and Taroon Bharti and Ying Qiao\n  and Jiun-Hung Chen and Winnie Wu and Shuguang Liu and Fan Yang and Daniel Campos\n  and Rangan Majumder and Ming Zhou},\n  journal={arXiv},\n  year={2020},\n  volume={abs/2004.01401}\n}","description":"XGLUE is a new benchmark dataset to evaluate the performance of cross-lingual pre-trained\nmodels with respect to cross-lingual natural language understanding and generation.\nThe benchmark is composed of the following 11 tasks:\n- NER\n- POS Tagging (POS)\n- News Classification (NC)\n- MLQA\n- XNLI\n- PAWS-X\n- Query-Ad Matching (QADSM)\n- Web Page Ranking (WPR)\n- QA Matching (QAM)\n- Question Generation (QG)\n- News Title Generation (NTG)\n\nFor more information, please take a look at https://microsoft.github.io/XGLUE/.","key":""},{"id":"xnli","tags":["languages:en"],"citation":"@InProceedings{conneau2018xnli,\n  author = {Conneau, Alexis\n                 and Rinott, Ruty\n                 and Lample, Guillaume\n                 and Williams, Adina\n                 and Bowman, Samuel R.\n                 and Schwenk, Holger\n                 and Stoyanov, Veselin},\n  title = {XNLI: Evaluating Cross-lingual Sentence Representations},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods\n               in Natural Language Processing},\n  year = {2018},\n  publisher = {Association for Computational Linguistics},\n  location = {Brussels, Belgium},\n}","description":"XNLI is a subset of a few thousand examples from MNLI which has been translated\ninto a 14 different languages (some low-ish resource). As with MNLI, the goal is\nto predict textual entailment (does sentence A imply/contradict/neither sentence\nB) and is a classification task (given two sentences, predict one of three\nlabels).","paperswithcode_id":"xnli","key":""},{"id":"xor_tydi_qa","tags":["annotations_creators:crowdsourced","language_creators:expert-generated","language_creators:found","languages:ar","languages:bn","languages:fi","languages:ja","languages:ko","languages:ru","languages:te","licenses:mit","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","source_datasets:extended|tydiqa","task_categories:question-answering","task_ids:open-domain-qa"],"citation":"    @misc{asai2020xor,\n      title={XOR QA: Cross-lingual Open-Retrieval Question Answering},\n      author={Akari Asai and Jungo Kasai and Jonathan H. Clark and Kenton Lee and Eunsol Choi and Hannaneh Hajishirzi},\n      year={2020},\n      eprint={2010.11856},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"    XOR-TyDi QA brings together for the first time information-seeking questions,\n    open-retrieval QA, and multilingual QA to create a multilingual open-retrieval\n    QA dataset that enables cross-lingual answer retrieval. It consists of questions\n    written by information-seeking native speakers in 7 typologically diverse languages\n    and answer annotations that are retrieved from multilingual document collections.\n    There are three sub-tasks: XOR-Retrieve, XOR-EnglishSpan, and XOR-Full.","paperswithcode_id":"xor-tydi-qa","key":""},{"id":"xquad","tags":["pretty_name:XQuAD","annotations_creators:expert-generated","language_creators:expert-generated","languages:en","languages:fr","languages:es","languages:de","languages:el","languages:bg","languages:ru","languages:tr","languages:ar","languages:vi","languages:zh","licenses:cc-by-sa-4.0","multilinguality:multilingual","size_categories:unknown","source_datasets:extended|squad","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@article{Artetxe:etal:2019,\n      author    = {Mikel Artetxe and Sebastian Ruder and Dani Yogatama},\n      title     = {On the cross-lingual transferability of monolingual representations},\n      journal   = {CoRR},\n      volume    = {abs/1910.11856},\n      year      = {2019},\n      archivePrefix = {arXiv},\n      eprint    = {1910.11856}\n}","description":"XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering\nperformance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set\nof SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German,\nGreek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi and Romanian. Consequently, the dataset is entirely parallel\nacross 12 languages.","paperswithcode_id":"xquad","key":""},{"id":"xquad_r","tags":["annotations_creators:expert-generated","language_creators:found","languages:ar","languages:de","languages:el","languages:en","languages:es","languages:hi","languages:ru","languages:th","languages:tr","languages:vi","languages:zh","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|squad","source_datasets:extended|xquad","task_categories:question-answering","task_ids:extractive-qa"],"citation":"@article{roy2020lareqa,\n  title={LAReQA: Language-agnostic answer retrieval from a multilingual pool},\n  author={Roy, Uma and Constant, Noah and Al-Rfou, Rami and Barua, Aditya and Phillips, Aaron and Yang, Yinfei},\n  journal={arXiv preprint arXiv:2004.05484},\n  year={2020}\n}","description":"XQuAD-R is a retrieval version of the XQuAD dataset (a cross-lingual extractive QA dataset). Like XQuAD, XQUAD-R is an 11-way parallel dataset, where each question appears in 11 different languages and has 11 parallel correct answers across the languages.","paperswithcode_id":"xquad-r","key":""},{"id":"xsum","tags":["languages:en"],"citation":"@article{Narayan2018DontGM,\n  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},\n  author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},\n  journal={ArXiv},\n  year={2018},\n  volume={abs/1808.08745}\n}","description":"Extreme Summarization (XSum) Dataset.\n\nThere are three features:\n  - document: Input news article.\n  - summary: One sentence summary of the article.\n  - id: BBC ID of the article.","paperswithcode_id":"xsum","key":""},{"id":"xsum_factuality","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|other-xsum","task_categories:conditional-text-generation","task_ids:summarization"],"citation":"@InProceedings{maynez_acl20,\n  author =      \"Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan Thomas Mcdonald\",\n  title =       \"On Faithfulness and Factuality in Abstractive Summarization\",\n  booktitle =   \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n  year =        \"2020\",\n  pages = \"1906--1919\",\n  address = \"Online\",\n}","description":"Neural abstractive summarization models are highly prone to hallucinate content that is unfaithful to the input\ndocument. The popular metric such as ROUGE fails to show the severity of the problem. The dataset consists of\nfaithfulness and factuality annotations of abstractive summaries for the XSum dataset. We have crowdsourced 3 judgements\n for each of 500 x 5 document-system pairs. This will be a valuable resource to the abstractive summarization community.","key":""},{"id":"xtreme","tags":["languages:en"],"citation":"@article{hu2020xtreme,\n      author    = {Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},\n      title     = {XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization},\n      journal   = {CoRR},\n      volume    = {abs/2003.11080},\n      year      = {2020},\n      archivePrefix = {arXiv},\n      eprint    = {2003.11080}\n}","description":"The Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark is a benchmark for the evaluation of\nthe cross-lingual generalization ability of pre-trained multilingual models. It covers 40 typologically diverse languages\n(spanning 12 language families) and includes nine tasks that collectively require reasoning about different levels of\nsyntax and semantics. The languages in XTREME are selected to maximize language diversity, coverage in existing tasks,\nand availability of training data. Among these are many under-studied languages, such as the Dravidian languages Tamil\n(spoken in southern India, Sri Lanka, and Singapore), Telugu and Malayalam (spoken mainly in southern India), and the\nNiger-Congo languages Swahili and Yoruba, spoken in Africa.","paperswithcode_id":"xtreme","key":""},{"id":"yahoo_answers_qa","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|other-yahoo-webscope-l6","task_categories:question-answering","task_ids:open-domain-qa"],"description":"Yahoo Non-Factoid Question Dataset is derived from Yahoo's Webscope L6 collection using machine learning techiques such that the questions would contain non-factoid answers.The dataset contains 87,361 questions and their corresponding answers. Each question contains its best answer along with additional other answers submitted by users. Only the best answer was reviewed in determining the quality of the question-answer pair.","key":""},{"id":"yahoo_answers_topics","tags":["annotations_creators:found","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1M<n<10M","source_datasets:extended|other-yahoo-answers-corpus","task_categories:text-classification","task_ids:topic-classification"],"description":"Yahoo! Answers Topic Classification is text classification dataset. The dataset is the Yahoo! Answers corpus as of 10/25/2007. The Yahoo! Answers topic classification dataset is constructed using 10 largest main categories. From all the answers and other meta-information, this dataset only used the best answer content and the main category information.","key":""},{"id":"yelp_polarity","tags":["languages:en"],"citation":"@article{zhangCharacterlevelConvolutionalNetworks2015,\n  archivePrefix = {arXiv},\n  eprinttype = {arxiv},\n  eprint = {1509.01626},\n  primaryClass = {cs},\n  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},\n  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},\n  journal = {arXiv:1509.01626 [cs]},\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  month = sep,\n  year = {2015},\n}","description":"Large Yelp Review Dataset.\nThis is a dataset for binary sentiment classification. We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \nORIGIN\nThe Yelp reviews dataset consists of reviews from Yelp. It is extracted\nfrom the Yelp Dataset Challenge 2015 data. For more information, please\nrefer to http://www.yelp.com/dataset_challenge\n\nThe Yelp reviews polarity dataset is constructed by\nXiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\nIt is first used as a text classification benchmark in the following paper:\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\nfor Text Classification. Advances in Neural Information Processing Systems 28\n(NIPS 2015).\n\n\nDESCRIPTION\n\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2\nnegative, and 3 and 4 positive. For each polarity 280,000 training samples and\n19,000 testing samples are take randomly. In total there are 560,000 trainig\nsamples and 38,000 testing samples. Negative polarity is class 1,\nand positive class 2.\n\nThe files train.csv and test.csv contain all the training samples as\ncomma-sparated values. There are 2 columns in them, corresponding to class\nindex (1 and 2) and review text. The review texts are escaped using double\nquotes (\"), and any internal double quote is escaped by 2 double quotes (\"\").\nNew lines are escaped by a backslash followed with an \"n\" character,\nthat is \"\\n\".","key":""},{"id":"yelp_review_full","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:other-yelp-licence","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"citation":"@inproceedings{zhang2015character,\n  title={Character-level convolutional networks for text classification},\n  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle={Advances in neural information processing systems},\n  pages={649--657},\n  year={2015}\n}","description":"The Yelp reviews dataset consists of reviews from Yelp. It is extracted from the Yelp Dataset Challenge 2015 data.\nThe Yelp reviews full star dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\nIt is first used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun.\nCharacter-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).","key":""},{"id":"yoruba_bbc_topics","tags":["annotations_creators:expert-generated","language_creators:found","languages:yo","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:topic-classification"],"citation":"@inproceedings{hedderich-etal-2020-transfer,\n    title = \"Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages\",\n    author = \"Hedderich, Michael A.  and\n      Adelani, David  and\n      Zhu, Dawei  and\n      Alabi, Jesujoba  and\n      Markus, Udia  and\n      Klakow, Dietrich\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.204\",\n    doi = \"10.18653/v1/2020.emnlp-main.204\",\n}","description":"A collection of news article headlines in Yoruba from BBC Yoruba.\nEach headline is labeled with one of the following classes: africa,\nentertainment, health, nigeria, politics, sport or world.\n\nThe dataset was presented in the paper:\nHedderich, Adelani, Zhu, Alabi, Markus, Klakow: Transfer Learning and\nDistant Supervision for Multilingual Transformer Models: A Study on\nAfrican Languages (EMNLP 2020).","key":""},{"id":"yoruba_gv_ner","tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:yo","licenses:Creative Commons 3.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"citation":"@inproceedings{alabi-etal-2020-massive,\n    title = \"Massive vs. Curated Embeddings for Low-Resourced Languages: the Case of {Yorùbá} and {T}wi\",\n    author = \"Alabi, Jesujoba  and\n      Amponsah-Kaakyire, Kwabena  and\n      Adelani, David  and\n      Espa{\\\\~n}a-Bonet, Cristina\",\n    booktitle = \"Proceedings of the 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.335\",\n    pages = \"2754--2762\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}","description":"The Yoruba GV NER dataset is a labeled dataset for named entity recognition in Yoruba. The texts were obtained from\nYoruba Global Voices News articles https://yo.globalvoices.org/ . We concentrate on\nfour types of named entities: persons [PER], locations [LOC], organizations [ORG], and dates & time [DATE].\n\nThe Yoruba GV NER data files contain 2 columns separated by a tab ('\\t'). Each word has been put on a separate line and\nthere is an empty line after each sentences i.e the CoNLL format. The first item on each line is a word, the second\nis the named entity tag. The named entity tags have the format I-TYPE which means that the word is inside a phrase\nof type TYPE. For every multi-word expression like 'New York', the first word gets a tag B-TYPE and the subsequent words\nhave tags I-TYPE, a word with tag O is not part of a phrase. The dataset is in the BIO tagging scheme.\n\nFor more details, see https://www.aclweb.org/anthology/2020.lrec-1.335/","key":""},{"id":"yoruba_text_c3","tags":["annotations_creators:expert-generated","language_creators:found","languages:yo","licenses:cc-by-nc-4.0","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_ids:language-modeling"],"citation":"@inproceedings{alabi-etal-2020-massive,\n    title = \"Massive vs. Curated Embeddings for Low-Resourced Languages: the Case of Yoruba and {T}wi\",\n    author = \"Alabi, Jesujoba  and\n      Amponsah-Kaakyire, Kwabena  and\n      Adelani, David  and\n      Espa{\\\\~n}a-Bonet, Cristina\",\n    booktitle = \"Proceedings of the 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.335\",\n    pages = \"2754--2762\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}","description":"Yoruba Text C3 is the largest Yoruba texts collected and used to train FastText embeddings in the\nYorubaTwi Embedding paper: https://www.aclweb.org/anthology/2020.lrec-1.335/","key":""},{"id":"yoruba_wordsim353","tags":["annotations_creators:crowdsourced","language_creators:expert-generated","languages:en","languages:yo","licenses:unknown","multilinguality:multilingual","size_categories:n<1K","source_datasets:original","task_categories:text-scoring","task_ids:semantic-similarity-scoring"],"citation":"@inproceedings{alabi-etal-2020-massive,\n    title = \"Massive vs. Curated Embeddings for Low-Resourced Languages: the Case of {Y}or{\\\\`u}b{\\\\'a} and {T}wi\",\n    author = \"Alabi, Jesujoba  and\n      Amponsah-Kaakyire, Kwabena  and\n      Adelani, David  and\n      Espa{\\\\~n}a-Bonet, Cristina\",\n    booktitle = \"Proceedings of the 12th Language Resources and Evaluation Conference\",\n    month = may,\n    year = \"2020\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.335\",\n    pages = \"2754--2762\",\n    language = \"English\",\n    ISBN = \"979-10-95546-34-4\",\n}","description":"A translation of the word pair similarity dataset wordsim-353 to Yorùbá.\n\nThe dataset was presented in the paper\nAlabi et al.: Massive vs. Curated Embeddings for Low-Resourced\nLanguages: the Case of Yorùbá and Twi (LREC 2020).","key":""},{"id":"youtube_caption_corrections","tags":["annotations_creators:expert-generated","annotations_creators:machine-generated","language_creators:machine-generated","languages:en","licenses:mit","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:other","task_categories:sequence-modeling","task_ids:other-other-token-classification-of-text-errors","task_ids:slot-filling"],"description":"Dataset built from pairs of YouTube captions where both 'auto-generated' and\n'manually-corrected' captions are available for a single specified language.\nThis dataset labels two-way (e.g. ignoring single-sided insertions) same-length\ntoken differences in the `diff_type` column. The `default_seq` is composed of\ntokens from the 'auto-generated' captions. When a difference occurs between\nthe 'auto-generated' vs 'manually-corrected' captions types, the `correction_seq`\ncontains tokens from the 'manually-corrected' captions.","key":""},{"id":"zest","tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_categories:structure-prediction","task_ids:closed-domain-qa","task_ids:extractive-qa","task_ids:question-answering-other-yes-no-qa","task_ids:structure-prediction-other-output-structure"],"citation":"@inproceedings{weller-etal-2020-learning,\n    title = \"Learning from Task Descriptions\",\n    author = \"Weller, Orion  and\n      Lourie, Nicholas  and\n      Gardner, Matt  and\n      Peters, Matthew\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.105\",\n    pages = \"1361--1375\",\n    abstract = \"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this frame- work with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model{'}s ability to solve each task. Moreover, the dataset{'}s structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.\",\n}","description":"ZEST tests whether NLP systems can perform unseen tasks in a zero-shot way, given a natural language description of\nthe task. It is an instantiation of our proposed framework \"learning from task descriptions\". The tasks include\nclassification, typed entity extraction and relationship extraction, and each task is paired with 20 different\nannotated (input, output) examples. ZEST's structure allows us to systematically test whether models can generalize\nin five different ways.","paperswithcode_id":"zest","key":""},{"id":"AConsApart/anime_subtitles_DialoGPT","private":false,"tags":[],"author":"AConsApart","key":""},{"id":"Abdo1Kamr/Arabic_Nine_Hadiths_Books","private":false,"tags":[],"author":"Abdo1Kamr","key":""},{"id":"AdWeeb/DravidianMT","private":false,"tags":[],"author":"AdWeeb","key":""},{"id":"Adnan/Urdu_News_Headlines","private":false,"tags":[],"author":"Adnan","key":""},{"id":"Akshith/aa","private":false,"tags":[],"author":"Akshith","key":""},{"id":"Akshith/g_rock","private":false,"tags":[],"author":"Akshith","key":""},{"id":"Akshith/test","private":false,"tags":[],"author":"Akshith","key":""},{"id":"Annielytics/DoctorsNotes","private":false,"tags":[],"author":"Annielytics","key":""},{"id":"Avishekavi/Avi","private":false,"tags":[],"author":"Avishekavi","key":""},{"id":"Binbin/my_dataset","private":false,"tags":[],"author":"Binbin","key":""},{"id":"CAGER/rick","private":false,"tags":[],"author":"CAGER","key":""},{"id":"Cropinky/flatearther","private":false,"tags":[],"author":"Cropinky","key":""},{"id":"Cropinky/rap_lyrics_english","private":false,"tags":[],"author":"Cropinky","key":""},{"id":"Cropinky/wow_fishing_bobber","private":false,"tags":[],"author":"Cropinky","key":""},{"id":"Darren/data","private":false,"tags":[],"author":"Darren","key":""},{"id":"EMBO/biolang","private":false,"tags":["annotations_creators:machine-generated","language_creators:expert-generated","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:n>1M","task_categories:sequence-modeling","task_ids:language-modeling"],"author":"EMBO","citation":"@Unpublished{\n    huggingface: dataset,\n    title = {biolang},\n    authors={Thomas Lemberger, EMBO},\n    year={2021}\n}","description":"This dataset is based on abstracts from the open access section of EuropePubMed Central to train language models in the domain of biology.","key":""},{"id":"EMBO/sd-nlp","private":false,"tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","task_categories:text-classification","task_categories:structure-prediction","task_ids:multi-class-classification","task_ids:named-entity-recognition","task_ids:parsing"],"author":"EMBO","citation":"@Unpublished{\n    huggingface: dataset,\n    title = {SourceData NLP},\n    authors={Thomas Lemberger, EMBO},\n    year={2021}\n}","description":"This dataset is based on the SourceData database and is intented to facilitate training of NLP tasks in the cell and molecualr biology domain.","key":""},{"id":"ESZER/H","private":false,"tags":[],"author":"ESZER","key":""},{"id":"Ebtihal/OSCAR_Arabic","private":false,"tags":[],"author":"Ebtihal","key":""},{"id":"Eymen3455/xsum_tr","private":false,"tags":[],"author":"Eymen3455","key":""},{"id":"FRTNX/cosuju","private":false,"tags":[],"author":"FRTNX","citation":"@InProceedings{huggingface:dataset,\ntitle   = {CoSuJu 500+ Court Judegements and Summaries for Machine Text Summarization},\nauthors = {Busani Ndlovu, Luke Jordan},\nyear    = {2021}\n}","description":"Court Summaries and Judgements (CoSuJu) Dataset","key":""},{"id":"Felix-ML/quoteli3","private":false,"tags":["languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K"],"author":"Felix-ML","citation":"@inproceedings{muzny2017two,\n  title={A two-stage sieve approach for quote attribution},\n  author={Muzny, Grace and Fang, Michael and Chang, Angel and Jurafsky, Dan},\n  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},\n  pages={460--470},\n  year={2017}\n}","description":"This dataset is a representation of Muzny et al.'s QuoteLi3 dataset as a Huggingface dataset. It can be best used for \nquote attribution.","key":""},{"id":"Firoj/CrisisBench","private":false,"tags":[],"author":"Firoj","key":""},{"id":"Fraser/mnist-text-default","private":false,"tags":[],"author":"Fraser","citation":"@dataset{dataset,\n    author = {Fraser Greenlee},\n    year = {2021},\n    month = {1},\n    pages = {},\n    title = {MNIST text dataset.},\n    doi = {}\n}","description":"MNIST dataset adapted to a text-based representation.\n\nThis allows testing interpolation quality for Transformer-VAEs.\n\nSystem is heavily inspired by Matthew Rayfield's work https://youtu.be/Z9K3cwSL6uM\n\nWorks by quantising each MNIST pixel into one of 64 characters.\nEvery sample has an up & down version to encourage the model to learn rotation invarient features.\n\nUse `.array_to_text(` and `.text_to_array(` methods to test your generated data.\n\nData format:\n- text: (30 x 28 tokens, 840 tokens total): Textual representation of MNIST digit, for example:\n```\n00 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n01 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n02 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n03 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n04 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n05 down ! ! ! ! ! ! ! ! ! ! ! ! ! % % % @ C L ' J a ^ @ ! ! ! !\n06 down ! ! ! ! ! ! ! ! ( * 8 G K ` ` ` ` ` Y L ` ] Q 1 ! ! ! !\n07 down ! ! ! ! ! ! ! - \\ ` ` ` ` ` ` ` ` _ 8 5 5 / * ! ! ! ! !\n08 down ! ! ! ! ! ! ! % W ` ` ` ` ` R N ^ ] ! ! ! ! ! ! ! ! ! !\n09 down ! ! ! ! ! ! ! ! 5 H ; ` ` T # ! + G ! ! ! ! ! ! ! ! ! !\n10 down ! ! ! ! ! ! ! ! ! $ ! G ` 7 ! ! ! ! ! ! ! ! ! ! ! ! ! !\n11 down ! ! ! ! ! ! ! ! ! ! ! C ` P ! ! ! ! ! ! ! ! ! ! ! ! ! !\n12 down ! ! ! ! ! ! ! ! ! ! ! # P ` 2 ! ! ! ! ! ! ! ! ! ! ! ! !\n13 down ! ! ! ! ! ! ! ! ! ! ! ! ) ] Y I < ! ! ! ! ! ! ! ! ! ! !\n14 down ! ! ! ! ! ! ! ! ! ! ! ! ! 5 ] ` ` > ' ! ! ! ! ! ! ! ! !\n15 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! , O ` ` F ' ! ! ! ! ! ! ! !\n16 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! % 8 ` ` O ! ! ! ! ! ! ! !\n17 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! _ ` _ 1 ! ! ! ! ! ! !\n18 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! , A N ` ` T ! ! ! ! ! ! ! !\n19 down ! ! ! ! ! ! ! ! ! ! ! ! * F Z ` ` ` _ N ! ! ! ! ! ! ! !\n20 down ! ! ! ! ! ! ! ! ! ! ' = X ` ` ` ` S 4 ! ! ! ! ! ! ! ! !\n21 down ! ! ! ! ! ! ! ! & 1 V ` ` ` ` R 5 ! ! ! ! ! ! ! ! ! ! !\n22 down ! ! ! ! ! ! % K W ` ` ` ` Q 5 # ! ! ! ! ! ! ! ! ! ! ! !\n23 down ! ! ! ! . L Y ` ` ` ` ^ B # ! ! ! ! ! ! ! ! ! ! ! ! ! !\n24 down ! ! ! ! C ` ` ` V B B % ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n25 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n26 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n27 down ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n```\n- label: Just a number with the texts matching label.","key":""},{"id":"Fraser/mnist-text-no-spaces","private":false,"tags":[],"author":"Fraser","citation":"@dataset{dataset,\n    author = {Fraser Greenlee},\n    year = {2021},\n    month = {2},\n    pages = {},\n    title = {MNIST text dataset (no spaces).},\n    doi = {}\n}","description":"MNIST dataset adapted to a text-based representation.\n\nThis allows testing interpolation quality for Transformer-VAEs.\n\nSystem is heavily inspired by Matthew Rayfield's work https://youtu.be/Z9K3cwSL6uM\n\nWorks by quantising each MNIST pixel into one of 64 characters.\nEvery sample has an up & down version to encourage the model to learn rotation invarient features.\n\nUse `.array_to_text(` and `.text_to_array(` methods to test your generated data.\n\nRemoved spaces to get better BPE compression on sequences.\n**Should only be used with a trained tokenizer.**\n\nData format:\n- text: (30 x 28 tokens, 840 tokens total): Textual representation of MNIST digit, for example:\n```\n00down!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n01down!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n02down!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n03down!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n04down!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n05down!!!!!!!!!!!!!%%%@CL'Ja^@!!!!\n06down!!!!!!!!(*8GK`````YL`]Q1!!!!\n07down!!!!!!!-\\\\````````_855/*!!!!!\n08down!!!!!!!%W`````RN^]!!!!!!!!!!\n09down!!!!!!!!5H;``T#!+G!!!!!!!!!!\n10down!!!!!!!!!$!G`7!!!!!!!!!!!!!!\n11down!!!!!!!!!!!C`P!!!!!!!!!!!!!!\n12down!!!!!!!!!!!#P`2!!!!!!!!!!!!!\n13down!!!!!!!!!!!!)]YI<!!!!!!!!!!!\n14down!!!!!!!!!!!!!5]``>'!!!!!!!!!\n15down!!!!!!!!!!!!!!,O``F'!!!!!!!!\n16down!!!!!!!!!!!!!!!%8``O!!!!!!!!\n17down!!!!!!!!!!!!!!!!!_`_1!!!!!!!\n18down!!!!!!!!!!!!!!,AN``T!!!!!!!!\n19down!!!!!!!!!!!!*FZ```_N!!!!!!!!\n20down!!!!!!!!!!'=X````S4!!!!!!!!!\n21down!!!!!!!!&1V````R5!!!!!!!!!!!\n22down!!!!!!%KW````Q5#!!!!!!!!!!!!\n23down!!!!.LY````^B#!!!!!!!!!!!!!!\n24down!!!!C```VBB%!!!!!!!!!!!!!!!!\n25down!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n26down!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n27down!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n```\n- label: Just a number with the texts matching label.","key":""},{"id":"Fraser/mnist-text-small","private":false,"tags":[],"author":"Fraser","citation":"@dataset{dataset,\n    author = {Fraser Greenlee},\n    year = {2021},\n    month = {1},\n    pages = {},\n    title = {MNIST small text dataset.},\n    doi = {}\n}","description":"MNIST dataset adapted to a text-based representation.\n\n*Modified images to be ~1/4 the original area.*\nDone by taking a max pool.\n\nThis allows testing interpolation quality for Transformer-VAEs.\n\nSystem is heavily inspired by Matthew Rayfield's work https://youtu.be/Z9K3cwSL6uM\n\nWorks by quantising each MNIST pixel into one of 64 characters.\nEvery sample has an up & down version to encourage the model to learn rotation invarient features.\n\nUse `.array_to_text(` and `.text_to_array(` methods to test your generated data.\n\nData format:\n- text: (16 x 14 tokens, 224 tokens total): Textual representation of MNIST digit, for example:\n```\n00 down ! ! ! ! ! ! ! ! ! ! ! ! ! !\n01 down ! ! ! ! ! ! ! ! ! ! ! ! ! !\n02 down ! ! ! ! ! ! % % C L a ^ ! !\n03 down ! ! ! - ` ` ` ` ` Y ` Q ! !\n04 down ! ! ! % ` ` ` R ^ ! ! ! ! !\n05 down ! ! ! ! $ G ` ! ! ! ! ! ! !\n06 down ! ! ! ! ! # ` Y < ! ! ! ! !\n07 down ! ! ! ! ! ! 5 ` ` F ! ! ! !\n08 down ! ! ! ! ! ! ! % ` ` 1 ! ! !\n09 down ! ! ! ! ! ! F ` ` ` ! ! ! !\n10 down ! ! ! ! 1 ` ` ` ` 4 ! ! ! !\n11 down ! ! L ` ` ` ` 5 ! ! ! ! ! !\n12 down ! ! ` ` V B ! ! ! ! ! ! ! !\n13 down ! ! ! ! ! ! ! ! ! ! ! ! ! !\n```\n- label: Just a number with the texts matching label.","key":""},{"id":"Fraser/news-category-dataset","private":false,"tags":[],"author":"Fraser","citation":"@dataset{dataset,\n    author = {Misra, Rishabh},\n    year = {2018},\n    month = {06},\n    pages = {},\n    title = {News Category Dataset},\n    doi = {10.13140/RG.2.2.20331.18729}\n}","description":"Copy of [Kaggle dataset](https://www.kaggle.com/rmisra/news-category-dataset), adding to Huggingface for ease of use.\n\nDescription from Kaggle:\n\nContext\n\nThis dataset contains around 200k news headlines from the year 2012 to 2018 obtained from HuffPost. The model trained on this dataset could be used to identify tags for untracked news articles or to identify the type of language used in different news articles.\n\nContent\n\nEach news headline has a corresponding category. Categories and corresponding article counts are as follows:\n```\nPOLITICS: 32739\nWELLNESS: 17827\nENTERTAINMENT: 16058\nTRAVEL: 9887\nSTYLE & BEAUTY: 9649\nPARENTING: 8677\nHEALTHY LIVING: 6694\nQUEER VOICES: 6314\nFOOD & DRINK: 6226\nBUSINESS: 5937\nCOMEDY: 5175\nSPORTS: 4884\nBLACK VOICES: 4528\nHOME & LIVING: 4195\nPARENTS: 3955\nTHE WORLDPOST: 3664\nWEDDINGS: 3651\nWOMEN: 3490\nIMPACT: 3459\nDIVORCE: 3426\nCRIME: 3405\nMEDIA: 2815\nWEIRD NEWS: 2670\nGREEN: 2622\nWORLDPOST: 2579\nRELIGION: 2556\nSTYLE: 2254\nSCIENCE: 2178\nWORLD NEWS: 2177\nTASTE: 2096\nTECH: 2082\nMONEY: 1707\nARTS: 1509\nFIFTY: 1401\nGOOD NEWS: 1398\nARTS & CULTURE: 1339\nENVIRONMENT: 1323\nCOLLEGE: 1144\nLATINO VOICES: 1129\nCULTURE & ARTS: 1030\nEDUCATION: 1004\n```\n\nAcknowledgements\n\nThis dataset was collected from HuffPost.\n\nInspiration\n\nCan you categorize news articles based on their headlines and short descriptions?\nDo news articles from different categories have different writing styles?\nA classifier trained on this dataset could be used on a free text to identify the type of language being used.","key":""},{"id":"Fraser/python-lines","private":false,"tags":[],"author":"Fraser","citation":"@dataset{dataset,\n    author = {Fraser Greenlee},\n    year = {2020},\n    month = {12},\n    pages = {},\n    title = {Python single line dataset.},\n    doi = {}\n}","description":"Dataset of single lines of Python code taken from the [CodeSearchNet](https://github.com/github/CodeSearchNet) dataset.\n\nContext\n\nThis dataset allows checking the validity of Variational-Autoencoder latent spaces by testing what percentage of random/intermediate latent points can be greedily decoded into valid Python code.\n\nContent\n\nEach row has a parsable line of source code.\n{'text': '{python source code line}'}\n\nMost lines are < 100 characters while all are under 125 characters.\n\nContains 2.6 million lines.\n\nAll code is in parsable into a python3 ast.","key":""},{"id":"Fraser/short-jokes","private":false,"tags":[],"author":"Fraser","description":"Copy of [Kaggle dataset](https://www.kaggle.com/abhinavmoudgil95/short-jokes), adding to Huggingface for ease of use.\n\nDescription from Kaggle:\n\nContext\n\nGenerating humor is a complex task in the domain of machine learning, and it requires the models to understand the deep semantic meaning of a joke in order to generate new ones. Such problems, however, are difficult to solve due to a number of reasons, one of which is the lack of a database that gives an elaborate list of jokes. Thus, a large corpus of over 0.2 million jokes has been collected by scraping several websites containing funny and short jokes.\n\nVisit my Github repository for more information regarding collection of data and the scripts used.\n\nContent\n\nThis dataset is in the form of a csv file containing 231,657 jokes. Length of jokes ranges from 10 to 200 characters. Each line in the file contains a unique ID and joke.\n\nDisclaimer\n\nIt has been attempted to keep the jokes as clean as possible. Since the data has been collected by scraping websites, it is possible that there may be a few jokes that are inappropriate or offensive to some people.","key":""},{"id":"Gabriel/squad_v2_sv","private":false,"tags":[],"author":"Gabriel","key":""},{"id":"GalacticAI/Noirset","private":false,"tags":[],"author":"GalacticAI","key":""},{"id":"Gwangho/NCBI-Sars-Cov-2","private":false,"tags":[],"author":"Gwangho","key":""},{"id":"Halilyesilceng/autonlp-data-nameEntityRecognition","private":false,"tags":[],"author":"Halilyesilceng","key":""},{"id":"HarleyQ/WitcherDialogue","private":false,"tags":[],"author":"HarleyQ","key":""},{"id":"Harveenchadha/Gujarati-Monolingual-Data","private":false,"tags":[],"author":"Harveenchadha","key":""},{"id":"HarveyBWest/mybot","private":false,"tags":[],"author":"HarveyBWest","key":""},{"id":"Jean-Baptiste/wikiner_fr","private":false,"tags":["language_creators:crowdsourced","languages:fr","licenses:unknown","multilinguality:None","size_categories:100K<n<1M","source_datasets:wikiner","task_categories:structure-prediction","task_ids:named-entity-recognition"],"author":"Jean-Baptiste","citation":"@InProceedings{huggingface:dataset,\ntitle = {Wikiner dataset for NER task in french},\nauthors={Created by Nothman et al. at 2013},\nyear={2013}\n}","description":"Dataset can be used to train on NER task for french langugage.","key":""},{"id":"KETI-AIR/klue","private":false,"tags":[],"author":"KETI-AIR","citation":"@misc{park2021klue,\n      title={KLUE: Korean Language Understanding Evaluation}, \n      author={Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho Alice Oh Jungwoo Ha Kyunghyun Cho},\n      year={2021},\n      eprint={2105.09680},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","key":""},{"id":"KETI-AIR/kor_corpora","private":false,"tags":[],"author":"KETI-AIR","key":""},{"id":"KETI-AIR/korquad","private":false,"tags":[],"author":"KETI-AIR","citation":"@article{DBLP:journals/corr/abs-1909-07005,\n  author    = {Seungyoung Lim and\n               Myungji Kim and\n               Jooyoul Lee},\n  title     = {KorQuAD1.0: Korean {QA} Dataset for Machine Reading Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1909.07005},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1909.07005},\n  archivePrefix = {arXiv},\n  eprint    = {1909.07005},\n  timestamp = {Mon, 23 Sep 2019 18:07:15 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-07005.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"KorQuAD1.0","key":""},{"id":"KETI-AIR/nikl","private":false,"tags":[],"author":"KETI-AIR","citation":"","description":"Description is **formatted** as markdown.\n\nIt should also contain any processing which has been applied (if any),\n(e.g. corrupted example skipped, images cropped,...):","key":""},{"id":"LIAMF-USP/arc-retrieval-c4","private":false,"tags":[],"author":"LIAMF-USP","description":"A combined ARC/ARC-Easy/OBQA/RegLivEnv train/dev/test sets,\n along with associated retrieved contexts from the full corpus.\n The \"para\" field for each answer choice is the retrieved context,\n typically 10 sentences ordered such that the one with highest IR\n score comes last","key":""},{"id":"MKK/Dhivehi-English","private":false,"tags":[],"author":"MKK","key":""},{"id":"MarianaSahagun/test","private":false,"tags":[],"author":"MarianaSahagun","key":""},{"id":"Melinoe/TheLabTexts","private":false,"tags":[],"author":"Melinoe","key":""},{"id":"Mina/Preprocessed_Wikipedia_Articles","private":false,"tags":[],"author":"Mina","key":""},{"id":"NTUYG/RAGTest","private":false,"tags":[],"author":"NTUYG","key":""},{"id":"Narsil/Somedataset","private":false,"tags":[],"author":"Narsil","key":""},{"id":"NbAiLab/nb_nn","private":false,"tags":[],"author":"NbAiLab","citation":"@article{XXX,\n    author = {...},\n    title = {...},\n    journal = {arXiv e-prints},\n    year = {2021},\n    archivePrefix = {arXiv},\n    eprint = {},\n}","description":"A dataset of ... based on ...","key":""},{"id":"NbAiLab/norec_agg","private":false,"tags":["annotations_creators:expert-generated","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"author":"NbAiLab","citation":"@InProceedings{OvrMaeBar20,\n  author = {Lilja {\\O}vrelid and Petter M{\\ae}hlum and Jeremy Barnes and Erik Velldal},\n  title = {A Fine-grained Sentiment Dataset for {N}orwegian},\n  booktitle = {{Proceedings of the 12th Edition of the Language Resources and Evaluation Conference}},\n  year = 2020,\n  address = \"Marseille, France, 2020\"\n}","description":"Aggregated NoRec_fine: A Fine-grained Sentiment Dataset for Norwegian\nThis dataset was created by the Nordic Language Processing Laboratory by\naggregating the fine-grained annotations in NoReC_fine and removing sentences\nwith conflicting or no sentiment.","key":""},{"id":"NbAiLab/norne","private":false,"tags":["annotations_creators:expert-generated","language_creators:crowdsourced","languages:no","licenses:other-national-library-of-norway","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:structure-prediction","task_ids:named-entity-recognition"],"author":"NbAiLab","citation":"@inproceedings{johansen2019ner,\n  title={NorNE: Annotating Named Entities for Norwegian},\n  author={Fredrik Jørgensen, Tobias Aasmoe, Anne-Stine Ruud Husevåg,\n          Lilja Øvrelid, and Erik Velldal},\n  booktitle={LREC 2020},\n  year={2020},\n  url={https://arxiv.org/abs/1911.12146}\n}","description":"NorNE is a manually annotated\ncorpus of named entities which extends the annotation of the existing\nNorwegian Dependency Treebank. Comprising both of the official standards of\nwritten Norwegian (Bokmål and Nynorsk), the corpus contains around 600,000\ntokens and annotates a rich set of entity types including persons,\norganizations, locations, geo-political entities, products, and events,\nin addition to a class corresponding to nominals derived from names.","key":""},{"id":"NbAiLab/norwegian_parliament","private":false,"tags":["annotations_creators:expert-generated","language_creators:found","languages:no","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification"],"author":"NbAiLab","citation":"@InProceedings{--,\n  author = {---},\n  title = {---},\n  booktitle = {---},\n  year = 2021,\n  address = \"---\"\n}","description":"The Norwegian Parliament Speeches is a collection of text passages from\n1998 to 2016 and pronounced at the Norwegian Parliament (Storting) by members\nof the two major parties: Fremskrittspartiet and Sosialistisk Venstreparti.","key":""},{"id":"Ofrit/tmp","private":false,"tags":[],"author":"Ofrit","key":""},{"id":"QA/abk-eng","private":false,"tags":[],"author":"QA","key":""},{"id":"Remesita/tagged_reviews","private":false,"tags":[],"author":"Remesita","key":""},{"id":"SCourthial/test","private":false,"tags":[],"author":"SCourthial","key":""},{"id":"SajjadAyoubi/persian_qa","private":false,"tags":[],"author":"SajjadAyoubi","citation":"\\@misc{PersianQA, \n  author          = {Sajjad Ayoubi, Mohammad Yasin Davoodeh},\n  title           = {PersianQA: a dataset for Persian Question Answering},\n  year            = 2021,\n  publisher       = {GitHub},\n  journal         = {GitHub repository},\n  howpublished    = {url{https://github.com/SajjjadAyobi/PersianQA}},\n}","description":"\\\\\\\\\\\\\\Persian Question Answering (PersianQA) Dataset is a reading comprehension dataset on Persian Wikipedia. \nThe crowd-sourced dataset consists of more than 9,000 entries. Each entry can be either an impossible to answer or a question with one or more answers spanning in the passage (the context) from which the questioner proposed the question. Much like the SQuAD2.0 dataset, the impossible or unanswerable questions can be utilized to create a system which \"knows that it doesn't know the answer\".","key":""},{"id":"Shreesha/discord-rick-bot","private":false,"tags":[],"author":"Shreesha","key":""},{"id":"TRoboto/masc","private":false,"tags":[],"author":"TRoboto","key":""},{"id":"Tatyana/ru_sentiment_dataset","private":false,"tags":["language:ru","tags:sentiment","tags:text-classification"],"author":"Tatyana","key":""},{"id":"Terry0107/RiSAWOZ","private":false,"tags":[],"author":"Terry0107","key":""},{"id":"TimTreasure4/Test","private":false,"tags":[],"author":"TimTreasure4","key":""},{"id":"Trainmaster9977/957","private":false,"tags":[],"author":"Trainmaster9977","key":""},{"id":"Trainmaster9977/zbakuman","private":false,"tags":[],"author":"Trainmaster9977","key":""},{"id":"Tyler/wikimatrix_collapsed","private":false,"tags":[],"author":"Tyler","key":""},{"id":"Valahaar/wsdmt","private":false,"tags":[],"author":"Valahaar","key":""},{"id":"Vishva/UniFAQ_DataSET","private":false,"tags":[],"author":"Vishva","key":""},{"id":"Wikidepia/IndoParaCrawl","private":false,"tags":[],"author":"Wikidepia","key":""},{"id":"Wikidepia/IndoSQuAD","private":false,"tags":[],"author":"Wikidepia","key":""},{"id":"XiangXiang/clt","private":false,"tags":[],"author":"XiangXiang","key":""},{"id":"Yves/fhnw_swiss_parliament","private":false,"tags":["annotations_creators:other","language_creators:other","languages:sg","licenses:mit","multilinguality:monolingual","size_categories:90K<n<150K","task_categories:cross-language-transcription","task_ids:automatic-speech-recognition"],"author":"Yves","citation":"@inproceedings{\n  author = {Michel Pluss, Lukas Neukom, Manfred Vogel},\n  title = {Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech to Standard German Text Corpus},\n  pages = {1-5},\n  year = 2020\n}","description":"Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech to Standard German Text Corpus (Michel Plüss et. al.)\nThe inputs are a Swiss German audio recording of arbitrary length, e.g. in FLAC format, and the corresponding manual Standard German text transcript. The audio file is transcribed with Amazon Transcribe. The Amazon transcript is then globally aligned to the manual transcript using Biopython’s (Cock et al., 2009) PairwiseAligner. The manual transcript is split into sentences using spaCy (Honnibal and Montani, 2017). Each of these sentences is mapped to a start and end time in the recording via the global alignment and the per-word start and end times provided by Amazon Transcribe.","key":""},{"id":"abhishek/autonlp-data-imdb_eval","private":false,"tags":[],"author":"abhishek","key":""},{"id":"abwicke/C-B-R","private":false,"tags":[],"author":"abwicke","key":""},{"id":"abwicke/koplo","private":false,"tags":[],"author":"abwicke","key":""},{"id":"adamlin/re_dial","private":false,"tags":[],"author":"adamlin","citation":"@inproceedings{li2018conversational,\n  title={Towards Deep Conversational Recommendations},\n  author={Li, Raymond and Kahou, Samira Ebrahimi and Schulz, Hannes and Michalski, Vincent and Charlin, Laurent and Pal, Chris},\n  booktitle={Advances in Neural Information Processing Systems 31 (NIPS 2018)},\n  year={2018}\n}","description":"ReDial (Recommendation Dialogues) is an annotated dataset of dialogues, where users\nrecommend movies to each other. The dataset was collected by a team of researchers working at\nPolytechnique Montréal, MILA – Quebec AI Institute, Microsoft Research Montréal, HEC Montreal, and Element AI.\nThe dataset allows research at the intersection of goal-directed dialogue systems\n(such as restaurant recommendation) and free-form (also called “chit-chat”) dialogue systems.","key":""},{"id":"ajmbell/test-dataset","private":false,"tags":[],"author":"ajmbell","key":""},{"id":"albertvillanova/tests-raw-jsonl","private":false,"tags":[],"author":"albertvillanova","key":""},{"id":"alireza655/alireza655","private":false,"tags":[],"author":"alireza655","key":""},{"id":"allenai/c4","private":false,"tags":[],"author":"allenai","key":""},{"id":"anukaver/EstQA","private":false,"tags":["language:et"],"author":"anukaver","key":""},{"id":"artrsousa/brwac-pt","private":false,"tags":[],"author":"artrsousa","key":""},{"id":"artrsousa/oscar-pt","private":false,"tags":[],"author":"artrsousa","key":""},{"id":"ashish-shrivastava/dont-know-dataset","private":false,"tags":[],"author":"ashish-shrivastava","key":""},{"id":"astarostap/antisemitic-tweets","private":false,"tags":[],"author":"astarostap","key":""},{"id":"astarostap/antisemitic_tweets","private":false,"tags":[],"author":"astarostap","key":""},{"id":"athivvat/thai-rap-lyrics","private":false,"tags":[],"author":"athivvat","key":""},{"id":"ausgequetschtem/jtrddfhfgh","private":false,"tags":[],"author":"ausgequetschtem","key":""},{"id":"bavard/personachat_truecased","private":false,"tags":[],"author":"bavard","citation":"@article{zhang2018personalizing,\n  title={Personalizing dialogue agents: I have a dog, do you have pets too?},\n  author={Zhang, Saizheng and Dinan, Emily and Urbanek, Jack and Szlam, Arthur and Kiela, Douwe and Weston, Jason},\n  journal={arXiv preprint arXiv:1801.07243},\n  year={2018}\n}","description":"A version of the PersonaChat dataset that has been true-cased, and also has been given more normalized punctuation.\nThe original PersonaChat dataset is in all lower case, and has extra space around each clause/sentence separating\npunctuation mark. This version of the dataset has more of a natural language look, with sentence capitalization,\nproper noun capitalization, and normalized whitespace. Also, each dialogue turn includes a pool of distractor\ncandidate responses, which can be used by a multiple choice regularization loss during training.","key":""},{"id":"bemanningssitua/dplremjfjfj","private":false,"tags":[],"author":"bemanningssitua","key":""},{"id":"berkergurcay/2020-10K-Reports","private":false,"tags":[],"author":"berkergurcay","key":""},{"id":"bsc/ancora-ca-ner","private":false,"tags":[],"author":"bsc","citation":"","description":"AnCora Catalan NER.\n                  This is a dataset for Named Eentity Reacognition (NER) from Ancora corpus adapted for \n                  Machine Learning and Language Model evaluation purposes.\n                  Since multiwords (including Named Entites) in the original Ancora corpus are aggregated as \n                  a single lexical item using underscores (e.g. \"Ajuntament_de_Barcelona\") \n                  we splitted them to align with word-per-line format, and added conventional Begin-Inside-Outside (IOB)\n                   tags to mark and classify Named Entites. \n                   We did not filter out the different categories of NEs from Ancora (weak and strong). \n                   We did 6 minor edits by hand.\n                  AnCora corpus is used under [CC-by] (https://creativecommons.org/licenses/by/4.0/) licence.\n                  This dataset was developed by BSC TeMU as part of the AINA project, and to enrich the Catalan Language Understanding Benchmark (CLUB).","key":""},{"id":"bsc/sts-ca","private":false,"tags":[],"author":"bsc","citation":"Rodriguez-Penagos, Carlos Gerardo, Armentano-Oller, Carme, Gonzalez-Agirre, Aitor, & Gibert Bonet, Ona. (2021). \n               Semantic Textual Similarity in Catalan (Version 1.0.1) [Data set]. \n               Zenodo. http://doi.org/10.5281/zenodo.4761434","description":"Semantic Textual Similarity in Catalan.\n                  STS corpus is a benchmark for evaluating Semantic Text Similarity in Catalan.\n                  It consists of more than 3000 sentence pairs, annotated with the semantic similarity between them, \n                  using a scale from 0 (no similarity at all) to 5 (semantic equivalence). \n                  It is done manually by 4 different annotators following our guidelines based on previous work from the SemEval challenges (https://www.aclweb.org/anthology/S13-1004.pdf).\n                  The source data are scraped sentences from the Catalan Textual Corpus (https://doi.org/10.5281/zenodo.4519349), used under CC-by-SA-4.0 licence (https://creativecommons.org/licenses/by-sa/4.0/). The dataset is released under the same licence.\n                  This dataset was developed by BSC TeMU as part of the AINA project, and to enrich the Catalan Language Understanding Benchmark (CLUB).\n                  This is the version 1.0.2 of the dataset with the complete human and automatic annotations and the analysis scripts. It also has a more accurate license.\n                  This dataset can be used to build and score semantic similiarity models.","key":""},{"id":"bsc/tecla","private":false,"tags":[],"author":"bsc","citation":"Carrino, Casimiro Pio, Rodriguez-Penagos, Carlos Gerardo, & Armentano-Oller, Carme. (2021). \n               TeCla: Text Classification Catalan dataset (Version 1.0) [Data set]. \n               Zenodo. http://doi.org/10.5281/zenodo.4627198","description":"TeCla: Text Classification Catalan dataset\n                   Catalan News corpus for Text classification, crawled from ACN (Catalan News Agency) site: www.acn.cat\n                   Corpus de notícies en català per a classificació textual, extret del web de l'Agència Catalana de Notícies - www.acn.cat","key":""},{"id":"bsc/viquiquad","private":false,"tags":[],"author":"bsc","citation":"Rodriguez-Penagos, Carlos Gerardo, & Armentano-Oller, Carme. (2021). \n               ViquiQuAD: an extractive QA dataset from Catalan Wikipedia (Version ViquiQuad_v.1.0.1) \n               [Data set]. Zenodo. http://doi.org/10.5281/zenodo.4761412","description":"ViquiQuAD: an extractive QA dataset from Catalan Wikipedia.\n                  This dataset contains 3111 contexts extracted from a set of 597 high quality original (no translations) \n                  articles in the Catalan Wikipedia \"Viquipèdia\" (ca.wikipedia.org), and 1 to 5 questions with their\n                  answer for each fragment. Viquipedia articles are used under CC-by-sa licence. \n                  This dataset can be used to build extractive-QA and Language Models.\n                  Funded by the Generalitat de Catalunya, Departament de Polítiques Digitals i Administració Pública (AINA),\n                  MT4ALL and Plan de Impulso de las Tecnologías del Lenguaje (Plan TL).","key":""},{"id":"bsc/xquad-ca","private":false,"tags":[],"author":"bsc","citation":"Carlos Gerardo Rodriguez-Penagos, & Carme Armentano-Oller. (2021). XQuAD-ca [Data set].\n                Zenodo. http://doi.org/10.5281/zenodo.4757559","description":"Professional translation into Catalan of XQuAD dataset (https://github.com/deepmind/xquad).\n                  XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating \n                  cross-lingual question answering performance. \n                  The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from \n                  the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with \n                  their professional translations into ten languages: \n                  Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. \n                  Rumanian was added later.\n                  We added the 13th language to the corpus using also professional native catalan translators.\n                  XQuAD and XQuAD-Ca datasets are released under CC-by-sa licence.","key":""},{"id":"caca/zscczs","private":false,"tags":[],"author":"caca","key":""},{"id":"cakiki/args_me","private":false,"tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:'en-US'","licenses:cc-by-4.0","multilinguality:monolingual","pretty_name:Webis args.me argument corpus","size_categories:100K<n<1M","source_datasets:original","task_categories:text-retrieval","task_ids:document-retrieval"],"author":"cakiki","citation":"@dataset{yamen_ajjour_2020_4139439,\n  author       = {Yamen Ajjour and\n                  Henning Wachsmuth and\n                  Johannes Kiesel and\n                  Martin Potthast and\n                  Matthias Hagen and\n                  Benno Stein},\n  title        = {args.me corpus},\n  month        = oct,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {1.0-cleaned},\n  doi          = {10.5281/zenodo.4139439},\n  url          = {https://doi.org/10.5281/zenodo.4139439}\n}","description":"The args.me corpus (version 1.0, cleaned) comprises 382 545 arguments crawled from four debate portals in the middle of 2019. The debate portals are Debatewise, IDebate.org, Debatepedia, and Debate.org. The arguments are extracted using heuristics that are designed for each debate portal.","key":""},{"id":"canwenxu/dogwhistle","private":false,"tags":[],"author":"canwenxu","key":""},{"id":"ccccccc/hdjw_94ejrjr","private":false,"tags":[],"author":"ccccccc","key":""},{"id":"cdminix/mgb1","private":false,"tags":[],"author":"cdminix","citation":"@inproceedings{bell2015mgb,\n  title={The MGB challenge: Evaluating multi-genre broadcast media recognition},\n  author={Bell, Peter and Gales, Mark JF and Hain, Thomas and Kilgour, Jonathan and Lanchantin, Pierre and Liu, Xunying and McParland, Andrew and Renals, Steve and Saz, Oscar and Wester, Mirjam and others},\n  booktitle={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},\n  pages={687--693},\n  year={2015},\n  organization={IEEE}\n}","description":"The first edition of the Multi-Genre Broadcast (MGB-1) Challenge is an evaluation of speech recognition, speaker diarization, and lightly supervised alignment using TV recordings in English.\n\nThe speech data is broad and multi-genre, spanning the whole range of TV output, and represents a challenging task for speech technology.\n\nIn 2015, the challenge used data from the British Broadcasting Corporation (BBC).","key":""},{"id":"cemigo/taylor_vs_shakes","private":false,"tags":[],"author":"cemigo","key":""},{"id":"cemigo/test-data","private":false,"tags":[],"author":"cemigo","key":""},{"id":"cheulyop/ksponspeech","private":false,"tags":[],"author":"cheulyop","citation":"@article{bang2020ksponspeech,\n  title={KsponSpeech: Korean spontaneous speech corpus for automatic speech recognition},\n  author={Bang, Jeong-Uk and Yun, Seung and Kim, Seung-Hi and Choi, Mu-Yeol and Lee, Min-Kyu and Kim, Yeo-Jeong and Kim, Dong-Hyun and Park, Jun and Lee, Young-Jik and Kim, Sang-Hun},\n  journal={Applied Sciences},\n  volume={10},\n  number={19},\n  pages={6936},\n  year={2020},\n  publisher={Multidisciplinary Digital Publishing Institute}\n}","description":"KsponSpeech is a large-scale spontaneous speech corpus of Korean conversations. This corpus contains 969 hrs of general open-domain dialog utterances, spoken by about 2,000 native Korean speakers in a clean environment. All data were constructed by recording the dialogue of two people freely conversing on a variety of topics and manually transcribing the utterances. The transcription provides a dual transcription consisting of orthography and pronunciation, and disfluency tags for spontaneity of speech, such as filler words, repeated words, and word fragments. KsponSpeech is publicly available on an open data hub site of the Korea government. (https://aihub.or.kr/aidata/105)","key":""},{"id":"clarin-pl/cst-wikinews","private":false,"tags":[],"author":"clarin-pl","description":"CST Wikinews dataset.","key":""},{"id":"clarin-pl/kpwr-ner","private":false,"tags":[],"author":"clarin-pl","description":"KPWR-NER tagging dataset.","key":""},{"id":"clarin-pl/nkjp-pos","private":false,"tags":[],"author":"clarin-pl","description":"NKJP-POS tagging dataset.","key":""},{"id":"clarin-pl/polemo2-official","private":false,"tags":[],"author":"clarin-pl","citation":"@inproceedings{kocon-etal-2019-multi,\n    title = \"Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: Extended Corpus of Multi-Domain Consumer Reviews\",\n    author = \"Koco{\\'n}, Jan  and\n      Mi{\\l}kowski, Piotr  and\n      Za{\\'s}ko-Zieli{\\'n}ska, Monika\",\n    booktitle = \"Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)\",\n    month = nov,\n    year = \"2019\",\n    address = \"Hong Kong, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/K19-1092\",\n    doi = \"10.18653/v1/K19-1092\",\n    pages = \"980--991\",}","description":"PolEmo 2.0:  Corpus of Multi-Domain Consumer Reviews, evaluation data for article presented at CoNLL.","key":""},{"id":"classla/copa_hr","private":false,"tags":["languages:hr","licenses:cc-by-sa-4.0","task_categories:text-classification","task_ids:natural-language-inference","task_ids:textual-entailment","task_ids:causal-reasoning","task_ids:commonsense-reasoning"],"author":"classla","description":"The COPA-HR dataset (Choice of plausible alternatives in Croatian) is a translation \nof the English COPA dataset (https://people.ict.usc.edu/~gordon/copa.html) by following the \nXCOPA dataset translation methodology (https://arxiv.org/abs/2005.00333). The dataset consists of 1000 premises \n(My body cast a shadow over the grass), each given a question (What is the cause?), and two choices \n(The sun was rising; The grass was cut), with a label encoding which of the choices is more plausible \ngiven the annotator or translator (The sun was rising).\n\nThe dataset is split into 400 training samples, 100 validation samples, and 500 test samples. It includes the \nfollowing features: 'premise', 'choice1', 'choice2', 'label', 'question', 'changed' (boolean).","key":""},{"id":"classla/hr500k","private":false,"tags":["languages:hr","licenses:cc-by-sa-4.0","task_categories:structure-prediction","task_ids:tokenization","task_ids:normalization","task_ids:part-of-speech-tagging","task_ids:lemmatization","task_ids:named-entity-recognition"],"author":"classla","description":"The hr500k training corpus contains about 500,000 tokens manually annotated on the levels of \ntokenisation, sentence segmentation, morphosyntactic tagging, lemmatisation and named entities. \n\nOn the sentence level, the dataset contains 20159 training samples, 1963 validation samples and 2672 test samples \nacross the respective data splits. Each sample represents a sentence and includes the following features:\nsentence ID ('sent_id'), sentence text ('text'), list of tokens ('tokens'), list of lemmas ('lemmas'), \nlist of Multext-East tags ('xpos_tags), list of UPOS tags ('upos_tags'),\nlist of morphological features ('feats'), and list of IOB tags ('iob_tags'). The 'upos_tags' and 'iob_tags' features\nare encoded as class labels.","key":""},{"id":"classla/reldi_hr","private":false,"tags":["languages:hr","licenses:cc-by-sa-4.0","task_categories:structure-prediction","task_ids:tokenization","task_ids:normalization","task_ids:part-of-speech-tagging","task_ids:lemmatization","task_ids:named-entity-recognition"],"author":"classla","description":"The dataset contains 6339 training samples, 815 validation samples and 785 test samples. \nEach sample represents a sentence and includes the following features: sentence ID ('sent_id'), \nlist of tokens ('tokens'), list of lemmas ('lemmas'), list of UPOS tags ('upos_tags'), \nlist of Multext-East tags ('xpos_tags), list of morphological features ('feats'), \nand list of IOB tags ('iob_tags'), which are encoded as class labels.","key":""},{"id":"classla/reldi_sr","private":false,"tags":["languages:sr","licenses:cc-by-sa-4.0","task_categories:structure-prediction","task_ids:tokenization","task_ids:normalization","task_ids:part-of-speech-tagging","task_ids:lemmatization","task_ids:named-entity-recognition"],"author":"classla","description":"The dataset contains 5462 training samples, 711 validation samples and 725 test samples. \nEach sample represents a sentence and includes the following features: sentence ID ('sent_id'), \nlist of tokens ('tokens'), list of lemmas ('lemmas'), list of UPOS tags ('upos_tags'), \nlist of Multext-East tags ('xpos_tags), list of morphological features ('feats'), \nand list of IOB tags ('iob_tags'), which are encoded as class labels.","key":""},{"id":"classla/setimes_sr","private":false,"tags":["languages:sr","licenses:cc-by-sa-4.0","task_categories:structure-prediction","task_ids:tokenization","task_ids:normalization","task_ids:part-of-speech-tagging","task_ids:lemmatization","task_ids:named-entity-recognition"],"author":"classla","description":"SETimes_sr is a Serbian dataset annotated for morphosyntactic information and named entities.\n\nThe dataset contains 3177 training samples, 395 validation samples and 319 test samples \nacross the respective data splits. Each sample represents a sentence and includes the following features:\nsentence ID ('sent_id'), sentence text ('text'), list of tokens ('tokens'), list of lemmas ('lemmas'), \nlist of Multext-East tags ('xpos_tags), list of UPOS tags ('upos_tags'),\nlist of morphological features ('feats'), and list of IOB tags ('iob_tags'). The 'upos_tags' and 'iob_tags' features\nare encoded as class labels.","key":""},{"id":"cnrcastroli/aaaa","private":false,"tags":[],"author":"cnrcastroli","key":""},{"id":"cointegrated/ParaNMT-Ru-Leipzig","private":false,"tags":[],"author":"cointegrated","key":""},{"id":"congpt/dstc23_asr","private":false,"tags":[],"author":"congpt","key":""},{"id":"corypaik/prost","private":false,"tags":["annotations_creators:expert-generated","extended:original","language_creators:expert-generated","languages:en-US","licenses:apache-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:multiple-choice-qa","task_ids:open-domain-qa"],"author":"corypaik","citation":"\\","description":"*Physical Reasoning about Objects Through Space and Time* (PROST) is a probing dataset to evaluate the ability of pretrained LMs to understand and reason about the physical world. PROST consists of 18,736 cloze-style multiple choice questions from 14 manually curated templates, covering 10 physical reasoning concepts:  direction, mass, height, circumference, stackable, rollable, graspable, breakable, slideable, and bounceable.","paperswithcode_id":"piqa","key":""},{"id":"ctl/ConceptualCaptions","private":false,"tags":[],"author":"ctl","description":"\"\"\"\n\n_CITATION =","key":""},{"id":"dasago78/dasago78dataset","private":false,"tags":[],"author":"dasago78","key":""},{"id":"dataset/wikipedia_bn","private":false,"tags":[],"author":"dataset","citation":"@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}","description":"Bengali Wikipedia from the dump of 03/20/2021.\nThe data was processed using the huggingface datasets wikipedia script early april 2021.\nThe dataset was built from the Wikipedia dump (https://dumps.wikimedia.org/).\nEach example contains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).","key":""},{"id":"david-wb/zeshel","private":false,"tags":[],"author":"david-wb","key":""},{"id":"deepset/germandpr","private":false,"tags":["thumbnail:https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg","languages:de","multilinguality:monolingual","source_datasets:original","task_categories:question-answering","task_categories:text-retrieval","task_ids:extractive-qa","task_ids:closed-domain-qa"],"author":"deepset","citation":"@misc{möller2021germanquad,\n      title={GermanQuAD and GermanDPR: Improving Non-English Question Answering and Passage Retrieval}, \n      author={Timo Möller and Julian Risch and Malte Pietsch},\n      year={2021},\n      eprint={2104.12741},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"We take GermanQuAD as a starting point and add hard negatives from a dump of the full German Wikipedia following the approach of the DPR authors (Karpukhin et al., 2020). The format of the dataset also resembles the one of DPR. GermanDPR comprises 9275 question/answer pairs in the training set and 1025 pairs in the test set. For each pair, there are one positive context and three hard negative contexts.","key":""},{"id":"deepset/germanquad","private":false,"tags":["thumbnail:https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg","languages:de","multilinguality:monolingual","source_datasets:original","task_categories:question-answering","task_categories:text-retrieval","task_ids:extractive-qa","task_ids:closed-domain-qa","task_ids:open-domain-qa"],"author":"deepset","citation":"@misc{möller2021germanquad,\n      title={GermanQuAD and GermanDPR: Improving Non-English Question Answering and Passage Retrieval}, \n      author={Timo Möller and Julian Risch and Malte Pietsch},\n      year={2021},\n      eprint={2104.12741},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"In order to raise the bar for non-English QA, we are releasing a high-quality, human-labeled German QA dataset consisting of 13 722 questions, incl. a three-way annotated test set.\nThe creation of GermanQuAD is inspired by insights from existing datasets as well as our labeling experience from several industry projects. We combine the strengths of SQuAD, such as high out-of-domain performance, with self-sufficient questions that contain all relevant information for open-domain QA as in the NaturalQuestions dataset. Our training and test datasets do not overlap like other popular datasets and include complex questions that cannot be answered with a single entity or only a few words.","key":""},{"id":"dfgvhxfgv/fghghj","private":false,"tags":[],"author":"dfgvhxfgv","key":""},{"id":"dfki-nlp/few-nerd","private":false,"tags":[],"author":"dfki-nlp","citation":"@inproceedings{ding2021few,\ntitle={Few-NERD: A Few-Shot Named Entity Recognition Dataset},\nauthor={Ding, Ning and Xu, Guangwei and Chen, Yulin, and Wang, Xiaobin and Han, Xu and Xie, \nPengjun and Zheng, Hai-Tao and Liu, Zhiyuan},\nbooktitle={ACL-IJCNLP},\nyear={2021}\n}","description":"Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, \nwhich contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities \nand 4,601,223 tokens. Three benchmark tasks are built, one is supervised: Few-NERD (SUP) and the \nother two are few-shot: Few-NERD (INTRA) and Few-NERD (INTER).","key":""},{"id":"dgknrsln/Yorumsepeti","private":false,"tags":[],"author":"dgknrsln","key":""},{"id":"dispenst/jhghdghfd","private":false,"tags":[],"author":"dispenst","key":""},{"id":"dispix/test-dataset","private":false,"tags":[],"author":"dispix","key":""},{"id":"dk-crazydiv/huggingface-modelhub","private":false,"tags":[],"author":"dk-crazydiv","citation":"\\","description":"Metadata information of all the models available on HuggingFace's modelhub","key":""},{"id":"dynabench/dynasent","private":false,"tags":[],"author":"dynabench","description":"    Dynabench.DynaSent is a Sentiment Analysis dataset collected using a \n    human-and-model-in-the-loop.","key":""},{"id":"dynabench/qa","private":false,"tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-sa-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:question-answering","task_ids:extractive-qa","task_ids:open-domain-qa"],"author":"dynabench","description":"    Dynabench.QA is a Reading Comprehension dataset collected using a human-and-model-in-the-loop.","key":""},{"id":"eason929/test","private":false,"tags":[],"author":"eason929","key":""},{"id":"edfews/szdfcszdf","private":false,"tags":[],"author":"edfews","key":""},{"id":"edsas/fgrdtgrdtdr","private":false,"tags":[],"author":"edsas","key":""},{"id":"edsas/grttyi","private":false,"tags":[],"author":"edsas","key":""},{"id":"ervis/aaa","private":false,"tags":[],"author":"ervis","key":""},{"id":"ervis/qqq","private":false,"tags":[],"author":"ervis","key":""},{"id":"fatvvs/autonlp-data-entity_model_conll2003","private":false,"tags":[],"author":"fatvvs","key":""},{"id":"flax-community/dummy-oscar-als-32","private":false,"tags":[],"author":"flax-community","key":""},{"id":"flax-community/german-common-voice-processed","private":false,"tags":[],"author":"flax-community","key":""},{"id":"flax-community/german_common_crawl","private":false,"tags":[],"author":"flax-community","citation":"@inproceedings{wenzek2020ccnet,\n  title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data},\n  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\\'a}n, Francisco and Joulin, Armand and Grave, {\\'E}douard},\n  booktitle={Proceedings of The 12th Language Resources and Evaluation Conference},\n  pages={4003--4012},\n  year={2020}\n}","description":"German Only Extract from Common Crawl\n\nThis Dataset is for pretraining a German Language Model (Unsupervised) or tune a Multilingual Model specifically to German","key":""},{"id":"flax-community/norwegian-clean-dummy","private":false,"tags":[],"author":"flax-community","key":""},{"id":"flax-community/swahili-safi","private":false,"tags":[],"author":"flax-community","citation":"@InProceedings{huggingface:flax-community,\ntitle = Cleaned dataset for Swahili Language Modeling,\nauthors={Fitsum, Alok, Patrick},\nyear={2021},\nlink = https://huggingface.co/datasets/flax-community/swahili-safi\n}","description":"Cleaned dataset for Swahili Language Modeling","key":""},{"id":"flax-sentence-embeddings/paws-jsonl","private":false,"tags":[],"author":"flax-sentence-embeddings","key":""},{"id":"flax-sentence-embeddings/stackexchange_math_jsonl","private":false,"tags":[],"author":"flax-sentence-embeddings","key":""},{"id":"flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl","private":false,"tags":[],"author":"flax-sentence-embeddings","key":""},{"id":"flax-sentence-embeddings/stackexchange_title_body_jsonl","private":false,"tags":[],"author":"flax-sentence-embeddings","key":""},{"id":"flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl","private":false,"tags":[],"author":"flax-sentence-embeddings","key":""},{"id":"flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl","private":false,"tags":[],"author":"flax-sentence-embeddings","key":""},{"id":"flax-sentence-embeddings/stackexchange_xml","private":false,"tags":[],"author":"flax-sentence-embeddings","key":""},{"id":"formermagic/github_python_1m","private":false,"tags":["annotations_creators:found","language_creators:found","languages:py","licenses:mit","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:sequence-modeling","task_categories:conditional-text-generation","task_ids:language-modeling","task_ids:slot-filling","task_ids:code-generation"],"author":"formermagic","key":""},{"id":"formu/CVT","private":false,"tags":[],"author":"formu","key":""},{"id":"fulai/DuReader","private":false,"tags":[],"author":"fulai","key":""},{"id":"fuliucansheng/data_for_test","private":false,"tags":[],"author":"fuliucansheng","citation":"DataTest","description":"DataTest","key":""},{"id":"fuliucansheng/mininlp","private":false,"tags":[],"author":"fuliucansheng","citation":"MiniNLP Data","description":"MiniNLP Data","key":""},{"id":"fuliucansheng/pascal_voc","private":false,"tags":[],"author":"fuliucansheng","citation":"PASCAL_VOC","description":"PASCAL_VOC","key":""},{"id":"fvillena/cantemist","private":false,"tags":[],"author":"fvillena","citation":"\\","description":"\\","key":""},{"id":"fvillena/spanish_diagnostics","private":false,"tags":[],"author":"fvillena","key":""},{"id":"german-nlp-group/german_common_crawl","private":false,"tags":[],"author":"german-nlp-group","citation":"@inproceedings{wenzek2020ccnet,\n  title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data},\n  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\\'a}n, Francisco and Joulin, Armand and Grave, {\\'E}douard},\n  booktitle={Proceedings of The 12th Language Resources and Evaluation Conference},\n  pages={4003--4012},\n  year={2020}\n}","description":"German Only Extract from Common Crawl \n\nThis Dataset is for pretraining a German Language Model (Unsupervised) or tune a Multilingual Model specifically to German","key":""},{"id":"gmnlp/TICO19","private":false,"tags":[],"author":"gmnlp","key":""},{"id":"godzillavskongonlinetv/ergfdg","private":false,"tags":[],"author":"godzillavskongonlinetv","key":""},{"id":"godzillavskongonlinetv/godzillavskongfullmovie","private":false,"tags":[],"author":"godzillavskongonlinetv","key":""},{"id":"gpt3mix/rt20","private":false,"tags":[],"author":"gpt3mix","key":""},{"id":"gpt3mix/sst2","private":false,"tags":[],"author":"gpt3mix","key":""},{"id":"gustavecortal/fr_covid_news","private":false,"tags":[],"author":"gustavecortal","key":""},{"id":"hartzeer/kdfjdshfje","private":false,"tags":[],"author":"hartzeer","key":""},{"id":"hfface/poopi","private":false,"tags":[],"author":"hfface","key":""},{"id":"howardmiddleton382/esuyertusutr","private":false,"tags":[],"author":"howardmiddleton382","key":""},{"id":"howardmiddleton382/wgweagwege","private":false,"tags":[],"author":"howardmiddleton382","key":""},{"id":"huggingFaceUser02/air21_grp13_inference_results","private":false,"tags":[],"author":"huggingFaceUser02","key":""},{"id":"huggingFaceUser02/air21_grp13_tokenized_results","private":false,"tags":[],"author":"huggingFaceUser02","key":""},{"id":"huggingface/superb-data","private":false,"tags":[],"author":"huggingface","key":""},{"id":"huseinzol05/translated-The-Pile","private":false,"tags":[],"author":"huseinzol05","key":""},{"id":"iamshsdf/sssssssssss","private":false,"tags":[],"author":"iamshsdf","key":""},{"id":"iarfmoose/question_generator","private":false,"tags":[],"author":"iarfmoose","key":""},{"id":"imthanhlv/binhvq_news21_raw","private":false,"tags":[],"author":"imthanhlv","key":""},{"id":"jaimin/wav2vec2-large-xlsr-gujarati-demo","private":false,"tags":[],"author":"jaimin","key":""},{"id":"jdepoix/junit_test_completion","private":false,"tags":[],"author":"jdepoix","key":""},{"id":"jglaser/binding_affinity","private":false,"tags":[],"author":"jglaser","citation":"@InProceedings{huggingface:dataset,\ntitle = {jglaser/binding_affinity},\nauthor={Jens Glaser, ORNL\n},\nyear={2021}\n}","description":"A dataset to refine language models on protein-ligand binding affinity prediction.","key":""},{"id":"jimregan/clarinpl_sejmsenat","private":false,"tags":["annotations_creators:expert-generated","languages:pl","licenses:other","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:other","task_ids:other-other-automatic speech recognition"],"author":"jimregan","citation":"@article{marasek2014system,\n  title={System for automatic transcription of sessions of the {P}olish {S}enate},\n  author={Marasek, Krzysztof and Kor{\\v{z}}inek, Danijel and Brocki, {\\L}ukasz},\n  journal={Archives of Acoustics},\n  volume={39},\n  number={4},\n  pages={501--509},\n  year={2014}\n}","description":"A collection of 97 hours of parliamentary speeches published on the ClarinPL website\n\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .wav format and is not converted to a float32 array. To convert the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```","key":""},{"id":"jimregan/clarinpl_studio","private":false,"tags":["annotations_creators:expert-generated","languages:pl","licenses:other","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:other","task_ids:other-other-automatic speech recognition"],"author":"jimregan","citation":"@article{korvzinek2017polish,\n  title={Polish read speech corpus for speech tools and services},\n  author={Kor{\\v{z}}inek, Danijel and Marasek, Krzysztof and Brocki, {\\L}ukasz and Wo{\\l}k, Krzysztof},\n  journal={arXiv preprint arXiv:1706.00245},\n  year={2017}\n}","description":"The corpus consists of 317 speakers recorded in 554\nsessions, where each session consists of 20 read sentences and 10 phonetically rich words. The size of\nthe audio portion of the corpus amounts to around 56 hours, with transcriptions containing 356674 words\nfrom a vocabulary of size 46361.\n\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .wav format and is not converted to a float32 array. To convert the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```","key":""},{"id":"jiyoojeong/targetizer","private":false,"tags":[],"author":"jiyoojeong","key":""},{"id":"jmamou/augmented-glue-sst2","private":false,"tags":["annotations_creators:machine-generated","extended:original","language_creators:machine-generated","languages:en-US","licenses:unknown","multilinguality:monolingual","size_categories:100K<n<1M","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"author":"jmamou","key":""},{"id":"joelito/ler","private":false,"tags":[],"author":"joelito","citation":"@inproceedings{leitner2019fine,\n  author = {Elena Leitner and Georg Rehm and Julian Moreno-Schneider},\n  title = {{Fine-grained Named Entity Recognition in Legal Documents}},\n  booktitle = {Semantic Systems. The Power of AI and Knowledge\n                  Graphs. Proceedings of the 15th International Conference\n                  (SEMANTiCS 2019)},\n  year = 2019,\n  editor = {Maribel Acosta and Philippe Cudré-Mauroux and Maria\n                  Maleshkova and Tassilo Pellegrini and Harald Sack and York\n                  Sure-Vetter},\n  keywords = {aip},\n  publisher = {Springer},\n  series = {Lecture Notes in Computer Science},\n  number = {11702},\n  address = {Karlsruhe, Germany},\n  month = 9,\n  note = {10/11 September 2019},\n  pages = {272--287},\n  pdf = {https://link.springer.com/content/pdf/10.1007%2F978-3-030-33220-4_20.pdf}\n}","description":"We describe a dataset developed for Named Entity Recognition in German federal court decisions. \nIt consists of approx. 67,000 sentences with over 2 million tokens. \nThe resource contains 54,000 manually annotated entities, mapped to 19 fine-grained semantic classes: \nperson, judge, lawyer, country, city, street, landscape, organization, company, institution, court, brand, law, \nordinance, European legal norm, regulation, contract, court decision, and legal literature. \nThe legal documents were, furthermore, automatically annotated with more than 35,000 TimeML-based time expressions. \nThe dataset, which is available under a CC-BY 4.0 license in the CoNNL-2002 format, \nwas developed for training an NER service for German legal documents in the EU project Lynx.","key":""},{"id":"joelito/sem_eval_2010_task_8","private":false,"tags":[],"author":"joelito","citation":"@inproceedings{hendrickx-etal-2010-semeval,\n    title = \"{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals\",\n    author = \"Hendrickx, Iris  and\n      Kim, Su Nam  and\n      Kozareva, Zornitsa  and\n      Nakov, Preslav  and\n      {\\'O} S{\\'e}aghdha, Diarmuid  and\n      Pad{\\'o}, Sebastian  and\n      Pennacchiotti, Marco  and\n      Romano, Lorenza  and\n      Szpakowicz, Stan\",\n    booktitle = \"Proceedings of the 5th International Workshop on Semantic Evaluation\",\n    month = jul,\n    year = \"2010\",\n    address = \"Uppsala, Sweden\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/S10-1006\",\n    pages = \"33--38\",\n}","description":"The SemEval-2010 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals.\nThe task was designed to compare different approaches to semantic relation classification\nand to provide a standard testbed for future research.","key":""},{"id":"julien-c/dummy-dataset-from-colab","private":false,"tags":[],"author":"julien-c","key":""},{"id":"julien-c/reactiongif","private":false,"tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"author":"julien-c","paperswithcode_id":"reactiongif","key":""},{"id":"k-halid/ar","private":false,"tags":[],"author":"k-halid","citation":"@inproceedings{eisele2010multiun,\n  title={MultiUN: A Multilingual Corpus from United Nation Documents.},\n  author={Eisele, Andreas and Chen, Yu},\n  booktitle={LREC},\n  year={2010}\n}","description":"The corpus is a part of the MultiUN corpus.It is a collection of translated documents from the United Nations.The corpus is download from the following website : [open parallel corpus](http://opus.datasetsl.eu/)  \\","key":""},{"id":"karinev/lanuitdudroit","private":false,"tags":[],"author":"karinev","key":""},{"id":"katoensp/VR-OP","private":false,"tags":[],"author":"katoensp","key":""},{"id":"keshan/clean-si-mc4","private":false,"tags":[],"author":"keshan","citation":"@article{2019t5,\n    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n    journal = {arXiv e-prints},\n    year = {2019},\n    archivePrefix = {arXiv},\n    eprint = {1910.10683},\n}","description":"A colossal, cleaned version of Common Crawl's web crawl corpus.\nBased on Common Crawl dataset: \"https://commoncrawl.org\".\nThis is the processed version of Google's mC4 dataset by AllenAI.","key":""},{"id":"keshan/large-sinhala-asr-dataset","private":false,"tags":[],"author":"keshan","citation":" @inproceedings{kjartansson-etal-sltu2018,\n    title = {{Crowd-Sourced Speech Corpora for Javanese, Sundanese,  Sinhala, Nepali, and Bangladeshi Bengali}},\n    author = {Oddur Kjartansson and Supheakmungkol Sarin and Knot Pipatsrisawat and Martin Jansche and Linne Ha},\n    booktitle = {Proc. The 6th Intl. Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU)},\n    year  = {2018},\n    address = {Gurugram, India},\n    month = aug,\n    pages = {52--55},\n    URL   = {http://dx.doi.org/10.21437/SLTU.2018-11}\n  }","description":"This data set contains ~185K transcribed audio data for Sinhala. The data set consists of wave files, and a TSV file. The file utt_spk_text.tsv contains a FileID, anonymized UserID and the transcription of audio in the file.\nThe data set has been manually quality checked, but there might still be errors.\n\nSee LICENSE.txt file for license information.\n\nCopyright 2016, 2017, 2018 Google, Inc.","key":""},{"id":"kmyoo/klue-tc-dev","private":false,"tags":[],"author":"kmyoo","key":""},{"id":"lavis-nlp/german_legal_sentences","private":false,"tags":["annotations_creators:machine-generated","language_creators:found","languages:de","licenses:unknown","multilinguality:monolingual","size_categories:n>1M","source_datasets:original","task_categories:text-retrieval","task_categories:text-scoring","task_ids:semantic-similarity-scoring","task_ids:text-retrieval-other-example-based-retrieval"],"author":"lavis-nlp","citation":"coming soon","description":"German Legal Sentences (GLS) is an automatically generated training dataset for semantic sentence \nmatching in the domain in german legal documents. It follows the concept of weak supervision, where \nimperfect labels are generated using multiple heuristics. For this purpose we use a combination of \nlegal citation matching and BM25 similarity. The contained sentences and their citations are parsed \nfrom real judicial decisions provided by [Open Legal Data](http://openlegaldata.io/)","key":""},{"id":"lewtun/asr_dummy","private":false,"tags":[],"author":"lewtun","citation":"@article{DBLP:journals/corr/abs-2105-01051,\n  author    = {Shu{-}Wen Yang and\n               Po{-}Han Chi and\n               Yung{-}Sung Chuang and\n               Cheng{-}I Jeff Lai and\n               Kushal Lakhotia and\n               Yist Y. Lin and\n               Andy T. Liu and\n               Jiatong Shi and\n               Xuankai Chang and\n               Guan{-}Ting Lin and\n               Tzu{-}Hsien Huang and\n               Wei{-}Cheng Tseng and\n               Ko{-}tik Lee and\n               Da{-}Rong Liu and\n               Zili Huang and\n               Shuyan Dong and\n               Shang{-}Wen Li and\n               Shinji Watanabe and\n               Abdelrahman Mohamed and\n               Hung{-}yi Lee},\n  title     = {{SUPERB:} Speech processing Universal PERformance Benchmark},\n  journal   = {CoRR},\n  volume    = {abs/2105.01051},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.01051},\n  archivePrefix = {arXiv},\n  eprint    = {2105.01051},\n  timestamp = {Thu, 01 Jul 2021 13:30:22 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-01051.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"Self-supervised learning (SSL) has proven vital for advancing research in\nnatural language processing (NLP) and computer vision (CV). The paradigm\npretrains a shared model on large volumes of unlabeled data and achieves\nstate-of-the-art (SOTA) for various tasks with minimal adaptation. However, the\nspeech processing community lacks a similar setup to systematically explore the\nparadigm. To bridge this gap, we introduce Speech processing Universal\nPERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the\nperformance of a shared model across a wide range of speech processing tasks\nwith minimal architecture changes and labeled data. Among multiple usages of the\nshared model, we especially focus on extracting the representation learned from\nSSL due to its preferable re-usability. We present a simple framework to solve\nSUPERB tasks by learning task-specialized lightweight prediction heads on top of\nthe frozen shared model. Our results demonstrate that the framework is promising\nas SSL representations show competitive generalizability and accessibility\nacross SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a\nbenchmark toolkit to fuel the research in representation learning and general\nspeech processing.\n\nNote that in order to limit the required storage for preparing this dataset, the\naudio is stored in the .flac format and is not converted to a float32 array. To\nconvert, the audio file to a float32 array, please make use of the `.map()`\nfunction as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```","key":""},{"id":"lewtun/binary_classification_dummy","private":false,"tags":[],"author":"lewtun","key":""},{"id":"lewtun/mnist-preds","private":false,"tags":["benchmark:raft"],"author":"lewtun","citation":"@InProceedings{huggingface:dataset,\ntitle = {A great new dataset},\nauthor={huggingface, Inc.\n},\nyear={2020}\n}","description":"This new dataset is designed to solve this great NLP task and is crafted with a lot of care.","key":""},{"id":"lewtun/text_classification_dummy","private":false,"tags":[],"author":"lewtun","key":""},{"id":"lhoestq/custom_squad","private":false,"tags":["annotations_creators:crowdsourced","language_creators:crowdsourced","language_creators:found","languages:en","licenses:cc-by-4.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|wikipedia","task_categories:question-answering","task_ids:extractive-qa"],"author":"lhoestq","citation":"@article{2016arXiv160605250R,\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                 Konstantin and {Liang}, Percy},\n        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n      journal = {arXiv e-prints},\n         year = 2016,\n          eid = {arXiv:1606.05250},\n        pages = {arXiv:1606.05250},\narchivePrefix = {arXiv},\n       eprint = {1606.05250},\n}","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.","key":""},{"id":"lhoestq/squad","private":false,"tags":[],"author":"lhoestq","citation":"@article{2016arXiv160605250R,\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                 Konstantin and {Liang}, Percy},\n        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n      journal = {arXiv e-prints},\n         year = 2016,\n          eid = {arXiv:1606.05250},\n        pages = {arXiv:1606.05250},\narchivePrefix = {arXiv},\n       eprint = {1606.05250},\n}","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.","key":""},{"id":"lhoestq/test","private":false,"tags":["type:test","annotations_creators:expert-generated","language_creators:found","languages:en","licenses:mit","multilinguality:monolingual","size_categories:n<1K","source_datasets:original","task_categories:other-test","task_ids:other-test","pretty_name:Test Dataset"],"author":"lhoestq","citation":"\\","description":"This is a test dataset.","key":""},{"id":"lhoestq/wikipedia_bn","private":false,"tags":[],"author":"lhoestq","citation":"@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}","description":"Bengali Wikipedia from the dump of 03/20/2021.\nThe data was processed using the huggingface datasets wikipedia script early april 2021.\nThe dataset was built from the Wikipedia dump (https://dumps.wikimedia.org/).\nEach example contains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).","key":""},{"id":"lkiouiou/o9ui7877687","private":false,"tags":[],"author":"lkiouiou","key":""},{"id":"lohanna/testedjkcxkf","private":false,"tags":[],"author":"lohanna","key":""},{"id":"lucien/sciencemission","private":false,"tags":[],"author":"lucien","key":""},{"id":"lucien/voacantonesed","private":false,"tags":[],"author":"lucien","key":""},{"id":"lucien/wsaderfffjjjhhh","private":false,"tags":[],"author":"lucien","key":""},{"id":"lucio/common_voice_eval","private":false,"tags":[],"author":"lucio","key":""},{"id":"m3hrdadfi/recipe_nlg_lite","private":false,"tags":[],"author":"m3hrdadfi","citation":"@misc{RecipeNLGLite, \n  author          = {Mehrdad Farahani},\n  title           = {RecipeNLG: A Cooking Recipes Dataset for Semi-Structured Text Generation (Lite)},\n  year            = 2021,\n  publisher       = {GitHub},\n  journal         = {GitHub repository},\n  howpublished    = {url{https://github.com/m3hrdadfi/recipe-nlg-lite}},\n}","description":"RecipeNLG: A Cooking Recipes Dataset for Semi-Structured Text Generation - Lite version\nThe dataset we publish contains 7,198 cooking recipes (>7K). \nIt's processed in more careful way and provides more samples than any other dataset in the area.","key":""},{"id":"mad/IndonesiaNewsDataset","private":false,"tags":[],"author":"mad","key":""},{"id":"majod/CleanNaturalQuestionsDataset","private":false,"tags":[],"author":"majod","key":""},{"id":"makanan/umich","private":false,"tags":[],"author":"makanan","key":""},{"id":"medzaf/test","private":false,"tags":[],"author":"medzaf","key":""},{"id":"metalearning/kaggale-nlp-tutorial","private":false,"tags":[],"author":"metalearning","key":""},{"id":"mksaad/Arabic_news","private":false,"tags":[],"author":"mksaad","key":""},{"id":"mldmm/glass_alloy_composition","private":false,"tags":[],"author":"mldmm","description":"This is an alloy composition dataset","key":""},{"id":"mmm-da/rutracker_anime_torrent_titles","private":false,"tags":[],"author":"mmm-da","key":""},{"id":"mrojas/abbreviation","private":false,"tags":[],"author":"mrojas","citation":"\\","description":"\\","key":""},{"id":"mrojas/body","private":false,"tags":[],"author":"mrojas","citation":"\\","description":"\\","key":""},{"id":"mrojas/disease","private":false,"tags":[],"author":"mrojas","citation":"\\","description":"\\","key":""},{"id":"mrojas/family","private":false,"tags":[],"author":"mrojas","citation":"\\","description":"\\","key":""},{"id":"mrojas/finding","private":false,"tags":[],"author":"mrojas","citation":"\\","description":"\\","key":""},{"id":"mrojas/medication","private":false,"tags":[],"author":"mrojas","citation":"\\","description":"\\","key":""},{"id":"mrojas/procedure","private":false,"tags":[],"author":"mrojas","citation":"\\","description":"\\","key":""},{"id":"mulcyber/europarl-mono","private":false,"tags":[],"author":"mulcyber","citation":"@inproceedings{koehn2005europarl,\n  title={Europarl: A parallel corpus for statistical machine translation},\n  author={Koehn, Philipp},\n  booktitle={MT summit},\n  volume={5},\n  pages={79--86},\n  year={2005},\n  organization={Citeseer}\n}","description":"Europarl Monolingual Dataset.\n\nThe Europarl parallel corpus is extracted from the proceedings of the\nEuropean Parliament (from 2000 to 2011). It includes versions in 21 European\nlanguages: Romanic (French, Italian, Spanish, Portuguese, Romanian),\nGermanic (English, Dutch, German, Danish, Swedish), Slavik (Bulgarian,\nCzech, Polish, Slovak, Slovene), Finni-Ugric (Finnish, Hungarian, Estonian),\nBaltic (Latvian, Lithuanian), and Greek.\n\nUpstream url: https://www.statmt.org/europarl/","key":""},{"id":"mustafa12/db_ee","private":false,"tags":[],"author":"mustafa12","key":""},{"id":"mustafa12/edaaaas","private":false,"tags":[],"author":"mustafa12","key":""},{"id":"mustafa12/thors","private":false,"tags":[],"author":"mustafa12","key":""},{"id":"nateraw/auto-cats-and-dogs","private":false,"tags":["task_categories:other","task_ids:other-image-classification","task_ids:image-classification","tags:auto-generated","tags:image-classification"],"author":"nateraw","key":""},{"id":"nateraw/auto-exp-2","private":false,"tags":["task_categories:other","task_ids:other-image-classification","task_ids:image-classification","tags:auto-generated","tags:image-classification"],"author":"nateraw","key":""},{"id":"nateraw/beans","private":false,"tags":[],"author":"nateraw","citation":"@ONLINE {beansdata,\n    author=\"Makerere AI Lab\",\n    title=\"Bean disease dataset\",\n    month=\"January\",\n    year=\"2020\",\n    url=\"https://github.com/AI-Lab-Makerere/ibean/\"\n}","description":"Beans is a dataset of images of beans taken in the field using smartphone\ncameras. It consists of 3 classes: 2 disease classes and the healthy class.\nDiseases depicted include Angular Leaf Spot and Bean Rust. Data was annotated\nby experts from the National Crops Resources Research Institute (NaCRRI) in\nUganda and collected by the Makerere AI research lab.","key":""},{"id":"nateraw/cats-and-dogs","private":false,"tags":[],"author":"nateraw","citation":"@Inproceedings (Conference){asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization,\n    author = {Elson, Jeremy and Douceur, John (JD) and Howell, Jon and Saul, Jared},\n    title = {Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization},\n    booktitle = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},\n    year = {2007},\n    month = {October},\n    publisher = {Association for Computing Machinery, Inc.},\n    url = {https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/},\n    edition = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},\n}","key":""},{"id":"nateraw/fairface","private":false,"tags":["licenses:cc-by-4.0","size_categories:10K<n<100K","task_categories:other","task_ids:other-image-classification","task_ids:image-classification"],"author":"nateraw","paperswithcode_id":"fairface","key":""},{"id":"nateraw/image-folder","private":false,"tags":[],"author":"nateraw","key":""},{"id":"nateraw/rock_paper_scissors","private":false,"tags":[],"author":"nateraw","citation":"@ONLINE {rps,\nauthor = \"Laurence Moroney\",\ntitle = \"Rock, Paper, Scissors Dataset\",\nmonth = \"feb\",\nyear = \"2019\",\nurl = \"http://laurencemoroney.com/rock-paper-scissors-dataset\"\n}","key":""},{"id":"nateraw/test","private":false,"tags":[],"author":"nateraw","key":""},{"id":"naver-clova-conversation/klue-tc-dev-tsv","private":false,"tags":[],"author":"naver-clova-conversation","key":""},{"id":"naver-clova-conversation/klue-tc-tsv","private":false,"tags":[],"author":"naver-clova-conversation","key":""},{"id":"naver-clova-conversation-ul/klue-tc-dev","private":false,"tags":[],"author":"naver-clova-conversation-ul","key":""},{"id":"nbroad/few-nerd","private":false,"tags":[],"author":"nbroad","citation":"@inproceedings{ding2021few,\n  title={Few-NERD:A Few-shot Named Entity Recognition Dataset},\n  author={Ding, Ning and Xu, Guangwei and Chen, Yulin, and Wang, Xiaobin and Han, Xu and Xie, Pengjun and Zheng, Hai-Tao and Liu, Zhiyuan},\n  booktitle={ACL-IJCNLP},\n  year={2021}\n}","description":"Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities and 4,601,223 tokens. Three benchmark tasks are built, one is supervised (Few-NERD (SUP)) and the other two are few-shot (Few-NERD (INTRA) and Few-NERD (INTER)). Few-NERD is collected by researchers from Tsinghua University and DAMO Academy,\\ \nAlibaba Group .","key":""},{"id":"neelalex/raft-predictions","private":false,"tags":[],"author":"neelalex","citation":"@InProceedings{huggingface:dataset,\ntitle = {A great new dataset},\nauthor={huggingface, Inc.\n},\nyear={2020}\n}","description":"This dataset contains a corpus of AI papers. The first task is to determine whether or not a datapoint is an AI safety paper. The second task is to determine what type of paper it is.","key":""},{"id":"nielsr/FUNSD_layoutlmv2","private":false,"tags":["languages:en"],"author":"nielsr","citation":"@article{Jaume2019FUNSDAD,\n  title={FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents},\n  author={Guillaume Jaume and H. K. Ekenel and J. Thiran},\n  journal={2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)},\n  year={2019},\n  volume={2},\n  pages={1-6}\n}","description":"https://guillaumejaume.github.io/FUNSD/","paperswithcode_id":"funsd","key":""},{"id":"nielsr/XFUN","private":false,"tags":[],"author":"nielsr","key":""},{"id":"nucklehead/ht-voice-dataset","private":false,"tags":[],"author":"nucklehead","key":""},{"id":"oelkrise/CRT","private":false,"tags":[],"author":"oelkrise","key":""},{"id":"osanseviero/llama_test","private":false,"tags":[],"author":"osanseviero","key":""},{"id":"osanseviero/test","private":false,"tags":[],"author":"osanseviero","key":""},{"id":"ought/raft","private":false,"tags":[],"author":"ought","citation":"@InProceedings{huggingface:dataset,\ntitle = {A great new dataset},\nauthor={huggingface, Inc.\n},\nyear={2020}\n}","description":"This dataset contains a corpus of AI papers. The first task is to determine whether or not a datapoint is an AI safety paper. The second task is to determine what type of paper it is.","key":""},{"id":"parivartanayurveda/Malesexproblemsayurvedictreatment","private":false,"tags":[],"author":"parivartanayurveda","key":""},{"id":"pasinit/xlwic","private":false,"tags":["annotations_creators:expert-generated","extended:original","language_creators:found","languages:en","languages:bg","languages:zh","languages:hr","languages:da","languages:nl","languages:et","languages:fa","languages:ja","languages:ko","languages:it","languages:fr","languages:de","licenses:cc-by-nc-4.0","multilinguality:multilingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:semantic-similarity-classification"],"author":"pasinit","citation":"@inproceedings{raganato-etal-2020-xl-wic,\n  title={XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization},\n  author={Raganato, Alessandro and Pasini, Tommaso and Camacho-Collados, Jose and Pilehvar, Mohammad Taher},\n  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  pages={7193--7206},\n  year={2020}\n}","description":"A system's task on any of the XL-WiC datasets is to identify the intended meaning of a word in a context of a given language. XL-WiC is framed as a binary classification task. Each instance in XL-WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not.\n\nXL-WiC provides dev and test sets in the following 12 languages:\n\nBulgarian (BG)\nDanish (DA)\nGerman (DE)\nEstonian (ET)\nFarsi (FA)\nFrench (FR)\nCroatian (HR)\nItalian (IT)\nJapanese (JA)\nKorean (KO)\nDutch (NL)\nChinese (ZH)\nand training sets in the following 3 languages:\n\nGerman (DE)\nFrench (FR)\nItalian (IT)","key":""},{"id":"patrickvonplaten/librispeech_asr_dummy","private":false,"tags":[],"author":"patrickvonplaten","citation":"@inproceedings{panayotov2015librispeech,\n  title={Librispeech: an ASR corpus based on public domain audio books},\n  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},\n  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},\n  pages={5206--5210},\n  year={2015},\n  organization={IEEE}\n}","description":"LibriSpeech is a corpus of approximately 1000 hours of read English speech with sampling rate of 16 kHz,\nprepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read\naudiobooks from the LibriVox project, and has been carefully segmented and aligned.\n\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .flac format and is not converted to a float32 array. To convert, the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```","key":""},{"id":"patrickvonplaten/scientific_papers_dummy","private":false,"tags":[],"author":"patrickvonplaten","citation":"@article{Cohan_2018,\n   title={A Discourse-Aware Attention Model for Abstractive Summarization of\n            Long Documents},\n   url={http://dx.doi.org/10.18653/v1/n18-2097},\n   DOI={10.18653/v1/n18-2097},\n   journal={Proceedings of the 2018 Conference of the North American Chapter of\n          the Association for Computational Linguistics: Human Language\n          Technologies, Volume 2 (Short Papers)},\n   publisher={Association for Computational Linguistics},\n   author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},\n   year={2018}\n}","description":"Scientific papers datasets contains two sets of long and structured documents.\nThe datasets are obtained from ArXiv and PubMed OpenAccess repositories.\n\nBoth \"arxiv\" and \"pubmed\" have two features:\n  - article: the body of the document, pagragraphs seperated by \"/n\".\n  - abstract: the abstract of the document, pagragraphs seperated by \"/n\".\n  - section_names: titles of sections, seperated by \"/n\".","key":""},{"id":"pdesoyres/test","private":false,"tags":[],"author":"pdesoyres","key":""},{"id":"peixian/equity_evaluation_corpus","private":false,"tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:original","task_categories:text-classification","task_ids:text-classification-other-gender-classification"],"author":"peixian","citation":"@article{DBLP:journals/corr/abs-1805-04508,\n  author    = {Svetlana Kiritchenko and\n               Saif M. Mohammad},\n  title     = {Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems},\n  journal   = {CoRR},\n  volume    = {abs/1805.04508},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1805.04508},\n  archivePrefix = {arXiv},\n  eprint    = {1805.04508},\n  timestamp = {Mon, 13 Aug 2018 16:47:58 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-04508.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","description":"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems and resources. Further, there is a lack of benchmark datasets for examining inappropriate biases in system predictions. Here, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We used the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 ‘Affect in Tweets’. We found that several of the systems showed statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available, and encourage its use to evaluate biases in sentiment and other NLP tasks.","key":""},{"id":"peixian/rtGender","private":false,"tags":["annotations_creators:crowdsourced","language_creators:found","languages:en","licenses:unknown","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:multi-label-classification"],"author":"peixian","citation":"@inproceedings{voigt-etal-2018-rtgender,\n    title = \"{R}t{G}ender: A Corpus for Studying Differential Responses to Gender\",\n    author = \"Voigt, Rob  and\n      Jurgens, David  and\n      Prabhakaran, Vinodkumar  and\n      Jurafsky, Dan  and\n      Tsvetkov, Yulia\",\n    booktitle = \"Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)\",\n    month = may,\n    year = \"2018\",\n    address = \"Miyazaki, Japan\",\n    publisher = \"European Language Resources Association (ELRA)\",\n    url = \"https://www.aclweb.org/anthology/L18-1445\",\n}","description":"RtGender is a corpus for studying responses to gender online, including posts and responses from Facebook, TED, Fitocracy, and Reddit where the gender of the source poster/speaker is known.","key":""},{"id":"pelican/test_100","private":false,"tags":[],"author":"pelican","key":""},{"id":"pere/nb_nn_balanced_shuffled","private":false,"tags":[],"author":"pere","citation":"@inproceedings{kummervold-etal-2021-operationalizing,\n    title = \"Operationalizing a National Digital Library: The Case for a {N}orwegian Transformer Model\",\n    author = \"Kummervold, Per E  and\n      De la Rosa, Javier  and\n      Wetjen, Freddy  and\n      Brygfjeld, Svein Arne\",\n    booktitle = \"Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)\",\n    month = may # \" 31--2 \" # jun,\n    year = \"2021\",\n    address = \"Reykjavik, Iceland (Online)\",\n    publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n    url = \"https://aclanthology.org/2021.nodalida-main.3\",\n    pages = \"20--29\",\n    abstract = \"In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library. The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both Norwegian Bokm{\\aa}l and Norwegian Nynorsk. Our model also improves the mBERT performance for other languages present in the corpus such as English, Swedish, and Danish. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties. Therefore, we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.\",\n}","description":"A balanced dataset of Nynorsk and Bokmaal text. The total size is 19GB. The text is taken from the Colossal Norwegian Corpus v2.","key":""},{"id":"persiannlp/parsinlu_entailment","private":false,"tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:fa","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|translated|mnli","task_categories:textual-entailment","task_categories:natural-language-inference","task_ids:textual-entailment","task_ids:natural-language-inference"],"author":"persiannlp","citation":"@article{huggingface:dataset,\n    title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n    authors = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and others},\n    year={2020}\n    journal = {arXiv e-prints},\n    eprint = {2012.06154},    \n}","description":"A Persian textual entailment task (deciding `sent1` entails `sent2`).","key":""},{"id":"persiannlp/parsinlu_query_paraphrasing","private":false,"tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:fa","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|quora|google","task_categories:query-paraphrasing","task_ids:query-paraphrasing"],"author":"persiannlp","citation":"@article{huggingface:dataset,\n    title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n    authors = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and others},\n    year={2020}\n    journal = {arXiv e-prints},\n    eprint = {2012.06154},    \n}","description":"A Persian query paraphrasing task (paraphrase or not, given two questions). \nThe questions are partly mined using Google auto-complete, and partly translated from Quora paraphrasing dataset.","key":""},{"id":"persiannlp/parsinlu_reading_comprehension","private":false,"tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:fa","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|wikipedia|google","task_categories:question-answering","task_ids:extractive-qa"],"author":"persiannlp","citation":"@article{huggingface:dataset,\n    title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n    authors = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and others},\n    year={2020}\n    journal = {arXiv e-prints},\n    eprint = {2012.06154},    \n}","description":"A Persian reading comprehenion task (generating an answer, given a question and a context paragraph). \nThe questions are mined using Google auto-complete, their answers and the corresponding evidence documents are manually annotated by native speakers.","key":""},{"id":"persiannlp/parsinlu_sentiment","private":false,"tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:fa","licenses:cc-by-nc-sa-4.0","multilinguality:monolingual","size_categories:1K<n<10K","source_datasets:extended|translated|mnli","task_categories:sentiment-analysis","task_ids:sentiment-analysis"],"author":"persiannlp","citation":"@article{huggingface:dataset,\n    title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n    authors = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and others},\n    year={2020}\n    journal = {arXiv e-prints},\n    eprint = {2012.06154},    \n}","description":"A Persian sentiment analysis task (deciding whether a given sentence contains a particular sentiment).","key":""},{"id":"persiannlp/parsinlu_translation_en_fa","private":false,"tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:fa","licenses:cc-by-nc-sa-4.0","multilinguality:fa","multilinguality:en","size_categories:1K<n<10K","source_datasets:extended","task_categories:translation","task_ids:translation"],"author":"persiannlp","citation":"@article{huggingface:dataset,\n    title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n    authors = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and others},\n    year={2020}\n    journal = {arXiv e-prints},\n    eprint = {2012.06154},    \n}","description":"A Persian translation dataset (English -> Persian).","key":""},{"id":"persiannlp/parsinlu_translation_fa_en","private":false,"tags":["annotations_creators:expert-generated","language_creators:expert-generated","languages:fa","licenses:cc-by-nc-sa-4.0","multilinguality:fa","multilinguality:en","size_categories:1K<n<10K","source_datasets:extended","task_categories:translation","task_ids:translation"],"author":"persiannlp","citation":"@article{huggingface:dataset,\n    title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n    authors = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and others},\n    year={2020}\n    journal = {arXiv e-prints},\n    eprint = {2012.06154},    \n}","description":"A Persian translation dataset (Persian -> English).","key":""},{"id":"piEsposito/br-quad-2.0","private":false,"tags":[],"author":"piEsposito","citation":"@article{2020braquad,\n       author = {{Esposito}, Wladimir and {Esposito}, Piero and {Tamais},\n                 Ana Laura and {Gatti}, Daniel},\n        title = \"{BrQuAD - Brazilian\n                  Question-Answering Dataset: Dataset para benchmark de modelos de\n                  Machine Learning para question-answering em\n                  Portugu^es brasileiro traduzindo o SQuAD com Google Cloud API}\",\n         year = 2020,\n}","description":"Translates SQuAD 2.0 from english to portuguese using Google Cloud API","key":""},{"id":"piEsposito/br_quad_20","private":false,"tags":[],"author":"piEsposito","citation":"@article{2020braquad,\n       author = {{Esposito}, Wladimir and {Esposito}, Piero and {Tamais},\n                 Ana Laura and {Gatti}, Daniel},\n        title = \"{BrQuAD - Brazilian\n                  Question-Answering Dataset: Dataset para benchmark de modelos de\n                  Machine Learning para question-answering em\n                  Portugu^es brasileiro traduzindo o SQuAD com Google Cloud API}\",\n         year = 2020,\n}","description":"Translates SQuAD 2.0 from english to portuguese using Google Cloud API","key":""},{"id":"piEsposito/squad_20_ptbr","private":false,"tags":[],"author":"piEsposito","citation":"@article{2020braquad,\n       author = {{Esposito}, Wladimir and {Esposito}, Piero and {Tamais},\n                 Ana Laura and {Gatti}, Daniel},\n        title = \"{BrQuAD - Brazilian\n                  Question-Answering Dataset: Dataset para benchmark de modelos de\n                  Machine Learning para question-answering em\n                  Portugu^es brasileiro traduzindo o SQuAD com Google Cloud API}\",\n         year = 2020,\n}","description":"Translates SQuAD 2.0 from english to portuguese using Google Cloud API","key":""},{"id":"princeton-nlp/datasets-for-simcse","private":false,"tags":[],"author":"princeton-nlp","key":""},{"id":"priya3301/Graduation_admission","private":false,"tags":[],"author":"priya3301","key":""},{"id":"priya3301/tes","private":false,"tags":[],"author":"priya3301","key":""},{"id":"priya3301/test","private":false,"tags":[],"author":"priya3301","key":""},{"id":"rays2pix/example","private":false,"tags":[],"author":"rays2pix","key":""},{"id":"rays2pix/example_dataset","private":false,"tags":[],"author":"rays2pix","key":""},{"id":"rewardsignal/reddit_writing_prompts","private":false,"tags":[],"author":"rewardsignal","key":""},{"id":"rony/soccer-dialogues","private":false,"tags":[],"author":"rony","key":""},{"id":"roskoN/dstc8-reddit-corpus","private":false,"tags":[],"author":"roskoN","citation":"@article{lee2019multi,\n  title={Multi-domain task-completion dialog challenge},\n  author={Lee, S and Schulz, H and Atkinson, A and Gao, J and Suleman, K and El Asri, L and Adada, M and Huang, M and Sharma, S and Tay, W and others},\n  journal={Dialog system technology challenges},\n  volume={8},\n  pages={9},\n  year={2019}\n}","description":"The DSTC8 dataset as provided in the original form.\nThe only difference is that the splits are in separate zip files.\nIn the orignal output it is one big archive containing all splits.","key":""},{"id":"sagnikrayc/mctest","private":false,"tags":["annotations_creators:expert-generated","language_creators:found","languages:en-US","licenses:microsoft-research-license","multilinguality:monolingual","size_categories:1K<n<10K","task_categories:question-answering","task_ids:multiple-choice-qa","task_ids:question-answering-other-explanations-in-question-answering"],"author":"sagnikrayc","citation":"@inproceedings{richardson-etal-2013-mctest,\n    title = \"{MCT}est: A Challenge Dataset for the Open-Domain Machine Comprehension of Text\",\n    author = \"Richardson, Matthew  and\n      Burges, Christopher J.C.  and\n      Renshaw, Erin\",\n    booktitle = \"Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing\",\n    month = oct,\n    year = \"2013\",\n    address = \"Seattle, Washington, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D13-1020\",\n    pages = \"193--203\",\n}","description":"MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.","paperswithcode_id":"mctest","key":""},{"id":"sagnikrayc/quasar","private":false,"tags":["annotations_creators:expert-generated","language_creators:found","languages:en-US","licenses:bsd-3-clause","multilinguality:monolingual","size_categories:10K<n<100K","task_categories:question-answering","task_ids:open-domain-qa","task_ids:extractive-qa"],"author":"sagnikrayc","citation":"@article{dhingra2017quasar,\n  title={Quasar: Datasets for Question Answering by Search and Reading},\n  author={Dhingra, Bhuwan and Mazaitis, Kathryn and Cohen, William W},\n  journal={arXiv preprint arXiv:1707.03904},\n  year={2017}\n}","description":"We present two new large-scale datasets aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. The Quasar-S dataset consists of 37000 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow. The posts and comments on the website serve as the background corpus for answering the cloze questions. The Quasar-T dataset consists of 43000 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 serves as the background corpus for extracting these answers. We pose these datasets as a challenge for two related subtasks of factoid Question Answering: (1) searching for relevant pieces of text that include the correct answer to a query, and (2) reading the retrieved text to answer the query.","paperswithcode_id":"quasar-1","key":""},{"id":"salesken/Paraphrase_category_detection","private":false,"tags":[],"author":"salesken","key":""},{"id":"sdfufygvjh/fgghuviugviu","private":false,"tags":[],"author":"sdfufygvjh","key":""},{"id":"seamew/ChnSentiCorp","private":false,"tags":[],"author":"seamew","key":""},{"id":"seamew/Hotel","private":false,"tags":[],"author":"seamew","key":""},{"id":"seamew/THUCNews","private":false,"tags":[],"author":"seamew","key":""},{"id":"seamew/Weibo","private":false,"tags":[],"author":"seamew","key":""},{"id":"seamew/amazon_reviews_zh","private":false,"tags":[],"author":"seamew","key":""},{"id":"seamew/weibo_avg","private":false,"tags":[],"author":"seamew","key":""},{"id":"shahrukhx01/questions-vs-statements","private":false,"tags":[],"author":"shahrukhx01","key":""},{"id":"sharejing/BiPaR","private":false,"tags":[],"author":"sharejing","key":""},{"id":"sileod/metaeval","private":false,"tags":[],"author":"sileod","key":""},{"id":"sismetanin/rureviews","private":false,"tags":[],"author":"sismetanin","key":""},{"id":"smallv0221/my-test","private":false,"tags":[],"author":"smallv0221","key":""},{"id":"somaimanguyat/Genjer","private":false,"tags":[],"author":"somaimanguyat","key":""},{"id":"somaimanguyat/Koboy","private":false,"tags":[],"author":"somaimanguyat","key":""},{"id":"somaimanguyat/Movieonline2021","private":false,"tags":[],"author":"somaimanguyat","key":""},{"id":"somaimanguyat/Salome","private":false,"tags":[],"author":"somaimanguyat","key":""},{"id":"somaimanguyat/movie21","private":false,"tags":[],"author":"somaimanguyat","key":""},{"id":"somaimanguyat/xiomay","private":false,"tags":[],"author":"somaimanguyat","key":""},{"id":"spacemanidol/ms_marco_doc2query","private":false,"tags":[],"author":"spacemanidol","key":""},{"id":"spacemanidol/msmarco_passage_ranking","private":false,"tags":[],"author":"spacemanidol","key":""},{"id":"ssasaa/gghghgh","private":false,"tags":[],"author":"ssasaa","key":""},{"id":"sshleifer/pseudo_bart_xsum","private":false,"tags":[],"author":"sshleifer","citation":"@article{Narayan2018DontGM,\n  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},\n  author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},\n  journal={ArXiv},\n  year={2018},\n  volume={abs/1808.08745}\n}","description":"Extreme Summarization (XSum) Dataset.\n\nThere are two features:\n  - document: Input news article.\n  - summary: One sentence summary of the article.","key":""},{"id":"stas/openwebtext-10k","private":false,"tags":[],"author":"stas","citation":"@misc{Gokaslan2019OpenWeb,\n  title={OpenWebText Corpus},\n  author={Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, Stefanie Tellex},\n  howpublished{\\\\url{http://Skylion007.github.io/OpenWebTextCorpus}},\n  year={2019}\n}","description":"An open-source replication of the WebText dataset from OpenAI.\n\nThis is a small subset representing the first 10K records from the original dataset - created for testing.\n\nThe full 8M-record dataset is at https://huggingface.co/datasets/openwebtext","key":""},{"id":"stas/wmt14-en-de-pre-processed","private":false,"tags":[],"author":"stas","citation":"@InProceedings{huggingface:dataset,\ntitle = {WMT14 English-German Translation Data with further preprocessing},\nauthors={},\nyear={2016}\n}","key":""},{"id":"stas/wmt16-en-ro-pre-processed","private":false,"tags":[],"author":"stas","citation":"@InProceedings{huggingface:dataset,\ntitle = {WMT16 English-Romanian Translation Data with further preprocessing},\nauthors={},\nyear={2016}\n}","key":""},{"id":"stiel/skjdhjkasdhasjkd","private":false,"tags":[],"author":"stiel","key":""},{"id":"subiksha/OwnDataset","private":false,"tags":[],"author":"subiksha","key":""},{"id":"susumu2357/squad_v2_sv","private":false,"tags":["languages:sv","licenses:apache-2.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:extended|wikipedia","task_categories:question-answering","task_ids:extractive-qa"],"author":"susumu2357","description":"SQuAD_v2_sv is a Swedish version of SQuAD2.0. Translation was done automatically by using Google Translate API but it is not so straightforward because;\n\n1. the span which determines the start and the end of the answer in the context may vary after translation,\n2. tne translated context may not contain the translated answer if we translate both independently.\n\nMore details on how to handle these will be provided in another blog post.","key":""},{"id":"svalabs/all-nli-german-translation-wmt19","private":false,"tags":[],"author":"svalabs","key":""},{"id":"svalabs/ms-marco-german-translation-wmt19","private":false,"tags":["language_creators:machine-generated","languages:de-DE","multilinguality:monolingual","size_categories:unknown","source_datasets:extended|ms_marco","task_categories:text-retrieval","task_ids:document-retrieval"],"author":"svalabs","key":""},{"id":"tals/test","private":false,"tags":[],"author":"tals","key":""},{"id":"tanfiona/causenet_wiki","private":false,"tags":[],"author":"tanfiona","citation":"@inproceedings{heindorf2020causenet,\n  author    = {Stefan Heindorf and\n               Yan Scholten and\n               Henning Wachsmuth and\n               Axel-Cyrille Ngonga Ngomo and\n               Martin Potthast},\n  title     = CauseNet: Towards a Causality Graph Extracted from the Web,\n  booktitle = CIKM,\n  publisher = ACM,\n  year      = 2020\n}","description":"Crawled Wikipedia Data from CIKM 2020 paper \n'CauseNet: Towards a Causality Graph Extracted from the Web.'","key":""},{"id":"tarudesu/UIT-ViCTSD","private":false,"tags":[],"author":"tarudesu","citation":"@misc{nguyen2021constructive,\n      title={Constructive and Toxic Speech Detection for Open-domain Social Media Comments in Vietnamese}, \n      author={Luan Thanh Nguyen and Kiet Van Nguyen and Ngan Luu-Thuy Nguyen},\n      year={2021},\n      eprint={2103.10069},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}","description":"UIT-ViCTSD Dataset: A Vietnamese Constructive and Toxic Speech Detection Dataset.\n This is a dataset for constructive and toxic speech detection with 10,000 human-annotated comments.\n\n This is a dataset for binary sentiment classification, made of user comment scraped from VnExpress.net.\n It contains 10,000 comments divided into 3 splits: train (7K comments), dev (2K) and test (1K).","key":""},{"id":"thiemowa/argumentationreviewcorpus","private":false,"tags":[],"author":"thiemowa","key":""},{"id":"thiemowa/empathyreviewcorpus","private":false,"tags":[],"author":"thiemowa","key":""},{"id":"thomwolf/github-dataset","private":false,"tags":[],"author":"thomwolf","key":""},{"id":"thomwolf/github-python","private":false,"tags":[],"author":"thomwolf","key":""},{"id":"tianxing1994/temp","private":false,"tags":[],"author":"tianxing1994","key":""},{"id":"tommy19970714/common_voice","private":false,"tags":[],"author":"tommy19970714","citation":"@inproceedings{commonvoice:2020,\n  author = {Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F. M. and Weber, G.},\n  title = {Common Voice: A Massively-Multilingual Speech Corpus},\n  booktitle = {Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)},\n  pages = {4211--4215},\n  year = 2020\n}","description":"Common Voice is Mozilla's initiative to help teach machines how real people speak.\nThe dataset currently consists of 7,335 validated hours of speech in 60 languages, but we’re always adding more voices and languages.","key":""},{"id":"turingbench/TuringBench","private":false,"tags":["annotations_creators:found","language_creators:found","language_creators:machine-generated","languages:en","licenses:apache-2.0","multilinguality:monolingual","size_categories:unknown","source_datasets:original","task_categories:text-classification","task_ids:multi-class-classification"],"author":"turingbench","citation":"@InProceedings{huggingface:dataset,\ntitle = {A great new dataset},\nauthor={huggingface, Inc.\n},\nyear={2020}\n}","description":"This benchmark environment contains a dataset comprised of generated texts from pre-trained language models.\nWe also have two benchmark tasks - human vs. machine (i.e., binary classification) and authorship\nattribution (i.e., multi-class classification). These benchmark tasks and dataset are hosted on the\nTuringBench website with Leaderboards for each task.","key":""},{"id":"uasoyasser/rgfes","private":false,"tags":[],"author":"uasoyasser","key":""},{"id":"uva-irlab/canard_quretec","private":false,"tags":[],"author":"uva-irlab","citation":"@inproceedings{Elgohary:Peskov:Boyd-Graber-2019,\n  Title = {Can You Unpack That? Learning to Rewrite Questions-in-Context},\n  Author = {Ahmed Elgohary and Denis Peskov and Jordan Boyd-Graber},\n  Booktitle = {Empirical Methods in Natural Language Processing},\n  Year = {2019}\n}","description":"CANARD has been preprocessed by Voskarides et al. to train and evaluate their Query Resolution Term Classification\nmodel (QuReTeC).\n\nCANARD is a dataset for question-in-context rewriting that consists of questions each given in a dialog context\ntogether with a context-independent rewriting of the question. The context of each question is the dialog utterences\nthat precede the question. CANARD can be used to evaluate question rewriting models that handle important linguistic\nphenomena such as coreference and ellipsis resolution.","key":""},{"id":"uva-irlab/trec-cast-2019-multi-turn","private":false,"tags":["languages:en-US","multilinguality:monolingual","size_categories:10M<n<100M","task_categories:text-retrieval","task_ids:document-retrieval"],"author":"uva-irlab","citation":"@misc{dalton2020trec,\n      title={TREC CAsT 2019: The Conversational Assistance Track Overview}, \n      author={Jeffrey Dalton and Chenyan Xiong and Jamie Callan},\n      year={2020},\n      eprint={2003.13624},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}","description":"The Conversational Assistance Track (CAsT) is a new track for TREC 2019 to facilitate Conversational Information \nSeeking (CIS) research and to create a large-scale reusable test collection for conversational search systems. \nThe document corpus is 38,426,252 passages from the TREC Complex Answer Retrieval (CAR) and Microsoft MAchine \nReading COmprehension (MARCO) datasets.","key":""},{"id":"vasudevgupta/bigbird-tokenized-natural-questions","private":false,"tags":[],"author":"vasudevgupta","key":""},{"id":"vasudevgupta/data","private":false,"tags":[],"author":"vasudevgupta","key":""},{"id":"vasudevgupta/gsoc-librispeech","private":false,"tags":[],"author":"vasudevgupta","key":""},{"id":"vasudevgupta/natural-questions-validation","private":false,"tags":[],"author":"vasudevgupta","key":""},{"id":"vasudevgupta/temperature-distribution-2d-plate","private":false,"tags":[],"author":"vasudevgupta","key":""},{"id":"vasudevgupta/temperature-distribution-3d-cylinder","private":false,"tags":[],"author":"vasudevgupta","key":""},{"id":"vblagoje/wikipedia_snippets_streamed","private":false,"tags":[],"author":"vblagoje","citation":"@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}","description":"The dataset was built from the Wikipedia dump (https://dumps.wikimedia.org/).\nEach example contains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).","key":""},{"id":"vctc92/sdsd","private":false,"tags":[],"author":"vctc92","key":""},{"id":"vctc92/test","private":false,"tags":[],"author":"vctc92","key":""},{"id":"versae/adobo","private":false,"tags":[],"author":"versae","citation":"@inproceedings{XXX,\n  title={ADoBo IberCLEF 2021 Shared Task},\n  author={Elena Álvarez-Mellado, Luis Espinosa-Anke, Julio Gonzalo Arroyo, Constantine Lignos, Jordi Porta-Zamorano},\n  booktitle={SEPLN 2021},\n  year={2021},\n  url={}\n}","description":"ADoBo is the shared task on automatic detection of borrowings. We propose a\nshared task on detecting direct, unadapted, emerging borrowings in the Spanish\npress, i.e. detecting lexical borrowings that appear in the Spanish press and\nthat have recently been imported into the Spanish language (words like running,\nsmartwatch, youtuber or fake news).","key":""},{"id":"versae/mc4-es-sampled","private":false,"tags":[],"author":"versae","key":""},{"id":"vershasaxena91/datasets","private":false,"tags":[],"author":"vershasaxena91","key":""},{"id":"vershasaxena91/squad_multitask","private":false,"tags":[],"author":"vershasaxena91","citation":"\\@article{2016arXiv160605250R,\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                 Konstantin and {Liang}, Percy},\n        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n      journal = {arXiv e-prints},\n         year = 2016,\n          eid = {arXiv:1606.05250},\n        pages = {arXiv:1606.05250},\narchivePrefix = {arXiv},\n       eprint = {1606.05250},\n}","description":"\\Stanford Question Answering Dataset (SQuAD) is a reading comprehension \\dataset, consisting of questions posed by crowdworkers on a set of Wikipedia \\articles, where the answer to every question is a segment of text, or span, \\from the corresponding reading passage, or the question might be unanswerable.","key":""},{"id":"w-nicole/childes_data","private":false,"tags":[],"author":"w-nicole","key":""},{"id":"w-nicole/childes_data_no_tags","private":false,"tags":[],"author":"w-nicole","key":""},{"id":"w-nicole/childes_data_no_tags_","private":false,"tags":[],"author":"w-nicole","key":""},{"id":"w-nicole/childes_data_with_tags","private":false,"tags":[],"author":"w-nicole","key":""},{"id":"w-nicole/childes_data_with_tags_","private":false,"tags":[],"author":"w-nicole","key":""},{"id":"w11wo/imdb-javanese","private":false,"tags":["annotations_creators:found","extended:original","language_creators:machine-generated","languages:jv","licenses:odbl-1.0","multilinguality:monolingual","size_categories:10K<n<100K","source_datasets:original","task_categories:text-classification","task_ids:sentiment-classification"],"author":"w11wo","citation":"\\\r\n@InProceedings{maas-EtAl:2011:ACL-HLT2011,\r\n  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\r\n  title     = {Learning Word Vectors for Sentiment Analysis},\r\n  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\r\n  month     = {June},\r\n  year      = {2011},\r\n  address   = {Portland, Oregon, USA},\r\n  publisher = {Association for Computational Linguistics},\r\n  pages     = {142--150},\r\n  url       = {http://www.aclweb.org/anthology/P11-1015}\r\n}","description":"Large Movie Review Dataset translated to Javanese.\r\nThis is a dataset for binary sentiment classification containing substantially\r\nmore data than previous benchmark datasets. We provide a set of 25,000 highly\r\npolar movie reviews for training, and 25,000 for testing. There is additional\r\nunlabeled data for use as well. We translated the original IMDB Dataset to\r\nJavanese using the multi-lingual MarianMT Transformer model from\r\n`Helsinki-NLP/opus-mt-en-mul`.","key":""},{"id":"webek18735/ddvoacantonesed","private":false,"tags":[],"author":"webek18735","key":""},{"id":"webek18735/dhikhscook","private":false,"tags":[],"author":"webek18735","key":""},{"id":"webis/args_me","private":false,"tags":["annotations_creators:machine-generated","language_creators:crowdsourced","languages:'en-US'","licenses:cc-by-4.0","multilinguality:monolingual","pretty_name:Webis args.me argument corpus","size_categories:100K<n<1M","source_datasets:original","task_categories:text-retrieval","task_ids:document-retrieval"],"author":"webis","citation":"@dataset{yamen_ajjour_2020_4139439,\n  author       = {Yamen Ajjour and\n                  Henning Wachsmuth and\n                  Johannes Kiesel and\n                  Martin Potthast and\n                  Matthias Hagen and\n                  Benno Stein},\n  title        = {args.me corpus},\n  month        = oct,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {1.0-cleaned},\n  doi          = {10.5281/zenodo.4139439},\n  url          = {https://doi.org/10.5281/zenodo.4139439}\n}","description":"The args.me corpus (version 1.0, cleaned) comprises 382 545 arguments crawled from four debate portals in the middle of 2019. The debate portals are Debatewise, IDebate.org, Debatepedia, and Debate.org. The arguments are extracted using heuristics that are designed for each debate portal.","key":""},{"id":"weijieliu/senteval_cn","private":false,"tags":[],"author":"weijieliu","key":""},{"id":"wmt/europarl","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/news-commentary","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/uncorpus","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/wikititles","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/wmt10","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/wmt13","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/wmt14","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/wmt15","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/wmt16","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/wmt17","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/wmt18","private":false,"tags":[],"author":"wmt","key":""},{"id":"wmt/wmt19","private":false,"tags":[],"author":"wmt","key":""},{"id":"yluisfern/PBU","private":false,"tags":[],"author":"yluisfern","key":""},{"id":"thomwolf/codeparrot","private":false,"tags":[],"author":"thomwolf","key":""},{"id":"pierreant-p/jcvd-or-linkedin","private":false,"tags":[],"author":"pierreant-p","key":""}]
